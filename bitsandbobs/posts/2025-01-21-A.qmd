---
title: "OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints"
description: Using an ensemble to overcome the lack of 3D understanding
date: 2025-01-21
categories: [Robotics, Natural Language Processing]
---

OmniManip is a hot new paper from Agibot and Peking University. It's about zero shot natural language robotic manipulation tasks. A current issue with this task, and current approaches the utilize VLMs, is that VLMs lack 3D spatial understanding. They're only trained on 2D images and video after all.

OmniManip utilizes an ensemble of models to achieve this goal. This is how it works at a high level.

A segmentation model is used to extract the relevant objects from the robot's vision. A VLM then filters out task relevant objects, and also breaks down the input task (input as text) into multiple stages.

Next, interaction primitives and their spatial constraints, for each stage, are extracted. A single view 3D generation model is used to generate meshes for all objects relevant to the task. A pose estimation model is then use to canonicalize the objects. The VLM then extracts interaction primitives and their corresponding constraints from, that are relevant for the task. Along the way, a LLM is used to grade various primitives and constriants.

Models hallucinate (in particular, the VLM). The world is not static. To overcome this, a closed loop system is introduced; a self correction mechanism based on resample, rendering and checking (RRC). The mechanism employs realtime feedback from the VLM to detect and correct interaction errors. A pose tracking algorithm is also used to continuously update the poses of all relevant objects, allowing the robot to be dynamically adjusted.