[
  {
    "objectID": "web_apps/index.html",
    "href": "web_apps/index.html",
    "title": "App Playground",
    "section": "",
    "text": "Here you can view various apps and gizmos I’ve created. Have a play through some of them let me know what you think!\nMore apps coming soon.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMore apps coming soon…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlood Classifier\n\n\n\nImage Classification\n\n\n\nHow well can you classify floods?\n\n\n\nSalman Naqvi\n\n\nTuesday, 20 September 2022 | 2022-09-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBear Classifier\n\n\n\nImage Classification\n\n\n\nCan you spot a black bear in a black night?\n\n\n\nSalman Naqvi\n\n\nSaturday, 30 April 2022 | 2022-04-30\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nLoading…\n\nClick here to unsubscribe from the App Playground."
  },
  {
    "objectID": "web_apps/apps/coming_soon.html",
    "href": "web_apps/apps/coming_soon.html",
    "title": "More apps coming soon…",
    "section": "",
    "text": "I never told you how soon…\n\n\n\nThis image was generated by Dall-E 2!"
  },
  {
    "objectID": "web_apps/apps/bear_detector.html",
    "href": "web_apps/apps/bear_detector.html",
    "title": "Bear Classifier",
    "section": "",
    "text": "This webapp was remade on Tuesday, 8 November 2022."
  },
  {
    "objectID": "unsubscribe.html",
    "href": "unsubscribe.html",
    "title": "Unsubscribe from ForBlog and App Playground Notifications",
    "section": "",
    "text": "Loading…"
  },
  {
    "objectID": "testing_page.html",
    "href": "testing_page.html",
    "title": "Testing Page",
    "section": "",
    "text": "ForBlog\n\n\n\n\n  \n\n\n\n\nAI in a Nutshell\n\n\nThis nutshell contains very little math!\n\n\nAI models are much, much simpler than you think.\n\n\n\n\n\n\nOct 4, 2022\n\n\nSalman Naqvi\n\n\n\n\n\n\n  \n\n\n\n\nDetecting Floods for Disaster Relief\n\n\nHow good are you at detecting floods?\n\n\nA rundown of the creation of my flood classifier.\n\n\n\n\n\n\nSep 12, 2022\n\n\nSalman Naqvi\n\n\n\n\n\n\n  \n\n\n\n\nData Quality is Important | Car Classifier\n\n\nClassy Cars\n\n\nMost of the time, data matters more than the model.\n\n\n\n\n\n\nJun 4, 2022\n\n\nSalman Naqvi\n\n\n\n\n\n\nNo matching items\n\n\n\n\nPlayground\n\n\n\n\n\n\n\n\n\n\nMore apps coming soon…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlood Classifier\n\n\nHow well can you classify floods?\n\n\n\nSalman Naqvi\n\n\nSep 20, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Salman Naqvi",
    "section": "",
    "text": "Greetings, fellow human! I’m a curious individual who dabbles in AI, does 3D CG, loves learning, and is a scitech geek and space nerd. I’ve also been to 18 countries."
  },
  {
    "objectID": "about.html#lived-in",
    "href": "about.html#lived-in",
    "title": "Salman Naqvi",
    "section": "Lived in",
    "text": "Lived in\nUnited Kingdom (5 years) | Saudi Arabia (6 years) | Pakistan (9 years)"
  },
  {
    "objectID": "about.html#been-to",
    "href": "about.html#been-to",
    "title": "Salman Naqvi",
    "section": "Been to",
    "text": "Been to\n\n\nEgypt* | England | France | Greece | Italy | Jordan* | Lebanon* | Malaysia | Pakistan | Qatar* | Saudi Arabia | Sri Lanka | Switzerland | Tanzania | Thailand | Turkey | UAE | Wales\n*Transit destinations; I count them still because I was technically within the country’s border."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the world of ForBo7",
    "section": "",
    "text": "I’m a curious individual who dabbles in AI, does 3D CG, loves learning, and is a scitech geek and space nerd. I’ve also been to 18 countries.\nRead more about me here.\nFeel free to contact me with one of the fancy buttons below!\n \n  \n   \n  \n    \n     Twitter\n  \n  \n    \n     GitHub\n  \n  \n    \n     Kaggle\n  \n  \n    \n     Artstation\n  \n  \n    \n     Email"
  },
  {
    "objectID": "index.html#forblog",
    "href": "index.html#forblog",
    "title": "Welcome to the world of ForBo7",
    "section": "ForBlog",
    "text": "ForBlog\nClick here to check out the latest on the ForBlog.\n\n\n\n\n  \n\n\n\n\nAI in a Nutshell\n\n\nThis nutshell contains very little math!\n\n\nAI models are much, much simpler than you think.\n\n\n\n\n\n\nOct 4, 2022\n\n\nSalman Naqvi\n\n\n\n\n\n\n  \n\n\n\n\nDetecting Floods for Disaster Relief\n\n\nHow good are you at detecting floods?\n\n\nA rundown of the creation of my flood classifier.\n\n\n\n\n\n\nSep 12, 2022\n\n\nSalman Naqvi\n\n\n\n\n\n\n  \n\n\n\n\nData Quality is Important | Car Classifier\n\n\nClassy Cars\n\n\nMost of the time, data matters more than the model.\n\n\n\n\n\n\nJun 4, 2022\n\n\nSalman Naqvi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#playground",
    "href": "index.html#playground",
    "title": "Welcome to the world of ForBo7",
    "section": "Playground",
    "text": "Playground\nClick here to play more in the playground.\n\n\n\n\n\n\n\n\n\n\nMore apps coming soon…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlood Classifier\n\n\nHow well can you classify floods?\n\n\n\nSalman Naqvi\n\n\nSep 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBear Classifier\n\n\nCan you spot a black bear in a black night?\n\n\n\nSalman Naqvi\n\n\nApr 30, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "patch_notes.html",
    "href": "patch_notes.html",
    "title": "Site Patch Notes",
    "section": "",
    "text": "Detailed patchnotes are unavailable prior to site version 2.0.0.0."
  },
  {
    "objectID": "patch_notes.html#version-2.0.3.0-27-november-2022",
    "href": "patch_notes.html#version-2.0.3.0-27-november-2022",
    "title": "Site Patch Notes",
    "section": "Version 2.0.3.0 | 27 November 2022",
    "text": "Version 2.0.3.0 | 27 November 2022\n\nFully implemented Twitter Cards"
  },
  {
    "objectID": "patch_notes.html#version-2.0.2.0-26-november-2022",
    "href": "patch_notes.html#version-2.0.2.0-26-november-2022",
    "title": "Site Patch Notes",
    "section": "Version 2.0.2.0 | 26 November 2022",
    "text": "Version 2.0.2.0 | 26 November 2022\n\nFully implemented Open Graph\nAdded button for direct link to site’s source code.\nTweaked landing page description."
  },
  {
    "objectID": "patch_notes.html#version-2.0.1.2-17-november-2022",
    "href": "patch_notes.html#version-2.0.1.2-17-november-2022",
    "title": "Site Patch Notes",
    "section": "Version 2.0.1.2 | 17 November 2022",
    "text": "Version 2.0.1.2 | 17 November 2022\n\nFixed broken license link."
  },
  {
    "objectID": "patch_notes.html#version-2.0.1.1-17-november-2022",
    "href": "patch_notes.html#version-2.0.1.1-17-november-2022",
    "title": "Site Patch Notes",
    "section": "Version 2.0.1.1 | 17 November 2022",
    "text": "Version 2.0.1.1 | 17 November 2022\n\nFixed broken site feedback link.\nUpdated site version references."
  },
  {
    "objectID": "patch_notes.html#version-2.0.1.0-17-november-2022",
    "href": "patch_notes.html#version-2.0.1.0-17-november-2022",
    "title": "Site Patch Notes",
    "section": "Version 2.0.1.0 | 17 November 2022",
    "text": "Version 2.0.1.0 | 17 November 2022\n\nFixed a bunch of broken links.\nFixed RSS buttons.\nShifted links on the landing page."
  },
  {
    "objectID": "patch_notes.html#version-2.0.0.0-16-november-2022",
    "href": "patch_notes.html#version-2.0.0.0-16-november-2022",
    "title": "Site Patch Notes",
    "section": "Version 2.0.0.0 | 16 November 2022",
    "text": "Version 2.0.0.0 | 16 November 2022\n\nCreated, erm, site patch notes.\nSite is now entirely remade in Quarto.\nUI overhaul.\nForBlog is no longer the main landing page.\nApp playground has been added; a place where I can host my various creations.\n\nNew…\n\nfavicon.\nabout me page.\nlanding page.\nForBlog home page.\nForBlog post layout.\nglobal search bar.\n\nAdded…\n\na ForBlog only search bar.\nForBlog post filters.\nForBlog and App Playground subscriptions.\na form for site feedback.\ncopyright licences.\nnew fancy buttons."
  },
  {
    "objectID": "patch_notes.html#version-1.0.0.0-15-may-2022",
    "href": "patch_notes.html#version-1.0.0.0-15-may-2022",
    "title": "Site Patch Notes",
    "section": "Version 1.0.0.0 | 15 May 2022",
    "text": "Version 1.0.0.0 | 15 May 2022\n\nInitial release.\nSite is built on fastpages, by fastai."
  },
  {
    "objectID": "feedback.html",
    "href": "feedback.html",
    "title": "Site Feedback",
    "section": "",
    "text": "Loading…"
  },
  {
    "objectID": "forblog/posts/4_data_quality_is_important.html",
    "href": "forblog/posts/4_data_quality_is_important.html",
    "title": "Data Quality is Important | Car Classifier",
    "section": "",
    "text": "This article was updated on Thursday, 10 November 2022.\n\nI recently created a car classifier that classified cars into their respective brands.\nDespite having almost 5000 images in my training set, I ended up trying out over a hundred layers in my model, and twenty epochs. Even then, I had an error rate of 17.4%.\nThe culprit? My dataset.\nI scraped 5000 images of cars (500 for each company) from DuckDuckGo. Naturally, as expected, the data quality is not so good.\nWhy? Below are some potential reasons:\n\nNoncar images present in dataset\nCars of incorrect company present in dataset\nF1 cars present in dataset\nA large variety of cars from different time periods present in dataset\nDifferent companys’ cars look similar\nModded cars present in dataset\nConcept cars present in dataset\nMultiple cars present in a single image\nCertain angles of cars appear more than others\nCars appear in certain backgrounds more than others\nThe search term {car_brand} car could be skewing results\n\nI could have absolutely achieved better results with fewer layers and fewer epochs if I trained the model on better quality data — or manually combed through the 5000 images 💀. However, I did use fastai’s GUI for data cleaning. This GUI sorts images by their loss which helps to determine if certain images should be relabeled or deleted.\nBelow is the confusion matrix for this model.\n\nIt can be seen that this model “confuses” between quite a few different brands: Ford and Chevrolet, Chevrolet and Ford, Jaguar and Aston Martin, Renault and Ford.\nBut why is data quality important? Because without good data, the model will not be able to “see” things the way they actually are, and in turn end up making worse predictions and not generalize to other data.\nLet’s say you did not know how, say, a toaster looked like. So I taught you by showing you pictures of a kettle. Then to test you, I showed you a set of pictures depicting various kitchen appliances and told you to find the toaster. You would not be able to.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtending upon this example, say I showed you toasters only from the last two years and from two brands only. You would not be able to identify toasters older than two years, and toasters from other brands to much success.\nObviously, humans are smarter and can infer. AI methods can only infer to a certain degree, mainly based on what is in their dataset. This talk does start to become more philosophical.\nThe point of this post is to emphasize the importance of data quality and different aspects to consider as to why data quality may not be good. You can have the best architecture in the world, but it is useless if you do not have good data.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/1_how_to_approach_creating_ai_models.html",
    "href": "forblog/posts/1_how_to_approach_creating_ai_models.html",
    "title": "How to Approach Creating AI Models",
    "section": "",
    "text": "This article was rewritten on Monday, 31 October 2022."
  },
  {
    "objectID": "forblog/posts/1_how_to_approach_creating_ai_models.html#introduction",
    "href": "forblog/posts/1_how_to_approach_creating_ai_models.html#introduction",
    "title": "How to Approach Creating AI Models",
    "section": "Introduction",
    "text": "Introduction\nHow you approach making models is crucial. The way AI methods are used in today’s landscape is very different. AI methods are created to solve small, atomic problems. And we’ve got most of the methods to handle these small tasks hammered down. Therefore, applied AI is not about creating models; it’s only a small part of it. It’s 80% problem solving and 20% implementing (I would not be surprised if it actually followed the 80-20 rule1).1 The 80/20 Rule, also known as the Pareto Principle\nThink of AI methods as a tool; think of it as a pencil. You can use pencils to draw, take notes, poke holes, and much more. There are also dozens of pencils out there. But what point is there in using any of those pencils if you don’t even know how to properly use a pencil in the first place? The art of creating pencils has already been perfected too.\nOne highly successful approach is the Drivetrain Approach, created by Jeremy Howard — who’s widely known for his fastai course and library —, Margit Zwemer, and Mike Loukides.\nThe goal of the Drivetrain Approach is to not just use data to generate more data — data that is in the form of predictions. But rather to use data to also generate actionable outcomes.\nThe official blogpost goes into much more depth here.\nIn this post, I’ll be providing a short overview of my understanding of this approach by applying it to the Elements of AI course’s final project (this online course was created by the University of Helsinki and Reaktor)."
  },
  {
    "objectID": "forblog/posts/1_how_to_approach_creating_ai_models.html#overview-of-the-drivetrain-approach",
    "href": "forblog/posts/1_how_to_approach_creating_ai_models.html#overview-of-the-drivetrain-approach",
    "title": "How to Approach Creating AI Models",
    "section": "Overview of the Drivetrain Approach",
    "text": "Overview of the Drivetrain Approach\nThere are four main steps to this approach:\n\nDefine the objective\nConsider your possible actions\nConsider your data\nCreate the models\n\n\n\n\nImage Source\n\n\n\nDefine the objective\nWrite out what you are really trying to achieve. What is your goal? Writing it out puts it in a tangible manner.\n\n\nConsider your actions\nThink about what actions you can take to achieve your objective.\nAlso think about what would happen if you did those actions.\nWhat would happen if I did x? Would y really be a good idea? What if z worked out too well? Will x lead to y? What would happen if x turned out poorly?\n\n\nConsider your data\nThink about the data you already have and how it could be used.\nThink about any further data that is needed and how it could be collected.\n\n\nCreate the models\nCreate models. But create models that produce actions. Actions that produce the best results for your objective."
  },
  {
    "objectID": "forblog/posts/1_how_to_approach_creating_ai_models.html#endangered-language-chatbot",
    "href": "forblog/posts/1_how_to_approach_creating_ai_models.html#endangered-language-chatbot",
    "title": "How to Approach Creating AI Models",
    "section": "Endangered Language Chatbot",
    "text": "Endangered Language Chatbot\nThe final project of the Elements of AI course asked me to come up with my own AI method that would solve a problem, and how it would do so.\nThe problem I tackled was the endangerment of languages. The solution I came up with was to create a chatbot that could converse in these endangered languages. I created an overview of how this could be done.\nThe overview can be read here.\nLet’s tackle this problem through the Drivetrain Approach.\n\nDefine the objective\nThe objective is to preserve languages that are in danger of going extinct. Through preserving languages, histories and cultures can be preserved.\n\n\nConsider your actions\nOne way this could be done is to create a chatbot that could converse in endangered languages. However, this would be a monumental task considering the amount of data needed to achieve this.\nAnother action that could be taken is to create an information retrieval (IR) system of sorts. A corpus of written text of the language could be provided, from which insights about the language’s history, culture, and way of conversing could be gained. In turn the language is preserved.\nThe latter action may be easier to achieve.\n\n\nConsider your data\nThe obvious source of data would be a corpora of text.\nHowever, a major problem arises for those languages which are only spoken. Audio recordings of conversations would have to be made which would take a lot of time and effort. This would be especially difficult for those languages where very few speakers remain.\nEven if a language does have written text, gathering enough text for the language can also be a problem: the language may not have much written text. This may especially be the case for endangered languages. Again, one solution is to manually create texts — using an NLP method to create these texts is not viable.\nIn short, for some languages, there may be no choice other than to manually create the data that would be fed into the system — this manual creation also has the chance to skew the performance of the model.\n\n\n\nKuş dili, a whistled language spoken in Turkey. How would such a language be preserved? Image Source\n\n\n\n\nCreate the model\nEither a chatbot needs to be created that speaks as accurately as a native speaker, or an IR system needs to be created that gives meaningful, correct insights into a language and its associated culture.\nThis step may either be easy or hard, depending on the language. Most NLP or IR systems have been built on a few, select languages. Perhaps this step may be easy for those languages that are similar to languages on which NLP or IR systems have already been built on. It will most likely be harder for those languages which are not."
  },
  {
    "objectID": "forblog/posts/1_how_to_approach_creating_ai_models.html#conclusion",
    "href": "forblog/posts/1_how_to_approach_creating_ai_models.html#conclusion",
    "title": "How to Approach Creating AI Models",
    "section": "Conclusion",
    "text": "Conclusion\nThis concludes my understanding of the Drivetrain Approach, through an example.\nApproaches are crucial: you can have state-of-the-art tools, but they are useless if not correctly applied. The approach you take can either make it or break it. Putting it into a concrete, organized, tangible manner goes a long way.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/6_ai_in_a_nutshell.html",
    "href": "forblog/posts/6_ai_in_a_nutshell.html",
    "title": "AI in a Nutshell",
    "section": "",
    "text": "This blog post was updated on Saturday, 12 November 2022.\nArtificial Intelligence. Machine Learning. Neural Networks. Deep Learning. Fancy Words. Deceptively Simple. All really the same.\nThe basic workflow to create such a system is below.\nVery simple, eh? Of course, it’s a very high level abstraction, but this high level view will make this seemingly complex topic very simple.\nFirst, what’s the main thing modern AI methods try to do? They try to make predictions about certain things.\nSo a function of sorts is needed to achieve this. A function that can make these predictions. Think of a function as a machine. You put something into the machine and then, with whatever was input, the machine then produces an output.\nThe machine that we will be working with has two input slots: one slot is for training and the other slot is for predictions.\nTo create a function that produces predictions, we need to tell the function what sort of predictions it needs to make.\nTo do that, we can pour some data into the training slot. This data will tell the function what sort of predictions to output. This process is known as fitting the function to the data.\nTo fit the function onto data, you train the function."
  },
  {
    "objectID": "forblog/posts/6_ai_in_a_nutshell.html#simple-case-quadratic-function",
    "href": "forblog/posts/6_ai_in_a_nutshell.html#simple-case-quadratic-function",
    "title": "AI in a Nutshell",
    "section": "Simple Case: Quadratic Function",
    "text": "Simple Case: Quadratic Function\nGasp! A quadratic?? What’s this nonsense!\nA quadratic is a very simple equation. When shown on a graph, it looks like this.\n\n\n\n\n\nWe’ll be using this equation to demonstrate a very simple example.\nThe basic workflow for fitting a function to data is below.\n\n\n\n\n\n\n\n\n\nflowchart TB\n    B[Calculate Loss] --> C[Calculate Gradients] --> D[Update Parameters] --> B\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt can seem like a lot at first glance; quite a few new terms too.\nWe’ll break this down by going over the very simple example.\nLet’s say we have the following data points that describe, say, the speed of an object with respect to time. We want to predict what the speed of an object would be outside these data points.\nThe horizontal axis is time and the vertical axis is the object’s speed.\n\n\n\n\n\nWe can see that the data looks like the quadratic function shown above! Therefore, we could use the quadratic to predict what the speed of the object would be after 2.0 s and before -2.0 s.\nA quadratic equation includes three numbers which we will call \\(a\\), \\(b\\), and \\(c\\). These three numbers affect or control how our quadratic function will end up looking. \\(a\\), \\(b\\), and \\(c\\) are our parameters.\nLet’s let \\(a\\), \\(b\\), and \\(c\\) all equal \\(1\\) to begin with.\n\n\n\n\n\nHmm, not a very good fit.\nLet’s try another set of values for the parameters: \\(2\\), \\(1\\), \\(1.5\\).\n\n\n\n\n\nLooking much better now!\nLet’s see what \\(2\\), \\(0\\), and \\(1.5\\) gives us.\n\n\n\n\n\nEyeballing this is difficult. A certain set of parameters we use may be good by looking at the resulting graph, but in reality, it may not be.\nWhat we need is something that can tell us how good our function is; something that tells us whether the changes we are making are actually good or not. To do this, we can calculate a number called the loss. The smaller the loss, the better the function is.\nThere are many different ways loss can be calculated. The way we will be doing it is known as mean absolute error (MAE). In simple terms, it tells us how far off each prediction is from the actual value. For example, if we have a MAE of 1, this means that, on average, each prediction we make is 1 unit off from the real value.\nIn our case, a MAE of 1 would mean that each prediction is on average 1 m/s off from the real value.\nLet’s repeat what we did above, but this time, we’ll also see what the MAE is.\n\n\n\n\n\nAgain, this means that on average, each prediction we will make is 2.61 m/s off from the real value.\n\n\n\n\n\nThat’s a big jump!\n\n\n\n\n\nHmm, things got worse.\nDoing this process by hand is very tedious. How do we know if the new set of parameters we are using would improve the function? There needs to be a way to automate this so we don’t have to sit down and do this by hand.\nWhat we can do is update the parameters based on the loss. This would in turn create new parameters that would decrease the loss.\n\n\n\n\n\n\n\n\n\nflowchart TB\n    A[Loss] -- Updates ---> B[Parameters] -- Updates ---> A\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s give \\(a\\), \\(b\\), and \\(c\\) an arbitrary set of parameters \\(1.1\\), \\(1.1\\), and \\(1.1\\).\nNow let’s create a quadratic with this set of parameters and calculate its mean absolute error.\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe MAE is 2.42.\n\n\n\n\nNow comes the next step: how do we update the parameters based on this loss we have calculated?\nTo do this, we calculate a new set of quantities known as the gradients. Each parameter has its own gradient.\nLet’s say \\(a\\) has the value of \\(1\\). If \\(a\\) has a gradient of value \\(0.5\\), this would mean that if we increase \\(a\\) by \\(1\\), the loss would increase by \\(0.5\\). Therefore, if we decrease \\(a\\) by \\(1\\), this would mean the loss would decrease by \\(0.5\\), which is what we want!\nRead over this once more and it’ll make sense!\nLet’s quickly go over the inverse: if \\(a\\) has a gradient of value \\(-0.5\\), increasing \\(a\\) by \\(1\\) would decrease the loss by \\(0.5\\) — again, this is what we want! Similarly, decreasing \\(a\\) by \\(1\\) would increase the loss by \\(0.5\\).\nThe gradients are calculated from the loss. Then the gradients, the current parameters, and along with another value, the parameters are updated to new values. The “another value” is known as the learning rate. The learning rate controls how much the gradients update the parameters.\n\n\n\n\n\n\n\n\n\nflowchart TB\n    A[Gradients]\n    B[Current Parameters]\n    C[Learning Rate]\n    D[Magical Box]\n    E[Updated Paramters]\n    A & B & C ---> D ---> E\n\n\n\n\n\n\n\n\n\n\n\n\n\nLets see this tangibly.\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe gradients for each parameter respectively are [-1.35, -0.03, -0.5].\n\n\n\n\nOkay, let’s break this down. The gradient for the first parameter \\(a\\) is \\(-1.35\\). This tells us that if we increase the parameter \\(a\\) by \\(1\\), our loss will decrease by \\(-1.35\\). Similary, if we increase the parameter \\(b\\) by \\(1\\), this will result in the loss being decreased by \\(-0.03\\). The same logic holds for \\(c\\).\nLet’s now update the parameters. Remember, the current set of parameters, their gradients, and the learning rate all update the current set of parameters to new values.\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe new parameters are [1.11, 1.1, 1.11].\n\n\n\n\nWe can now repeat the process as many times as desired. Let’s do it 4 times.\n\n\nPass: 0; Loss: 2.4010409560416095\nPass: 1; Loss: 1.9847692009423128\nPass: 2; Loss: 1.498316818239171\nPass: 3; Loss: 1.171195547258246\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe MAE after 4 passes is 1.17.\n\n\n\n\n\n\n\nAnd there you go! An even better fitting quadratic!\nLet’s see what the object’s speed is at 1 second.\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe object’s velocity at 1 seconds is 5.65 m/s.\n\n\n\n\nThat roughly seems right!\nLet’s see what the object’s speed would be at 3 seconds.\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe object’s velocity at 1 seconds is 30.31 m/s.\n\n\n\n\nAnd now, the diagram below should make sense!\n\n\n\n\n\n\n\n\n\nflowchart TB\n    B[Calculate Loss] --> C[Calculate Gradients] --> D[Update Parameters] --> B"
  },
  {
    "objectID": "forblog/posts/6_ai_in_a_nutshell.html#the-cool-case-relus",
    "href": "forblog/posts/6_ai_in_a_nutshell.html#the-cool-case-relus",
    "title": "AI in a Nutshell",
    "section": "The Cool Case: ReLUs",
    "text": "The Cool Case: ReLUs\nThe quadratic example above is a nice, simple way to get a grasp of things. However, you may be wondering, “What if the data doesn’t follow a quadratic shape? What do we do then?”\nAnd that’s a good question! What if our data doesn’t follow any sort of mathematical shape? What if we don’t even know the shape the data will follow? How do we know what function to use in that case?\nThere is a solution to that! There is an easy way to create a function that bends and twists itself to fit the data; an “unbound” function of sorts, as I like to call it.\nThis can be achieved by using another equation known as the ReLU. Another fancy word that can make you sound like a professional, while also being really simple. ReLU is short for Rectified Linear Unit.\nThe ReLU takes any value that is less than 0, and converts to 0.\nLet’s see this.\nTake the following line. It has both positive and negative values on the vertical axis.\n\n\n\n\n\nWhen we use a ReLU, all negative values are converted to zero.\n\n\n\n\n\nLet’s return to our original data.\n\n\n\n\n\nNow a single ReLU won’t work as seen below.\n\n\n\n\n\nEven after we try to fit it.\n\n\n\n\n\nBut look at what happens when two ReLUs are, literally, added together!\n\n\n\n\n\n\nPretty neat, hey?\nLet’s add a third ReLU to the mix.\n\n\n\n\n\n\nYou can see here how the function is adapting to the shape of the data.\nWith some extra experimentation, I was able to get the loss down to 1.08!\n\n\n\n\n\nThat said, it’s not too much of a difference when compared to two ReLUs.\nWhat if we add 5 more to the mix, for a total of 8?\n\n\n\n\n\nNice! The MAE has gone below 1!\nIt’s even beat the quadratic function from before! With some expermimenting, I had managed to get the quadratic’s loss down to 1.03.\n\n\n\n\n\n\nLet’s use the model that has 8 ReLUs to predict what the object’s velocity would be at 1 second.\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe object’s speed at 1 s is 4.9 m/s.\n\n\n\n\nHmm, yes, that is a bit off. But that is fine because overall, the function is a lot more accurate for all the datapoints."
  },
  {
    "objectID": "forblog/posts/6_ai_in_a_nutshell.html#conclusion",
    "href": "forblog/posts/6_ai_in_a_nutshell.html#conclusion",
    "title": "AI in a Nutshell",
    "section": "Conclusion",
    "text": "Conclusion\nSee how easy this stuff all is? All those fancy terms makes this feel complex when in reality, it’s all really simple.\nWhy not now go and venture off to learn more and implement your own models!\nBelow are two free courses I can recommend:\n\nElements of AI\nA great primer into AI. The course goes over the history, the implementations, and the implications of this field, all without needing the knowledge of programming or complex mathematics.\nPractical Deep Learning for Coders\nThis course is different from other AI courses you’ll find. How? Because instead of starting off with the nitty gritty basics, you begin by actually implementing your own simple image classifier (a model that can tell what thing is in an image). You’ll be surprised at how simple it is to implement models with minimal code, and how little you need to know to get started (hint: you only really need high-school maths).\n\nIf you have any questions, comments, suggestions, or feedback, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/6_ai_in_a_nutshell.html#acknowledgements",
    "href": "forblog/posts/6_ai_in_a_nutshell.html#acknowledgements",
    "title": "AI in a Nutshell",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis article was inspired by the How does a neural net really work Kaggle Notebook by Jeremy Howard, and lesson 3 of Practical Deep Learning for Coders."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html",
    "href": "forblog/posts/2_bear_classifier_model.html",
    "title": "My first AI model",
    "section": "",
    "text": "This article was updated on Tuesday, 1 November 2022."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#introduction",
    "href": "forblog/posts/2_bear_classifier_model.html#introduction",
    "title": "My first AI model",
    "section": "Introduction",
    "text": "Introduction\nThis is my first attempt at creating an AI model: an image classifier. This classifier can tell whether a grizzly bear, black bear, or teddy bear is in an image.\nYou can visit the classifier here to test it out for yourself!"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#load-libraries",
    "href": "forblog/posts/2_bear_classifier_model.html#load-libraries",
    "title": "My first AI model",
    "section": "Load libraries",
    "text": "Load libraries\n\n# No need to fret! fastai is specifically designed to be used with import *.\nfrom fastbook import *\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#download-image-files",
    "href": "forblog/posts/2_bear_classifier_model.html#download-image-files",
    "title": "My first AI model",
    "section": "Download image files",
    "text": "Download image files\nSpecify the bear images we wish to download.\n\nbear_types = ('grizzly', 'black', 'teddy',)\npath = Path('bears')\n\nDownload 200 of each bear (search_images_ddg defaults to 200 URLs) and assign them to a specific directory.\n\nif not path.exists():\n    path.mkdir()\n    for bear_type in bear_types:\n        destination = (path / bear_type)\n        destination.mkdir(exist_ok=True)\n        urls = search_images_ddg(f\"{bear_type} bear\")\n        download_iamges(destination, urls=urls)\n\nCheck if our folder has the image files.\n\nfns = get_image_files(path)\nfns\n\n(#802) [Path('bears/grizzly/00000238.jpg'),Path('bears/grizzly/00000047.jpg'),Path('bears/grizzly/00000199.jpg'),Path('bears/grizzly/00000237.jpg'),Path('bears/grizzly/00000055.jpg'),Path('bears/grizzly/00000000.png'),Path('bears/grizzly/00000235.jpg'),Path('bears/grizzly/00000159.jpg'),Path('bears/grizzly/00000268.jpg'),Path('bears/grizzly/00000266.jpg')...]\n\n\nCheck for corrupt images.\n\ncorrupt_images = verify_images(fns)\ncorrupt_images\n\n(#0) []\n\n\nRemove corrupt images.\n\ncorrupt_images.map(pathlib.Path.unlink)\n\n(#0) []"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#load-image-files",
    "href": "forblog/posts/2_bear_classifier_model.html#load-image-files",
    "title": "My first AI model",
    "section": "Load image files",
    "text": "Load image files\nThe DataBlock API for creates the necessary DataLoaders for us.\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128),\n)\n\nThe blocks parameter allows us to specify the independent and dependent variables.\nThe get_items parameter tells fastai how to obtain our data. We use the get_image_files function to obtain our images.\nThe splitter parameter allows us to tell fastai how to split our data into training and validation sets. Since our data is one big set, we use the RandomSplitter class and tell it to use 20% of our data as the validation set. We specify a seed so the same split occurs each time.\nThe get_y parameter obtains our labels. The parent_label function simply gets the name of the folder a file is in. Since we have organized our bear images into different folders, this will nicely handle our target labels.\nThe item_tfms parameter allows us to specify a transform to apply to our data. Since we want all our images to be of the same size, we use the Resize() class.\nWe now have a DataBlock object from which can load the data.\n\ndataloaders = bears.dataloaders(path)\n\nLet us view a few images in the validation set.\n\ndataloaders.valid.show_batch(max_n=4, nrows=1)"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#data-augmentation",
    "href": "forblog/posts/2_bear_classifier_model.html#data-augmentation",
    "title": "My first AI model",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nData augmentation refers to creating random variations to our input data. This produces new data points based on the existing data points. This allows each data point to look different, without changing their meaning.\nTypical examples of image augmentation include rotation, flipping, perspective warping, brightness changing, and contrast changing.\n\nCropping\nThe validation set images shown above are cropped. We achieved this by specifying the Resize argument when defining the DataBlock. Resize crops images to the size specified.\nCropping results in detail being lost.\nAlternatively, we can squish or stretch images, or pad them to a desired size.\n\n\nSquishing/Stretching\nThe problem with squishing or stretching images is that the model will learn to “see” images the way they are not supposed to be.\n\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish))\ndataloaders = bears.dataloaders(path)\ndataloaders.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\nPadding\nBy padding, the image is surrounded typically by black, meaningless pixels. This results in extra, wasted computation.\n\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))\ndataloaders = bears.dataloaders(path)\ndataloaders.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\nThe best approach is to take random crops of different parts of the same image. This makes sure that the model does not miss out on any details whilst letting it “know” how an object fully looks like.\nBelow, we have unique=True so that the same image is repeated with different variations.\n\nbears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\ndataloaders = bears.dataloaders(path)\ndataloaders.train.show_batch(max_n=4, nrows=1, unique=True)\n\n\n\n\nfastai comes with a function that applies a variety of augmentations to images. This can allow a model to “see” and recognize images in a variety of scenarios.\n\nbears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\ndataloaders  = bears.dataloaders(path)\ndataloaders.train.show_batch(max_n=8, nrows=2, unique=True)\n\n\n\n\nI have not used RandomResizedCrop here so that the different augmentations can be seen more clearly. RandomResizedCrop will be used when the model is trained.\nbatch_tfms tells fastai that we want to use these transforms on a batch."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#training-the-model",
    "href": "forblog/posts/2_bear_classifier_model.html#training-the-model",
    "title": "My first AI model",
    "section": "Training the model",
    "text": "Training the model\nWe do not have a lot of data. Only 200 images of each bear at most. Therefore, we will augment our images not only to get more data, but so that the model can recognize data in a variety of situations.\n\nbears = bears.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms(),\n)\ndataloaders = bears.dataloaders(path)\n\nWe will now create our learner and fine-tune it.\nWe will be using the ResNet18 architecture (which is a convolutional neural network, or CNN for short). Error rate will be the metric.\n\nlearn = cnn_learner(dataloaders, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.985666\n      0.104632\n      0.025000\n      00:20\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.132230\n      0.073527\n      0.012500\n      00:22\n    \n    \n      1\n      0.106222\n      0.054833\n      0.018750\n      00:22\n    \n    \n      2\n      0.087129\n      0.058497\n      0.012500\n      00:20\n    \n    \n      3\n      0.069890\n      0.058845\n      0.018750\n      00:19\n    \n  \n\n\n\nOur model only has a 1.9% error rate! Not bad! Though it seems if I had done an extra epoch, the error rate may have gone down to 1.3%, judging by the previous epochs’ error rates."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#visualizing-mistakes",
    "href": "forblog/posts/2_bear_classifier_model.html#visualizing-mistakes",
    "title": "My first AI model",
    "section": "Visualizing mistakes",
    "text": "Visualizing mistakes\nWe can visualize the mistakes the model is making by a confusion matrix.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n3 grizzly bears were misclassified as black bears.\nLet us see where the errors are occurring, so we can determine if they are due to a dataset problem or a model problem.\nTo do this, we will sort images by their loss.\n\ninterp.plot_top_losses(5, nrows=1)"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#data-cleaning",
    "href": "forblog/posts/2_bear_classifier_model.html#data-cleaning",
    "title": "My first AI model",
    "section": "Data cleaning",
    "text": "Data cleaning\nThe intuitive approach to data cleaning is to do it before training the model. However, a trained model can help us clean the data. For example, we can see some mislabaled bears in the above cases.\nfastai includes a GUI for data cleaning. This GUI allows you to choose a category/label and its associated training and validation sets. It then shows you images in order of highest-loss first, from which you can select images for removal or relabeling.\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\nImageClassifierCleaner does not actually delete or relabel. It just returns the indices that are to be deleted or relabeled.\n\n# Delete images selected for deletion.\nfor index in cleaner.delete():\n    cleaner.fns[index].unlink()\n\n# Relabel images selected for relabeling.\nfor index, category in cleaner.change():\n    shutil.move(str(cleaner.fns[index]), path/category)\n\nWe can now retrain and better performance should be expected."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#saving-the-model",
    "href": "forblog/posts/2_bear_classifier_model.html#saving-the-model",
    "title": "My first AI model",
    "section": "Saving the model",
    "text": "Saving the model\nA model consists of two parts: the architecture and the parameters.\nWhen we use the export() method, both of these are saved.\nThis method also saves the definition of our DataLoaders. This is done so that we do not have to redefine how to transform our data when the model is used in production.\nfastai uses our validation set DataLoader by default, so the data augmentation will not be applied, which is generally what is wanted.\nThe export() method creates a file named “export.pkl”.\n\nlearn.export()\n\nLet us check that the file exists.\n\npath = Path()\npath.ls(file_exts='.pkl')\n\n(#1) [Path('export.pkl')]\n\n\nIf you wish to deploy an app, this is the file you will need."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#loading-the-model-for-inference",
    "href": "forblog/posts/2_bear_classifier_model.html#loading-the-model-for-inference",
    "title": "My first AI model",
    "section": "Loading the model for inference",
    "text": "Loading the model for inference\nNow obviously we do not need to load the model as we already have the learner variable. But I shall do so anyways.\n\nlearn_inf = load_learner(path/'export.pkl')\n\nWe generally do inference for a single image at a time.\n\nlearn_inf.predict('images/grizzly.jpg')\n\n\n\n\n('grizzly', TensorBase(1), TensorBase([1.4230e-06, 1.0000e+00, 3.9502e-08]))\n\n\nThree things have been returned: the predicted category, the index of the predicted category, and the probabilities of each category.\nThe order of each category is based on the order of the vocabulary of the DataLoaders; that is, the stored tuple of all possible categories.\nThe DataLoaders can be accessed as an attribute of the Learner.\n\nlearn_inf.dataloaders.vocab\n\n['black', 'grizzly', 'teddy']"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#why-cnns-work-so-well",
    "href": "forblog/posts/2_bear_classifier_model.html#why-cnns-work-so-well",
    "title": "My first AI model",
    "section": "Why CNNs work so well",
    "text": "Why CNNs work so well\nThe ResNet18 architecture is a sort of CNN. Below is my understanding as to why CNNs work so well.\nA neural network is comprised of many layers. Each layer is comprised of many neurons. In a CNN, each neuron in the same layer is given the exact same weights, while being given different input data. This allows all neurons in a layer to fire upon detecting the same pattern.\nBecause of this, CNNs can become really good at detecting objects in various patterns, orientations, shapes, positions, and so on."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#conclusion",
    "href": "forblog/posts/2_bear_classifier_model.html#conclusion",
    "title": "My first AI model",
    "section": "Conclusion",
    "text": "Conclusion\nWell then, that wraps up my first deep learning model! I have to say, it is much easier than I thought it would be to implement a model. You do not need to go into the nitty gritty details of artificial intelligence. A high level understanding can suffice in the beginning. It is like playing a sport: you do not need to understand the physics to be able to play it.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html",
    "title": "Detecting Floods for Disaster Relief",
    "section": "",
    "text": "You can find this notebook on Kaggle here.\nThis article was updated on Friday, 11 November 2022.\nThe model that will be created in this notebook can detect whether an area shown in an image is flooded or not. The idea for creating this model has been spurred from the recent floodings in Pakistan.\nSuch models can prove useful in flood relief, helping to detect which areas need immediate focus.\nThe dataset used to train this model is Louisiana flood 2016, uploaded by Kaggle user Rahul T P, which you can view here.\nThe fastai library, a high level PyTorch library, has been used.\nOne of the points of this notebook is to showcase how simple it is to create powerful models. That said, this notebook is not a tutorial or guide."
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#sort-data.",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#sort-data.",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Sort data.",
    "text": "Sort data.\nThe data in the dataset needs to be organized into train and valid folders. Each will contain the same subfolders, 0 and 1, which will be used to label the data. A label of 0 indicates the area shown in the image is not flooded, while a label of 1 indicates the area shown in the image is flooded.\nThe images in the dataset itself has been organized as follows:\n    If no underscore is in the file name, the image shows the area before or after the flood.\n    If an underscore is in the file name, the image shows the area during the flood:\n\nIf a zero follows the underscore, the area was not flooded.\nIf a one follows the underscore, the area was flooded.\n\nCreating the necessary paths.\n\nworking_path = Path.cwd(); print(working_path)\nfolders = ('train', 'valid')\nlabels = ('0', '1')\n\n/kaggle/working\n\n\n\ninput_path = Path('/kaggle/input')\ntrain_image_paths = sorted(input_path.rglob('train/*.png'))\nvalid_image_paths = sorted(input_path.rglob('test/*.png'))\nlen(train_image_paths), len(valid_image_paths)\n\n(270, 52)\n\n\nCreating the necessary directories.\n\nfor folder in folders:\n    if not (working_path/folder).exists():\n        (working_path/folder).mkdir()\n    for label in labels:\n        if not (working_path/folder/label).exists():\n            (working_path/folder/label).mkdir()\n\nMove images to new directories.\n\ntry:\n    for image_path in train_image_paths:\n        if '_1' in image_path.stem:\n            with (working_path/'train'/'1'/image_path.name).open(mode='xb') as f:\n                f.write(image_path.read_bytes())\n        else:\n            with (working_path/'train'/'0'/image_path.name).open(mode='xb') as f:\n                f.write(image_path.read_bytes())\nexcept FileExistsError:\n    print(\"Training images have already been moved.\")\nelse:\n    print(\"Training images moved.\")\n\nTraining images moved.\n\n\n\ntry:\n    for image_path in valid_image_paths:\n        if '_1' in image_path.stem:\n            with (working_path/'valid'/'1'/image_path.name).open(mode='xb') as f:\n                f.write(image_path.read_bytes())\n        else:\n            with (working_path/'valid'/'0'/image_path.name).open(mode='xb') as f:\n                f.write(image_path.read_bytes())\nexcept FileExistsError:\n    print(\"Testing images have already been moved.\")\nelse:\n    print(\"Testing images moved.\")\n\nTesting images moved.\n\n\nCheck that images have been moved.\n\ntraining_images = get_image_files(working_path/'train'); print(len(training_images))\n\n270\n\n\n\nImage.open(training_images[0])\n\n\n\n\n\nvalidation_images = get_image_files(working_path/'valid'); print(len(validation_images))\n\n52\n\n\n\nImage.open(validation_images[-1])"
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#load-data",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#load-data",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Load data",
    "text": "Load data\nCreate the training and validation dataloaders through fastai’s quick and easy DataBlock class.\n\ndataloaders = DataBlock(\n    blocks = (ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    splitter = GrandparentSplitter(),\n    get_y = parent_label,\n    item_tfms = [Resize(192, method='squish')]\n).dataloaders(working_path, bs=32)\n\nCheck that data has been loaded correctly.\n\ndataloaders.show_batch(max_n=8)"
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#instantiate-and-train-model",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#instantiate-and-train-model",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Instantiate and Train Model",
    "text": "Instantiate and Train Model\n\nlearner = vision_learner(dataloaders, resnet18, metrics=error_rate)\nlearner.fine_tune(9)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.919323\n      1.118264\n      0.365385\n      00:09\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.490039\n      0.628054\n      0.250000\n      00:02\n    \n    \n      1\n      0.367996\n      0.411558\n      0.192308\n      00:02\n    \n    \n      2\n      0.266664\n      0.472146\n      0.192308\n      00:02\n    \n    \n      3\n      0.203069\n      0.256436\n      0.115385\n      00:03\n    \n    \n      4\n      0.158453\n      0.127106\n      0.076923\n      00:03\n    \n    \n      5\n      0.124499\n      0.095927\n      0.038462\n      00:02\n    \n    \n      6\n      0.098409\n      0.089279\n      0.038462\n      00:03\n    \n    \n      7\n      0.079600\n      0.093277\n      0.038462\n      00:02\n    \n    \n      8\n      0.064886\n      0.090372\n      0.038462\n      00:02\n    \n  \n\n\n\nNice! A relatively low error rate for no tweaking."
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#visualizing-mistakes",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#visualizing-mistakes",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Visualizing Mistakes",
    "text": "Visualizing Mistakes\nWe have to see how the model is getting confuzzled.\n\ninterp = ClassificationInterpretation.from_learner(learner)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnly a couple of mistakes. Let’s see what they are.\n\ninterp.plot_top_losses(5, nrows=1)\n\n\n\n\n\n\n\n\n\n\n\nNothing has been mislabeled, but the first one is especially tricky to determine, even for human eyes."
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#model-inference",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#model-inference",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Model Inference",
    "text": "Model Inference\nLet’s test the model on some images of the recent flooding in Pakistan.\n\ndef infer_image(image_path):\n    display(Image.open(image_path))\n    label, _, probabilities = learner.predict(PILImage(PILImage.create(image_path)))\n    if label == '0':\n        print(f\"The area shown in the image is not flooded with probability {probabilities[0]*100:.2f}%.\")\n    elif label == '1':\n        print(f\"The area shown in the image is flooded with probability {probabilities[1]*100:.2f}%.\")\n    else:\n        print(\"Unknown label assigned to image.\")\n\n\ninfer_image(input_path/'floodclassifiertestset'/'1'/'1.jpeg')\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is not flooded with probability 65.65%.\n\n\nNot bad!\nLet’s try it on another image.\n\ninfer_image(input_path/'floodclassifiertestset'/'1'/'2.jpg')\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is flooded with probability 99.90%.\n\n\nThe label for this image is kind of meaningless. This is an image of a vast area of land, so certain areas could be flooded, while others are not. That said, it could be used to determine whether there is flooding in the image.\n\ninfer_image(input_path/'floodclassifiertestset'/'1'/'3.jpg')\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is flooded with probability 99.99%.\n\n\nThe model performed really well in this case: the input image is shown at a different angle. The images in the training set only show areas from a top-down view.\n\ninfer_image(input_path/'floodclassifiertestset'/'1'/'4.jpg')\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is not flooded with probability 64.56%.\n\n\nOver here, the limitations of the current state of the model can be seen. The model is not performing well on images where the view is more parallel to the ground, since the images in the training set are all top-down.\nLet’s do two more images.\n\ninfer_image(input_path/'floodclassifiertestset'/'1'/'5.jpg')\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is flooded with probability 99.94%.\n\n\n\ninfer_image(input_path/'floodclassifiertestset'/'1'/'6.jpg')\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is flooded with probability 100.00%.\n\n\nThe model is working well with images of different sizes too, and has given this image a very high, correct confidence."
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#improving-the-model.",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#improving-the-model.",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Improving the model.",
    "text": "Improving the model.\nLet’s see if we can get the model’s performance to improve on the following image through augmenting the training set.\n\nImage.open(input_path/'floodclassifiertestset'/'1'/'4.jpg')\n\n\n\n\n\naugmented_dataloaders = DataBlock(\n    blocks = (ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    splitter = GrandparentSplitter(),\n    get_y = parent_label,\n    item_tfms = RandomResizedCrop(192, min_scale=0.5),\n    batch_tfms=aug_transforms()\n).dataloaders(working_path, bs=32)\n\n\naugmented_dataloaders.show_batch(max_n=8)\n\n\n\n\n\naugmented_learner = vision_learner(augmented_dataloaders, resnet18, metrics=error_rate)\naugmented_learner.fine_tune(9)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.161182\n      0.835870\n      0.365385\n      00:02\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.442552\n      0.686252\n      0.288462\n      00:03\n    \n    \n      1\n      0.417739\n      0.411907\n      0.153846\n      00:02\n    \n    \n      2\n      0.346400\n      0.316388\n      0.057692\n      00:03\n    \n    \n      3\n      0.306782\n      0.213407\n      0.076923\n      00:02\n    \n    \n      4\n      0.251947\n      0.199586\n      0.076923\n      00:02\n    \n    \n      5\n      0.209951\n      0.141818\n      0.057692\n      00:02\n    \n    \n      6\n      0.188433\n      0.116713\n      0.057692\n      00:03\n    \n    \n      7\n      0.169689\n      0.125078\n      0.057692\n      00:02\n    \n    \n      8\n      0.151843\n      0.131188\n      0.057692\n      00:02\n    \n  \n\n\n\nLet’s try the new model out.\n\ndisplay(Image.open(input_path/'floodclassifiertestset'/'1'/'4.jpg'))\nlabel, _, probabilities = augmented_learner.predict(PILImage(PILImage.create(input_path/'floodclassifiertestset'/'1'/'4.jpg')))\nif label == '0':\n    print(f\"The area shown in the image is not flooded with probability {probabilities[0]*100:.2f}%.\")\nelif label == '1':\n    print(f\"The area shown in the image is flooded with probability {probabilities[1]*100:.2f}%.\")\nelse:\n    print(\"Unknown label assigned to image.\")\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is flooded with probability 99.91%.\n\n\nDang, impressive! The correct label and with excellent confidence!\nBefore we get too excited though, we should check the performance on the model with the previous images.\n\ntest_dataloader = learner.dls.test_dl([image_path for image_path in sorted((input_path/'floodclassifiertestset').rglob('*.*'))])\n\n\nprobabilities, _, labels = augmented_learner.get_preds(dl=test_dataloader, with_decoded=True)\n\n\n\n\n\n\n\n\n\nprint(\"Images are numbered horizontally.\")\ntest_dataloader.show_batch()\nfor probability, label, image_number in zip(probabilities, labels, range(1, 7)):\n    if label == 1:\n        print(f\"Image {image_number} is flooded with a probability of {probability[1]*100:.2f}%.\")\n    elif label == 0:\n        print(f\"Image {image_number} is not flooded with a probability of {probability[0]*100:.2f}%.\")\n    else:\n        print(f\"Image {image_number} has been assigned an unknown label.\")\n\nImages are numbered horizontally.\nImage 1 is flooded with a probability of 95.94%.\nImage 2 is flooded with a probability of 99.92%.\nImage 3 is flooded with a probability of 91.34%.\nImage 4 is flooded with a probability of 99.71%.\nImage 5 is flooded with a probability of 100.00%.\nImage 6 is flooded with a probability of 100.00%.\n\n\n\n\n\nDrastically improved probabilities! A little augmentation can go a long way."
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#takeaways",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#takeaways",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Takeaways",
    "text": "Takeaways\nThis model was trained on only 270 images and minimal code. Accessbility and abstraction to the field of machine learning has come a long, long way. Given the right data and the right pretrained model, a powerful model can be produced in less than an hour, if not half.\nThis is important: in disasters such as floods, the time taken to produce the logistics required for relief can be drastically reduced. It is also important because the barrier of entry to this field is dramatically lowered; more people can create powerful models, in turn producing better solutions.\nHowever, there could be some improvements and additions made to the model:\n\nInclude a third class to the model. Images that are not flooded, but show signs of having been flooded would be assigned this class. The dataset used for this model includes such images.\nTrain the model on images that include a variety of geographic locations and dwellings. The current dataset only contains images taken in a lush, green area with plenty of trees; infrastructure looks a certain way; the color of the floodwater is also dependent on the surroundings. All this makes the model good a prediciting whether an image is flooded for images with certain features.\n\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/3_the_confusion_matrix.html",
    "href": "forblog/posts/3_the_confusion_matrix.html",
    "title": "A No Nonsense Guide to Reading a Confusion Matrix",
    "section": "",
    "text": "This article was updated on Thursday, 10 November 2022.\nConfusion matrices help model designers view what mistakes a model has made.\nIn this post, I’ll be telling you how to easily read such matrices.\nJump to Section 2 for an ultra concise rundown.\nReady? Here we go."
  },
  {
    "objectID": "forblog/posts/3_the_confusion_matrix.html#case-1-introduction",
    "href": "forblog/posts/3_the_confusion_matrix.html#case-1-introduction",
    "title": "A No Nonsense Guide to Reading a Confusion Matrix",
    "section": "Case 1: Introduction",
    "text": "Case 1: Introduction\n\nIgnore the “Actual” and “Predicted” labels for now.\nLet’s compare grizzly bears to black bears.\nAll comparisons begin at the bottom, with the columns.\nFirst, highlight the grizzly bear column.\n\nNext, highlight the black bear row.\n\nNow find the common entry in the highlighted column and row.\n\nThis common entry is our required information.\nAll entries in the diagonal going from the top left to the bottom right (blue) are correct classifications. All other entries are incorrect classifications.\nOur common entry does not lie in the main diagonal. Therefore, we are looking at incorrect classifications.\nWe have compared grizzly bears to black bears. Therefore, from this deduction, three grizzly bears have been incorrectly classified as black bears.\n\n\n\n\n\n\nNote\n\n\n\nThere is a difference between comparing grizzly bears to black bears and black bears to grizzly bears.\nComparing grizzly bears to black bears means, “How many grizzly bears were misclassified as black bears?”\nComparing black bears to grizzly bears means, “How many black bears were misclassified as grizzly bears?”"
  },
  {
    "objectID": "forblog/posts/3_the_confusion_matrix.html#sec-case2",
    "href": "forblog/posts/3_the_confusion_matrix.html#sec-case2",
    "title": "A No Nonsense Guide to Reading a Confusion Matrix",
    "section": "Case 2: Ultra Concise",
    "text": "Case 2: Ultra Concise\nLet’s compare black bears to grizzly bears.\nHighlight the black bear column.\n\nHighlight the grizzly bear row.\n\nHighlight the common entry.\n\nZero black bears were misclassified as grizzly bears."
  },
  {
    "objectID": "forblog/posts/3_the_confusion_matrix.html#case-3-correct-classifications",
    "href": "forblog/posts/3_the_confusion_matrix.html#case-3-correct-classifications",
    "title": "A No Nonsense Guide to Reading a Confusion Matrix",
    "section": "Case 3: Correct Classifications",
    "text": "Case 3: Correct Classifications\nLet’s see how many teddy bears were correctly classified. We are essentially comparing teddy bears to teddy bears.\nHighlight the teddy bear column.\n\nHighlight the teddy bear row.\n\nHighlight the common entry.\n\nFifty three teddy bears were correctly classified as teddy bears."
  },
  {
    "objectID": "forblog/posts/3_the_confusion_matrix.html#exercise-do-it-yourself",
    "href": "forblog/posts/3_the_confusion_matrix.html#exercise-do-it-yourself",
    "title": "A No Nonsense Guide to Reading a Confusion Matrix",
    "section": "Exercise: Do It Yourself",
    "text": "Exercise: Do It Yourself\nBelow is a confusion matrix of a car classifier that classifies cars into their brand.\n\nYou learn by doing!\n\nHow many Lamborghinis were correctly classified?\nHow many Jaguars were incorrectly classified?\nHow many Chevrolets were misclassified as Fords?\nHow many Fords were misclassified as Chevrolets?\nWhich two car brands did the model have the most trouble differentiating between?\n\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/index.html",
    "href": "forblog/index.html",
    "title": "Welcome to ForBlog by ForBo7",
    "section": "",
    "text": "Here you can experience my various ventures into learning, as well as read about other various useful tid-bits.\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nAI in a Nutshell\n\n\nThis nutshell contains very little math!\n\n\nAI models are much, much simpler than you think.\n\n\n\n\n\n\n04 October 2022\n\n\nSalman Naqvi\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nDetecting Floods for Disaster Relief\n\n\nHow good are you at detecting floods?\n\n\nA rundown of the creation of my flood classifier.\n\n\n\n\n\n\n12 September 2022\n\n\nSalman Naqvi\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nData Quality is Important | Car Classifier\n\n\nClassy Cars\n\n\nMost of the time, data matters more than the model.\n\n\n\n\n\n\n04 June 2022\n\n\nSalman Naqvi\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nA No Nonsense Guide to Reading a Confusion Matrix\n\n\nAre you confused yet?\n\n\nA straight to the point guide about reading a confusion matrix.\n\n\n\n\n\n\n03 June 2022\n\n\nSalman Naqvi\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nMy first AI model\n\n\nCan you bare reading through this bear classifier?\n\n\nA rundown on my first attempt at creating model.\n\n\n\n\n\n\n28 May 2022\n\n\nSalman Naqvi\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nHow to Approach Creating AI Models\n\n\nPutting the Drive into the Train\n\n\nThere’s more to AI than just creating models.\n\n\n\n\n\n\n27 May 2022\n\n\nSalman Naqvi\n\n\n4 min\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\nLoading…\n\nClick here to unsubscribe from the ForBlog."
  }
]