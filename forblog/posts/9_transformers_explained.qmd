---
title: Transformers, Simply Explained
subtitles: Autobots or Decepticons?
description: All transformers really do is fill in the blanks and autocomplete.
image: ../images/9_transformers_explained/thumbnail.png
author: Salman Naqvi
date: 2023-02-25
open-graph:
  description: Autobots or Decepticons?
  image: ../images/9_transformers_explained/thumbnail.png
twitter-card:
  description: Autobots or Decepticons?
  image: ../images/9_transformers_explained/thumbnail.png
draft: true
---

![](../images/9_transformers_explained/thumbnail.png){fig-alt="A picture of Transformers — the ones that transform from a robot into cars — posing as English alphabets."}

## At a High View

Simply put, a transformer is a type of architecture used for NLP tasks that to fills-in-the-blanks or autocompletes.

Transformers consist of either an encoder, decoder, or both. Encoders and decoders contain attention layers.

Language models need numbers to work. To give text a numerical representation, text is broken down into smaller pieces. To keep this explanation simple, these pieces are words.

## Encoders

Encoder-only transformers are good for "understanding" text, such as classifying sentences by sentiment or figuring out what parts of a sentence refers, for example, to a person or location.

When training encoders, words are given a numerical representation by the **attention layers** also considering adjacent words. The goal of training encoders is to predict words omitted from text (e.g., "I ... hungry."). This is how encoders can "understand" text.

## Decoders

Decoder-only transformers are good for text generation. An example is the autocomplete feature on a smartphone's keyboard.

Decoders similary give text a numerical representation, except that the attention layers consider only the words to the left, if working with English for example. The goal of training decoders is to predict the most likely word to continue a piece of text (e.g., "I am ...."). Each time a word is generated, that word is used in generating the proceeding words. This is how decoders can generate text.

## Encoders and Decoders

Transformers that use both encoders and decoders are known as encoder-decoder models or sequence-to-sequence models. Such models are good for translation and summarization.

Encoder-decoder models are trained by first letting the encoder give the input text a numerical representation. Next, this representation is input to the decoder which is used to generate text as described above. The encoder part of the model provides the "understanding", while the decoder part of the model generates based off of this "understanding". This is how such models are able to translate and summarize.

## Closing Words

And there you have it! It's as simple as that!

If you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!