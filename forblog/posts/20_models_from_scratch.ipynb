{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Implementing a Neural Network from Scratch\n",
    "subtitle: The DIY Guide to Digital Brain Building\n",
    "description: First I implement from scratch. Then I progressively reimplement with PyTorch. It is simpler than you think.\n",
    "image: ../images/20_model_from_scratch/thumbnail.jpeg\n",
    "author: Salman Naqvi\n",
    "date: '2024-05-26'\n",
    "categories: [Creating Models, Programming, Python, PyTorch]\n",
    "open-graph:\n",
    "  description: First I implement from scratch. Then I progressively reimplement with PyTorch. It is simpler than you think.\n",
    "  image: ../images/20_model_from_scratch/thumbnail.jpeg\n",
    "twitter-card:\n",
    "  description: First I implement from scratch. Then I progressively reimplement with PyTorch. It is simpler than you think.\n",
    "  image: ../images/20_model_from_scratch/thumbnail.jpeg\n",
    "format:\n",
    "  html: default\n",
    "  # ipynb: default\n",
    "filters:\n",
    "  - line-highlight\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This notebook follows the [fastai style guide](https://docs.fast.ai/dev/style.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/20_model_from_scratch/thumbnail.jpeg){fig-alt=\"The image features a person engaged in wood carving, with their face hidden by a color block. They are wearing an orange robe and using a chisel to create intricate designs on wood. The setting blends elements of a forest and architectural columns, with dramatic lighting that accentuates the carver and their craft. The scene evokes a sense of both indoor and outdoor space, with a focus on the artistry of wood carving.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will implement a neural network from scratch, and iteratively reimplement with PyTorch. That is, I will implement each element of the training and inference process from scratch, before then using the corresponding element in PyTorch. This notebook assumes a prior understanding of the flow and pieces of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, the complete training loop of a neural network looks like this.\n",
    "\n",
    "::::{.grid}\n",
    "\n",
    ":::{.g-col-4}\n",
    ":::\n",
    "\n",
    ":::{.g-col-4}\n",
    "```{mermaid}\n",
    "graph TB\n",
    "  A[Load Data] --> B[Make Predictions] --> C[Compute Loss] --> D[Compute Gradients] --> E[Update Weights] --> F[Compute Metric] --> A\n",
    "```\n",
    "::: \n",
    "\n",
    ":::{.g-col-4}\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "This notebook also serves to show the modular nature of PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started with some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of our model will be to classify digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "MNIST_URL = 'https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\n",
    "d_path = Path('data')\n",
    "d_path.mkdir(exist_ok=True)\n",
    "d_path = d_path/'mnist.pkl.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "if not d_path.exists(): urlretrieve(MNIST_URL, d_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 33312\n",
      "-rw-r--r--  1 salmannaqvi  staff  17051982 May 12 12:37 mnist.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "! ls -l data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, pickle\n",
    "from torch import tensor\n",
    "with gzip.open(d_path, 'rb') as f: ((trn_x, trn_y), (vld_x, vld_y), _) = pickle.load(f, encoding='latin-1')\n",
    "trn_x, trn_y, vld_x, vld_y = map(tensor, [trn_x[:1000], trn_y[:1000], vld_x[:1000], vld_y[:1000]]) # <1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Taking 1000 samples each for the sake of speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Single Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neuron comprises of a set of weights, the linear function, and the activation function.\n",
    "\n",
    "::::{.grid}\n",
    "\n",
    ":::{.g-col-4}\n",
    ":::\n",
    "\n",
    ":::{.g-col-4}\n",
    "```{mermaid}\n",
    "graph LR\n",
    "  A1[Weights]\n",
    "  A2[Inputs]\n",
    "  A1 & A2 --> B[Linear Combination] --> C[Activation Function]\n",
    "```\n",
    ":::\n",
    "\n",
    ":::{.g-col-4}\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains one thousand 28x28 pixel samples. Therefore, each sample has 28x28=784 inputs. Since we will be classifying digits, there will be 10 outputsâ€“a probablity for each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784, tensor(10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, m = trn_x.shape\n",
    "c = trn_y.max() + 1\n",
    "n, m, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have 50 neurons comprise the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these dimensions, we can create our appropriate weights..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 50]), torch.Size([50]), torch.Size([50, 1]), torch.Size([1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch; torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "w1, b1 = torch.randn(m, nh), torch.zeros(nh)\n",
    "w2, b2 = torch.randn(nh, 1), torch.zeros(1)\n",
    "w1.shape, b1.shape, w2.shape, b2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and create our linear model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b): return x @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 50])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = lin(vld_x, w1, b1); t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 784]), torch.Size([784, 50]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vld_x.shape, w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.all import *\n",
    "import torch.nn.functional as F\n",
    "test_eq(lin(vld_x, w1, b1), F.linear(vld_x, w1.T, b1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation produces the same outputs as PyTorch's implementation.\n",
    "\n",
    "We now need to implement the activation function, which will be the ReLU (rectified linear unit). Any value less than 0 gets clipped to 0. There are multiple ways we can approach doing this, such as using `torch.max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "max(input) -> Tensor\n",
      "\n",
      "Returns the maximum value of all elements in the ``input`` tensor.\n",
      "\n",
      ".. warning::\n",
      "    This function produces deterministic (sub)gradients unlike ``max(dim=0)``\n",
      "\n",
      "Args:\n",
      "    input (Tensor): the input tensor.\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> a = torch.randn(1, 3)\n",
      "    >>> a\n",
      "    tensor([[ 0.6763,  0.7445, -2.2369]])\n",
      "    >>> torch.max(a)\n",
      "    tensor(0.7445)\n",
      "\n",
      ".. function:: max(input, dim, keepdim=False, *, out=None) -> (Tensor, LongTensor)\n",
      "   :noindex:\n",
      "\n",
      "Returns a namedtuple ``(values, indices)`` where ``values`` is the maximum\n",
      "value of each row of the :attr:`input` tensor in the given dimension\n",
      ":attr:`dim`. And ``indices`` is the index location of each maximum value found\n",
      "(argmax).\n",
      "\n",
      "If ``keepdim`` is ``True``, the output tensors are of the same size\n",
      "as ``input`` except in the dimension ``dim`` where they are of size 1.\n",
      "Otherwise, ``dim`` is squeezed (see :func:`torch.squeeze`), resulting\n",
      "in the output tensors having 1 fewer dimension than ``input``.\n",
      "\n",
      ".. note:: If there are multiple maximal values in a reduced row then\n",
      "          the indices of the first maximal value are returned.\n",
      "\n",
      "Args:\n",
      "    input (Tensor): the input tensor.\n",
      "    dim (int): the dimension to reduce.\n",
      "    keepdim (bool): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.\n",
      "\n",
      "Keyword args:\n",
      "    out (tuple, optional): the result tuple of two output tensors (max, max_indices)\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> a = torch.randn(4, 4)\n",
      "    >>> a\n",
      "    tensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n",
      "            [ 1.1949, -1.1127, -2.2379, -0.6702],\n",
      "            [ 1.5717, -0.9207,  0.1297, -1.8768],\n",
      "            [-0.6172,  1.0036, -0.6060, -0.2432]])\n",
      "    >>> torch.max(a, 1)\n",
      "    torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))\n",
      "\n",
      ".. function:: max(input, other, *, out=None) -> Tensor\n",
      "   :noindex:\n",
      "\n",
      "See :func:`torch.maximum`.\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "#| column: margin\n",
    "?torch.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 3, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(tensor([-5, 2, 3, -4]), tensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return torch.max(x, tensor([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is to use `torch.clamp_min`, which is more idiomatic for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = lin(vld_x, w1, b1)\n",
    "test_eq(relu(t), F.relu(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single neuron can now be constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xb):\n",
    "  l1 = relu(lin(xb, w1, b1))\n",
    "  return lin(l1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model(vld_x); res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the forward pass being implemented, it is time to determine the loss. Even though we have a multi-class classification problem at hand, I will use mean squared error for simplicity. Later in this post, I will switch to cross entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mean Squared Error (MSE) between two vectors can be represented as:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{\\sum_{i=1}^{n} (y_i - x_i)^2}{n}\n",
    "$$\n",
    "\n",
    "where $x$ and $y$ are vectors of length $n$, and $x_i$ and $y_i$ represent the $i$-th elements of the vectors.\n",
    "\n",
    "::: {.callout-note appearance=\"simple\"}\n",
    "MSE in its most basic form looks like this.\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{(y - x)^2}{1}\n",
    "$$\n",
    "\n",
    "If we have multiple data points, then it looks like this.\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{(y_1 - x_1)^2+(y_2 - x_2)^2+(y_3 - x_3)^2}{3}\n",
    "$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor holding the predictions and the tensor holding the targets have different shapes. Therefore, there are different ways in which both can be subtracted from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 1]), torch.Size([1000]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape, vld_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vld_y - res).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vld_y[:, None] - res).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000]), torch.Size([1000]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:, 0].shape, res.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vld_y - res[:, 0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it will be better to add a column to `vld_y` rather than remove a column from `res`, so as to keep the shape of all tensors consistent (i.e., all tensors having a row and column, as opposed to some having rows and columns, and others having only a column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(717.17)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((vld_y[:, None] - res)**2).sum() / res.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(preds, targs): return (targs[:, None] - preds).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(648.87)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(trn_x); mse(preds, trn_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(mse(preds, trn_y), F.mse_loss(preds, trn_y[:, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the backward pass; the pass responsible for computing the gradients of our model's weights.\n",
    "\n",
    "For brevity, I will not explain why I compute the gradients the way I do. It can be taken that the way I compute them is due to the result of calculating the derivatives of the foward pass by hand. If you would like to explore how I did so, you can refer to my other blog post, [Backpropagation Explained using English Words*](18_backprop_from_scratch.qmd).\n",
    "\n",
    "In short, the derivatives compute to be the following.\n",
    "\n",
    ":::{.callout}\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\frac{\\partial \\text{MSE}}{\\partial \\vec{\\rm{w}}_1} &=\n",
    "    \\begin{cases}\n",
    "      0 & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 â‰¤ 0 \\\\\n",
    "      \\frac{2}{N} \\sum^N_{i=1} (\\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2 - \\vec{\\rm{y}}_i) \\cdot \\vec{\\rm{w}}^T_2 \\cdot \\vec{\\rm{x}}_i^T & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 > 0\n",
    "    \\end{cases} \\\\\n",
    "  \\frac{\\partial \\text{MSE}}{\\partial b_1} &=\n",
    "    \\begin{cases}\n",
    "      0 & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 â‰¤ 0 \\\\\n",
    "      \\frac{2}{N} \\sum^N_{i=1} (\\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2 - \\vec{\\rm{y}}_i) \\cdot \\vec{\\rm{w}}_2^T & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 > 0\n",
    "    \\end{cases} \\\\\n",
    "  \\frac{\\partial \\text{MSE}}{\\partial \\vec{\\rm{x}}_i} &=\n",
    "    \\begin{cases}\n",
    "      0 & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 â‰¤ 0 \\\\\n",
    "      \\frac{2}{N} \\sum^N_{i=1} (\\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2 - \\vec{\\rm{y}}_i) \\cdot \\vec{\\rm{w}}^T_2 \\cdot \\vec{\\rm{w}}_1^T & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 > 0\n",
    "    \\end{cases} \\\\\n",
    "  \\frac{\\partial \\text{MSE}}{\\partial \\vec{\\rm{w}}_2} &= \\frac{2}{N} \\sum^N_{i=1} (\\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2 - \\vec{\\rm{y}}_i) \\cdot \\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\\\\n",
    "  \\frac{\\partial \\text{MSE}}{\\partial b_2} &= \\frac{2}{N} \\sum^N_{i=1} \\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2 - \\vec{\\rm{y}}_i\n",
    "\\end{align}\n",
    "$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing backpropagation, it is better to implement the entire equation in pieces, by storing the result of each intermediate gradient. These intermediate gradients can then be reused to calculate the gradients of another variable.\n",
    "\n",
    "Let's prepare the pieces we'll need and get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(648.87)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = relu(lin(trn_x, w1, b1))\n",
    "l2 = lin(l1, w2, b2)\n",
    "loss = mse(l2, trn_y); loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `w1` Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the maths to compute the gradients for `w1`, as also shown above.\n",
    "\n",
    "$$\n",
    "  \\frac{\\partial \\text{MSE}}{\\partial \\vec{\\rm{w}}_1} =\n",
    "    \\begin{cases}\n",
    "      0 & \\text{if } \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 \\leq 0 \\\\\n",
    "      \\frac{2}{N} \\sum^N_{i=1} (\\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2 - \\vec{\\rm{y}}_i) \\cdot \\vec{\\rm{w}}^T_2 \\cdot \\vec{\\rm{x}}_i^T & \\text{if } \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 > 0\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "::::{.callout-note appearance=\"simple\"}\n",
    "Here, you can see the individual pieces I will compute to implement this equation.\n",
    "\n",
    ":::{.callout-note appearance=\"minimal\" collapse=\"true\"}\n",
    "## Substitutions\n",
    "$$\n",
    "\\begin{align}\n",
    "    u_1 &= \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 \\\\\n",
    "    u_2 &= \\text{max}(0, u_1) \\\\\n",
    "    u_3 &= u_2 \\cdot \\vec{\\rm{w}}_2 + b_2 \\\\\n",
    "    u_4 &= \\vec{\\rm{y}}_i - u_3 \\\\\n",
    "    u_5 &= u_4^2\n",
    "\\end{align}\n",
    "$$\n",
    ":::\n",
    "\n",
    ":::{.callout-note appearance=\"minimal\" collapse=\"true\"}\n",
    "## Derivatives of the Substitutions\n",
    "$$\n",
    "\\begin{alignat}{4}\n",
    "  \\text{gradient of } u_4 &= \\frac{\\partial u_5}{\\partial u_4} &&=\n",
    "  \\frac{\\partial}{\\partial u_4} u_4^2 &&&= 2u_4 \\\\\n",
    "  \\text{gradient of } u_3 &= \\frac{\\partial u_4}{\\partial u_3} &&= \\frac{\\partial}{\\partial u_3} \\vec{\\rm{y}}_i - u_3 &&&= -1 \\\\\n",
    "  \\text{gradient of } u_2 &= \\frac{\\partial u_3}{\\partial u_2} &&= \\frac{\\partial}{\\partial u_2} u_2 \\cdot \\vec{\\rm{w}}_2 + b_2 &&&= \\vec{\\rm{w}}^T_2 \\\\\n",
    "  \\text{gradient of } u_1 &= \\frac{\\partial \\vec{\\rm{u}}_2}{\\partial \\vec{\\rm{u}}_1} &&= \\frac{\\partial}{\\partial u_1} \\text{max}(0, u_1) &&&= \n",
    "    \\begin{cases}\n",
    "      0 & \\vec{\\rm{u}}_1 â‰¤ 0 \\\\\n",
    "      1 & \\vec{\\rm{u}}_1 > 0\n",
    "    \\end{cases} \\\\\n",
    "  \\text{gradient of } \\vec{\\rm{w}}_1 &= \\frac{\\partial u_1}{\\partial \\vec{\\rm{w}}_1} &&= \\frac{\\partial}{\\partial w_1} \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 &&&= \\vec{\\rm{x}}^T_i\n",
    "\\end{alignat}\n",
    "$$\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = trn_y[:, None] - l2; diff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(648.87), torch.Size([1000, 1]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.g = (2/n) * diff; loss, loss.g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-15.34],\n",
       "         [-33.46],\n",
       "         [-35.26],\n",
       "         [ -6.92],\n",
       "         [-21.55]]),\n",
       " torch.Size([1000, 1]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.g = -1 * loss.g; diff[:5], diff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((torch.Size([50, 1]), torch.Size([1000, 1])),\n",
       " (torch.Size([1, 50]), torch.Size([1000, 1, 1])))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(w2.shape, diff.g.shape), (w2.T.shape, diff.g[:, None].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 50])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(diff.g @ w2.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 50])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2.g = diff.g @ w2.T; l2.g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 1., 0., 0.],\n",
       "        [1., 1., 1.,  ..., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [1., 1., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(l1 > 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 50])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.g = l2.g * (l1 > 0).float(); l1.g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((torch.Size([1000, 50]), torch.Size([1000, 784])),\n",
       " (torch.Size([1000, 1, 50]), torch.Size([1000, 784, 1])))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(l1.g.shape, trn_x.shape), (l1.g[:, None, :].shape, trn_x[..., None].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.g = tensor([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 50])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.g = (l1.g[:, None, :] * trn_x[..., None]).sum(0); w1.g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((torch.Size([784, 50]), torch.Size([784, 50])),\n",
       " (tensor(-17.50), tensor(25.09)))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(w1.shape, w1.g.shape), (w1.g.min(), w1.g.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify our derivation is correct by comparing it to the gradients computed by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_ = w1.clone().requires_grad_();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = relu(lin(trn_x, w1_, b1))\n",
    "l2 = lin(l1, w2, b2)\n",
    "loss = mse(l2, trn_y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1_.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor(-17.50), tensor(25.09)), (tensor(-17.50), tensor(25.09)))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(w1.g.min(), w1.g.max()), (w1_.grad.min(), w1_.grad.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w1.g, w1_.grad, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `b1` Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, I can reuse the computed gradients to calculate the gradients for $b_1$. For now though, I will show the entire implemention for easy reference and later, when we will encapsulate the backward pass, I will reuse the already computed gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 50]), torch.Size([50]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = trn_y[:, None] - l2\n",
    "loss.g = (2/n) * diff\n",
    "diff.g = loss.g * -1\n",
    "l2.g = diff.g @ w2.T\n",
    "l1.g = l2.g * (l1 > 0).float()\n",
    "l1.g.shape, b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.g = (l1.g * 1).sum(0); b1.g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.min(), b1.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `trn_x` Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 50]), torch.Size([784, 50]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = trn_y[:, None] - l2\n",
    "loss.g = (2/n) * diff\n",
    "diff.g = loss.g * -1\n",
    "l2.g = diff.g @ w2.T\n",
    "l1.g = l2.g * (l1 > 0).float()\n",
    "l1.g.shape, w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_x.g = l1.g @ w1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.85, grad_fn=<MinBackward1>), tensor(2.85, grad_fn=<MaxBackward1>))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_x.g.min(), trn_x.g.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `w1` Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 1]), torch.Size([1000, 50]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = trn_y[:, None] - l2\n",
    "loss.g = (2/n) * diff\n",
    "diff.g = loss.g * -1\n",
    "diff.g.shape, l1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(diff.g * l1).sum(0, keepdim=True).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(diff.g[:, None, :] * l1[..., None]).sum(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.g = (diff.g[:, None, :] * l1[..., None]).sum(0); w2.g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(8.37, grad_fn=<MinBackward1>), tensor(388.44, grad_fn=<MaxBackward1>))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.g.min(), w2.g.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `b2` Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1]), torch.Size([1]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = trn_y[:, None] - l2\n",
    "loss.g = (2/n) * diff\n",
    "diff.g = loss.g * -1\n",
    "b2.g = (diff.g * 1).sum(0)\n",
    "b2.g.shape, b2.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify our remaining gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: margin\n",
    "w1_, b1_, w2_, b2_, trn_x_ = [lambda w: w.clone.requires_grad_() for w in [w1, b1, w2, b2, trn_x]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.column-margin}\n",
    "The expression above does not work to create copies. Rather than returning a cloned copy that requires gradients, lambda objects will be returned.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_, b1_, w2_, b2_, trn_x_ = map(lambda w: w.clone().requires_grad_(), [w1, b1, w2, b2, trn_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.81, -1.72, -0.97,  ..., -0.29, -1.62, -0.45],\n",
       "        [-1.77, -0.17,  1.32,  ..., -0.92,  0.76,  2.77],\n",
       "        [ 0.58,  2.13, -0.98,  ...,  0.41,  1.50,  0.86],\n",
       "        ...,\n",
       "        [-0.50, -1.90, -0.10,  ..., -1.61,  0.78, -0.09],\n",
       "        [ 0.89,  0.50,  1.21,  ...,  0.93, -0.37, -0.85],\n",
       "        [ 0.57, -0.50, -1.47,  ...,  0.72,  1.64, -0.85]], requires_grad=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| column: margin\n",
    "#| echo: false\n",
    "w1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.81, -1.72, -0.97,  ..., -0.29, -1.62, -0.45],\n",
       "        [-1.77, -0.17,  1.32,  ..., -0.92,  0.76,  2.77],\n",
       "        [ 0.58,  2.13, -0.98,  ...,  0.41,  1.50,  0.86],\n",
       "        ...,\n",
       "        [-0.50, -1.90, -0.10,  ..., -1.61,  0.78, -0.09],\n",
       "        [ 0.89,  0.50,  1.21,  ...,  0.93, -0.37, -0.85],\n",
       "        [ 0.57, -0.50, -1.47,  ...,  0.72,  1.64, -0.85]], requires_grad=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = relu(lin(trn_x_, w1_, b1_))\n",
    "l2 = lin(l1, w2_, b2_)\n",
    "loss = mse(l2, trn_y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in zip((w1, b1, w2, b2, trn_x), (w1_, b1_, w2_, b2_, trn_x_)): test_close(a.g, b.grad, eps=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All comparisons passed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encapsulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the forward and backward passes sorted, let us cohesively bring them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inps, targs):\n",
    "  l1 = relu(lin(inps, w1, b1))\n",
    "  l2 = lin(l1, w2, b2)\n",
    "  loss = mse(l2, targs)\n",
    "  return l1, l2, loss\n",
    "\n",
    "def backward(inps, targs, l1, l2, loss):\n",
    "  diff = targs[:, None] - l2\n",
    "  loss.g = (2 / n) * diff\n",
    "  diff.g = loss.g * -1\n",
    "\n",
    "  w2.g = (diff.g[:, None, :] * l1[..., None]).sum(0)\n",
    "  b2.g = (diff.g * 1).sum(0)\n",
    "\n",
    "  l2.g = diff.g @ w2.T\n",
    "  l1.g = l2.g * (l1 > 0).float()\n",
    "\n",
    "  w1.g = (l1.g[:, None, :] * trn_x[..., None]).sum(0)\n",
    "  b1.g = (l1.g * 1).sum(0)\n",
    "\n",
    "  inps.g = l1.g @ w1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, l2, loss = forward(trn_x, trn_y)\n",
    "backward(trn_x, trn_y, l1, l2, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_grads(*ws):\n",
    "  for a, b in zip(ws, (w1_, b1_, w2_, b2_, trn_x_)): test_close(a.g, b.grad, eps=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_grads(w1, b1, w2, b2, trn_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward` function can be further refactored by taking the gradient computations of the linear layers common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(inps, targs, l1, l2, loss):\n",
    "  diff = targs[:, None] - l2\n",
    "  loss.g = (2/n) * diff\n",
    "  diff.g = loss.g * -1\n",
    "\n",
    "  lin_grad(l1, diff, w2, b2)\n",
    "  l2.g = diff.g @ w2.T\n",
    "  l1.g = l2.g * (l1 > 0).float()\n",
    "  lin_grad(inps, l1, w1, b1)\n",
    "\n",
    "def lin_grad(inp, out, w, b):\n",
    "  inp.g = out.g @ w.T\n",
    "  w.g = (out.g[:, None, :] * inp[..., None]).sum(0)\n",
    "  b.g = (out.g * 1).sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{.column-margin}\n",
    "\n",
    "Previous implementation.\n",
    "\n",
    "```py\n",
    "def backward(inps, targs, l1, l2, loss):\n",
    "  diff = targs[:, None] - l2\n",
    "  loss.g = (2 / n) * diff\n",
    "  diff.g = loss.g * -1\n",
    "\n",
    "  w2.g = (diff.g[:, None, :] * l1[..., None]).sum(0)\n",
    "  b2.g = (diff.g * 1).sum(0)\n",
    "\n",
    "  l2.g = diff.g @ w2.T\n",
    "  l1.g = l2.g * (l1 > 0).float()\n",
    "\n",
    "  w1.g = (l1.g[:, None, :] * trn_x[..., None]).sum(0)\n",
    "  b1.g = (l1.g * 1).sum(0)\n",
    "\n",
    "  inps.g = l1.g @ w1.T\n",
    "  ```\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward(trn_x, trn_y, *forward(trn_x, trn_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_grads(w1, b1, w2, b2, trn_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we have functions that each separately handle a part of the network. For instance, `mse` only computes its respective portion of the forward pass: the mean squared error. `backward` is a separate function that handles the backward pass for all pieces of the network.\n",
    "\n",
    "Let us change how this works, so each piece of the network also handles its respective backward pass. This means, `mse` will have the ability to compute both its forward pass and backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "  def __call__(self, inp, targs):\n",
    "    self.inp,self.targs = inp,targs\n",
    "    self.out = (inp[:, 0] - targs).pow(2).mean()\n",
    "    return self.out\n",
    "  \n",
    "  def backward(self): self.inp.g = (2 / self.inp.shape[0]) * (self.inp[:, 0] - self.targs)[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(MSE()(preds, trn_y), mse(preds, trn_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin:\n",
    "  def __init__(self, w, b): self.w,self.b = w,b\n",
    "\n",
    "  def __call__(self, inp):\n",
    "    self.inp = inp\n",
    "    self.out = self.inp @ self.w + self.b\n",
    "    return self.out\n",
    "  \n",
    "  def backward(self):\n",
    "    self.inp.g = self.out.g @ self.w.T\n",
    "    self.w.g = (self.out.g[:, None, :] * self.inp[..., None]).sum(0)\n",
    "    self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(Lin(w1, b1)(trn_x), lin(trn_x, w1, b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "  def __call__(self, inp):\n",
    "    self.inp = inp\n",
    "    self.out = self.inp.clamp_min(0.)\n",
    "    return self.out\n",
    "  \n",
    "  def backward(self): self.inp.g = self.out.g * (self.inp > 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ReLU()(l1), relu(l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "  def __init__(self, w1, b1, w2, b2):\n",
    "    self.layers = [Lin(w1, b1), ReLU(), Lin(w2, b2)]\n",
    "    self.loss = MSE()\n",
    "  \n",
    "  def __call__(self, inp, targs):\n",
    "    for l in self.layers: inp = l(inp)\n",
    "    return self.loss(inp, targs)\n",
    "  \n",
    "  def backward(self):\n",
    "    self.loss.backward()\n",
    "    for l in self.layers[::-1]: l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)\n",
    "l = model(trn_x, trn_y)\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_grads(w1, b1, w2, b2, trn_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Super Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes we have created have common functionality, meaning their is still room for further refactoring. In particular, all the classes store the forward pass arguments as attributes if needed, have a `__call__` dunder method that exectutes the forward pass, and a `backward` method for the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "  def __call__(self, *args):\n",
    "    self.args = args\n",
    "    self.out = self.forward(*args)\n",
    "    return self.out\n",
    "  \n",
    "  def forward(self): raise Exception('Forward pass not implemented')\n",
    "  def backward(self): self.bwd(self.out, *self.args)\n",
    "  def bwd(self): raise Exception('Backward pass not implemented.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Module):\n",
    "  def forward(self, inp, targs): return (inp[:, 0] - targs).pow(2).mean()\n",
    "  def bwd(self, out, inp, targs): inp.g = (2 / inp.shape[0]) * (inp[:, 0] - targs)[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(MSE()(preds, trn_y), mse(preds, trn_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "  def __init__(self, w, b): self.w,self.b = w,b\n",
    "  def forward(self, inp): return inp @ self.w + self.b\n",
    "  def bwd(self, out, inp):\n",
    "    inp.g = out.g @ self.w.T\n",
    "    self.w.g = (out.g[:, None, :] * inp[..., None]).sum(0)\n",
    "    self.b.g = out.g.sum(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(Lin(w1, b1)(trn_x), lin(trn_x, w1, b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "  def forward(self, inp): return inp.clamp_min(0.)\n",
    "  def bwd(self, out, inp): inp.g = out.g * (inp > 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ReLU()(l1), relu(l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)\n",
    "loss = model(trn_x, trn_y)\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_grads(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that, this is the basic underlying paradigm in which PyTorch implements its components.\n",
    "\n",
    "So let us now directly use PyTorch's `nn.Module` to handle our components. There is an added benefit that `nn.Module` automatically keeps track of our gradients, so we do not need to implement the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch's `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 50]), 1000, 784, tensor(10), torch.Size([50]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.shape, n, m, c, b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Linear(nn.Module):\n",
    "  def __init__(self, n_inps, n_outs):\n",
    "    super().__init__()\n",
    "    self.w = torch.randn(n_inps, n_outs).requires_grad_()\n",
    "    self.b = torch.randn(n_outs).requires_grad_()\n",
    "  \n",
    "  def forward(self, inp): return inp @ self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = nn.functional\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, n_inp, nh, n_out):\n",
    "    super().__init__()\n",
    "    self.layers = [Linear(n_inp, nh), nn.ReLU(), Linear(nh, n_out)]\n",
    "  \n",
    "  def __call__(self, inp, targ):\n",
    "    for l in self.layers: inp = l(inp)\n",
    "    return F.mse_loss(inp, targ[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 1)\n",
    "loss = model(trn_x, trn_y.float())\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(), ReLU(), Linear()]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 42.11, -25.91,   0.15,  15.73, -16.16,  41.61,  13.73,  81.32,  -8.91,  55.30, -14.12, -82.24,  12.02, -27.58,  -9.48, -90.85,\n",
       "        -25.55,  34.89,  -0.68, -14.24,   4.73,  49.70, -27.02,  19.55,  10.14,  38.86,  30.55,  74.17,   2.15,  -2.62, -37.11,  14.04,\n",
       "        -12.12,   0.89,  -0.99,  -6.29,  -1.15,  12.26,  -9.73,  -4.13,  -1.53,   1.67,   1.34,  -9.78,  20.50,   7.30,  62.45,   5.94,\n",
       "         -3.28, -18.14])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l0 = model.layers[0]; l0.b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement a much more appropriate loss function for our multi-target problem: cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: Redefinition of `Model`, but without with loss function.\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, n_inps, nh, n_outs):\n",
    "    super().__init__()\n",
    "    self.layers = [nn.Linear(n_inps, nh), nn.ReLU(), nn.Linear(nh, n_outs)]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for l in self.layers: x = l(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 10])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(m, nh, c)\n",
    "preds = model(trn_x); preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I have defined [here](../../dictionary/terms/cross_entropy_loss.qmd), cross entropy loss simply involves taking the logarithm of the softmax function, and multiplying the results with the [one hot encoded](../../dictionary/terms/one_hot_encoding.qmd) targets.\n",
    "\n",
    "[Softmax](../../dictionary/terms/softmax.qmd), a multi-class generalization of the sigmoid function, involves taking the exponent of each prediction, and dividing each resulting value with the sum of all predictions to the exponent.\n",
    "\n",
    "$$\n",
    "\\text{S}(y_i) = \\frac{e^{y_i}}{\\sum_{j} e^{y_j}}\n",
    "$$\n",
    "\n",
    ":::{.callout-note appearance=\"simple\"}\n",
    "# Sigmoid Function Definition\n",
    "$$\n",
    "\\sigma(y) = \\frac{1}{1 + e^{-y}}\n",
    "$$\n",
    ":::\n",
    "\n",
    "Let's begin by first taking the logarithm of the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.40, -2.33, -2.25,  ..., -2.33, -2.40, -2.34],\n",
       "        [-2.37, -2.44, -2.21,  ..., -2.30, -2.34, -2.28],\n",
       "        [-2.37, -2.45, -2.16,  ..., -2.24, -2.40, -2.40],\n",
       "        ...,\n",
       "        [-2.36, -2.45, -2.20,  ..., -2.24, -2.39, -2.37],\n",
       "        [-2.34, -2.41, -2.28,  ..., -2.20, -2.53, -2.25],\n",
       "        [-2.43, -2.37, -2.21,  ..., -2.26, -2.40, -2.37]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log_softmax(x): return ((x.exp() / x.exp().sum(-1, keepdim=True))).log()\n",
    "log_softmax(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.40, -2.33, -2.25,  ..., -2.33, -2.40, -2.34],\n",
       "        [-2.37, -2.44, -2.21,  ..., -2.30, -2.34, -2.28],\n",
       "        [-2.37, -2.45, -2.16,  ..., -2.24, -2.40, -2.40],\n",
       "        ...,\n",
       "        [-2.36, -2.45, -2.20,  ..., -2.24, -2.39, -2.37],\n",
       "        [-2.34, -2.41, -2.28,  ..., -2.20, -2.53, -2.25],\n",
       "        [-2.43, -2.37, -2.21,  ..., -2.26, -2.40, -2.37]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(preds, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(log_softmax(preds).detach(), F.log_softmax(preds, dim=-1).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation involves division. According to the rule, $\\lg\\left(\\frac{a}{b}\\right) = \\lg(a) - \\lg(b)$, we can simplify our computation by subtracting the numerators and denominators instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x.exp().log() - x.exp().sum(-1, keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.40, -2.33, -2.25,  ..., -2.33, -2.40, -2.34],\n",
       "        [-2.37, -2.44, -2.21,  ..., -2.30, -2.34, -2.28],\n",
       "        [-2.37, -2.45, -2.16,  ..., -2.24, -2.40, -2.40],\n",
       "        ...,\n",
       "        [-2.36, -2.45, -2.20,  ..., -2.24, -2.39, -2.37],\n",
       "        [-2.34, -2.41, -2.28,  ..., -2.20, -2.53, -2.25],\n",
       "        [-2.43, -2.37, -2.21,  ..., -2.26, -2.40, -2.37]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation has an issue though: it is unstable. Anything involving exponents is inherently unstable. Have a large enough value, and we converge to infinity relatively quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e^0=1.0\n",
      "e^10=22026.46484375\n",
      "e^20=485165184.0\n",
      "e^30=10686474223616.0\n",
      "e^40=2.353852703404196e+17\n",
      "e^50=5.184705457665547e+21\n",
      "e^60=1.1420073962419164e+26\n",
      "e^70=2.515438700355918e+30\n",
      "e^80=5.540622484676759e+34\n",
      "e^90=inf\n",
      "e^100=inf\n"
     ]
    }
   ],
   "source": [
    "for x in range(0, 101, 10): print(f'e^{x}={torch.exp(tensor(x))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, there is trick to overcoming this known as the LogSumExp simplification.\n",
    "\n",
    "$$\n",
    "\\lg\\left(\\sum^n_{j=1} e^{x_j}\\right) = \\lg\\left(e^a \\sum^n_{j=1} \\frac{e^{x_j}}{e^a}\\right) = \\lg\\left(e^a \\sum^n_{j=1} e^{x_j - a}\\right) = a + \\lg\\left(\\sum^n_{j=1} e^{x_j - a}\\right)\n",
    "$$\n",
    "\n",
    "$a$ is the largest element in $x$.\n",
    "\n",
    "To begin, we need to get the largest value in each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000]), torch.Size([1000, 10]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max = preds.max(-1)[0]; max.shape, preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "max(input) -> Tensor\n",
      "\n",
      "Returns the maximum value of all elements in the ``input`` tensor.\n",
      "\n",
      ".. warning::\n",
      "    This function produces deterministic (sub)gradients unlike ``max(dim=0)``\n",
      "\n",
      "Args:\n",
      "    input (Tensor): the input tensor.\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> a = torch.randn(1, 3)\n",
      "    >>> a\n",
      "    tensor([[ 0.6763,  0.7445, -2.2369]])\n",
      "    >>> torch.max(a)\n",
      "    tensor(0.7445)\n",
      "\n",
      ".. function:: max(input, dim, keepdim=False, *, out=None) -> (Tensor, LongTensor)\n",
      "   :noindex:\n",
      "\n",
      "Returns a namedtuple ``(values, indices)`` where ``values`` is the maximum\n",
      "value of each row of the :attr:`input` tensor in the given dimension\n",
      ":attr:`dim`. And ``indices`` is the index location of each maximum value found\n",
      "(argmax).\n",
      "\n",
      "If ``keepdim`` is ``True``, the output tensors are of the same size\n",
      "as ``input`` except in the dimension ``dim`` where they are of size 1.\n",
      "Otherwise, ``dim`` is squeezed (see :func:`torch.squeeze`), resulting\n",
      "in the output tensors having 1 fewer dimension than ``input``.\n",
      "\n",
      ".. note:: If there are multiple maximal values in a reduced row then\n",
      "          the indices of the first maximal value are returned.\n",
      "\n",
      "Args:\n",
      "    input (Tensor): the input tensor.\n",
      "    dim (int): the dimension to reduce.\n",
      "    keepdim (bool): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.\n",
      "\n",
      "Keyword args:\n",
      "    out (tuple, optional): the result tuple of two output tensors (max, max_indices)\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> a = torch.randn(4, 4)\n",
      "    >>> a\n",
      "    tensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n",
      "            [ 1.1949, -1.1127, -2.2379, -0.6702],\n",
      "            [ 1.5717, -0.9207,  0.1297, -1.8768],\n",
      "            [-0.6172,  1.0036, -0.6060, -0.2432]])\n",
      "    >>> torch.max(a, 1)\n",
      "    torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))\n",
      "\n",
      ".. function:: max(input, other, *, out=None) -> Tensor\n",
      "   :noindex:\n",
      "\n",
      "See :func:`torch.maximum`.\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "#| column: margin\n",
    "?torch.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can simply implement the rest of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 10])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds - max[..., None]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.32],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.36],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.31],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.30],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.36],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.36],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.32],\n",
       "        [2.31],\n",
       "        [2.34],\n",
       "        [2.37],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.31],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.31],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.36],\n",
       "        [2.35],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.37],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.35],\n",
       "        [2.36],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.39],\n",
       "        [2.30],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.37],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.31],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.31],\n",
       "        [2.36],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.36],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.36],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.31],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.36],\n",
       "        [2.34],\n",
       "        [2.36],\n",
       "        [2.37],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.31],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.36],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.34],\n",
       "        [2.36],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.36],\n",
       "        [2.36],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.30],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.36],\n",
       "        [2.35],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.36],\n",
       "        [2.30],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.29],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.37],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.30],\n",
       "        [2.30],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.36],\n",
       "        [2.36],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.31],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.36],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.36],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.36],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.37],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.30],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.31],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.33],\n",
       "        [2.36],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.30],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.32],\n",
       "        [2.31],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.30],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.31],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.33],\n",
       "        [2.37],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.35],\n",
       "        [2.37],\n",
       "        [2.35],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.38],\n",
       "        [2.32],\n",
       "        [2.31],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.36],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.31],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.37],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.36],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.31],\n",
       "        [2.32],\n",
       "        [2.38],\n",
       "        [2.37],\n",
       "        [2.34],\n",
       "        [2.29],\n",
       "        [2.33],\n",
       "        [2.36],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.34],\n",
       "        [2.31],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.37],\n",
       "        [2.29],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.31],\n",
       "        [2.38],\n",
       "        [2.31],\n",
       "        [2.34],\n",
       "        [2.30],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.31],\n",
       "        [2.32],\n",
       "        [2.38],\n",
       "        [2.34],\n",
       "        [2.31],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.30],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.31],\n",
       "        [2.29],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.37],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.31],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.36],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.31],\n",
       "        [2.36],\n",
       "        [2.32],\n",
       "        [2.36],\n",
       "        [2.32],\n",
       "        [2.31],\n",
       "        [2.36],\n",
       "        [2.32],\n",
       "        [2.36],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.31],\n",
       "        [2.30],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.30],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.31],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.31],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.31],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.36],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.28],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.35],\n",
       "        [2.31],\n",
       "        [2.36],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.36],\n",
       "        [2.35],\n",
       "        [2.31],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.30],\n",
       "        [2.34],\n",
       "        [2.31],\n",
       "        [2.35],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.30],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.37],\n",
       "        [2.34],\n",
       "        [2.36],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.36],\n",
       "        [2.36],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.31],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.31],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.31],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.36],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.31],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.31],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.36],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.31],\n",
       "        [2.34],\n",
       "        [2.30],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.39],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.36],\n",
       "        [2.30],\n",
       "        [2.30],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.36],\n",
       "        [2.32],\n",
       "        [2.31],\n",
       "        [2.34],\n",
       "        [2.36],\n",
       "        [2.37],\n",
       "        [2.36],\n",
       "        [2.34],\n",
       "        [2.30],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.35],\n",
       "        [2.33],\n",
       "        [2.37],\n",
       "        [2.35],\n",
       "        [2.31],\n",
       "        [2.35],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.37],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.36],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.36],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.31],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.33],\n",
       "        [2.31],\n",
       "        [2.35],\n",
       "        [2.32],\n",
       "        [2.36],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.37],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.37],\n",
       "        [2.31],\n",
       "        [2.33],\n",
       "        [2.32],\n",
       "        [2.36],\n",
       "        [2.36],\n",
       "        [2.36],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.31],\n",
       "        [2.37],\n",
       "        [2.34],\n",
       "        [2.35],\n",
       "        [2.35],\n",
       "        [2.38],\n",
       "        [2.35],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.32],\n",
       "        [2.37],\n",
       "        [2.34],\n",
       "        [2.32],\n",
       "        [2.33],\n",
       "        [2.33]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: false\n",
    "# Output hidden to prevent endless scrolling.\n",
    "max[..., None] + (preds - max[..., None]).exp().sum(-1, keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(torch.exp(preds).sum(-1, keepdim=True).log(), max[..., None] + (preds - max[..., None]).exp().sum(-1, keepdim=True).log())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "  max = x.max(-1)[0]\n",
    "  return max[..., None] + (preds - max[..., None]).exp().sum(-1, keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logsumexp(preds).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(logsumexp(preds), preds.logsumexp(-1)[..., None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare how quicker our new implemenation is compared to the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337 Âµs Â± 75.8 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit log_softmax(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - logsumexp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 10])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax(preds).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 Âµs Â± 56 Âµs per loop (mean Â± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit log_softmax(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Much_ faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that is left now is to multiply our softmax predictions with the one hot encoded targets, and sum the resulting vector. However, due to the nature of our targets, we can employ a nifty trick that removes the need to create a tensor of one hot encoded targets: integer array indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer Array Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]); t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fancy name for a simple concept, integer array indexing allows one to access elements in a tensor by simply specifing lists of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 5, 9])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[[0, 1, 2], [0, 1, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is best to think of the tensor as a grid of coordinates, with the first coordinate representing the row, and the second coordinate representing the column. Elements 1, 5, and 9 are at (0, 0), (1, 1), and (2, 2).\n",
    "\n",
    "1, 6, and 8 are at (0, 0), (1, 2), and (2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 6, 8])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[[0, 1, 2], [0, 2, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 and 8 are at (0, 2) and (2, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 8])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[[0, 2], [2, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our targets consist of the integers from 0 to 9. Each row, or sample, in our predictions tensor represents a set of probabilites for each target.\n",
    "\n",
    "This means we can directly access the prediction for the correct target through integer array indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_y[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The targets for the first three samples are 5, 0, and, 4. Instead of manually specifying the targets when obtaining the predictions for the first three samples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 10])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_preds = log_softmax(preds); sm_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.27, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.37, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.26, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_preds[0, 5], sm_preds[1, 0], sm_preds[2, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...we can use the targets themselves to directly obtain our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.27, -2.37, -2.26], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_preds[[0, 1, 2], trn_y[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, our implementation can be completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(preds, targs): return -preds[range(targs.shape[0]), targs].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.30, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nll(sm_preds, trn_y); loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(F.nll_loss(F.log_softmax(preds, -1), trn_y), loss, 1e-3) # <1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The difference between `F.cross_entropy` and `F.nll_loss` is that the former expects the input to be the raw model outputs, where as the latter expects the input to already be logarithmic probabilities. It can be said that `F.nll_loss` computes cross entropy loss by starting at an intemediary step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we have all the components of a machine that is the neural network:\n",
    "\n",
    "- the linear function,\n",
    "- the activation function,\n",
    "- the loss function,\n",
    "- and the backward pass.\n",
    "\n",
    "It is time to get the machine up and running as a whole. It's time to get the training loop looping.\n",
    "\n",
    "::::{.grid}\n",
    "\n",
    ":::{.g-col-4}\n",
    ":::\n",
    "\n",
    ":::{.g-col-4}\n",
    "```{mermaid}\n",
    "graph TB\n",
    "  A[Load Data] --> B[Make Predictions] --> C[Compute Loss] --> D[Compute Gradients] --> E[Update Weights] --> F[Compute Metric] --> A\n",
    "```\n",
    ":::\n",
    "\n",
    ":::{.g-col-4}\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.08, -0.01,  0.08,  0.11, -0.02,  0.06,  0.13, -0.00, -0.08, -0.01], grad_fn=<SelectBackward0>),\n",
       " torch.Size([50, 10]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 50\n",
    "xb = trn_x[0:bs]\n",
    "preds = model(xb); preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9,\n",
       "        3, 9, 8, 5, 9, 3])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = trn_y[:bs]; yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.30, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [accuracy](../../dictionary/terms/accuracy.qmd) as our metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 2, 2, 2, 5, 2, 5, 2, 5, 2, 2, 2, 3, 2, 5, 5, 2, 2, 2, 5, 6, 3, 5, 2, 5, 2, 2, 3, 3, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 5, 2, 5, 5,\n",
       "        2, 2, 2, 2, 5, 5])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds.argmax(-1) == yb).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, yb): return ((preds.argmax(-1) == yb).sum()) / yb.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.10)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(accuracy(preds, yb), (preds.argmax(-1) == yb).float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(loss, preds, yb): print(f'Loss: {loss:.2f}; Accuracy: {accuracy(preds, yb):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.30; Accuracy: 0.10\n"
     ]
    }
   ],
   "source": [
    "lr, epochs = .5, 3\n",
    "xb, yb = trn_x[:bs], trn_y[:bs]\n",
    "preds = model(xb)\n",
    "report(loss_func(preds, yb), preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop can now be assembled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.01; Accuracy: 0.66\n",
      "Loss: 0.45; Accuracy: 0.88\n",
      "Loss: 0.37; Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "  for i in range(0, n, bs):\n",
    "    s = slice(i, min(n, bs+i))\n",
    "    xb, yb = trn_x[s], trn_y[s]\n",
    "    preds = model(xb)\n",
    "    loss = loss_func(preds, yb)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "      for l in model.layers:\n",
    "        if hasattr(l, 'weight'):\n",
    "          l.weight -= l.weight.grad * lr\n",
    "          l.bias   -= l.bias.grad * lr\n",
    "          l.weight.grad.zero_()\n",
    "          l.bias  .grad.zero_()\n",
    "  report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at how we slice: `s = slice(i, min(n, bs+i))`. We have to use `min` to prevent the slices from going out of bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "slice(stop)\n",
      "slice(start, stop[, step])\n",
      "\n",
      "Create a slice object.  This is used for extended slicing (e.g. a[0:10:2]).\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "#| column: margin\n",
    "?slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice(0, 50, None)\n",
      "slice(50, 100, None)\n",
      "slice(100, 150, None)\n",
      "slice(150, 200, None)\n",
      "slice(200, 250, None)\n",
      "slice(250, 300, None)\n",
      "slice(300, 350, None)\n",
      "slice(350, 400, None)\n",
      "slice(400, 450, None)\n",
      "slice(450, 500, None)\n",
      "slice(500, 550, None)\n",
      "slice(550, 600, None)\n",
      "slice(600, 650, None)\n",
      "slice(650, 700, None)\n",
      "slice(700, 750, None)\n",
      "slice(750, 800, None)\n",
      "slice(800, 850, None)\n",
      "slice(850, 900, None)\n",
      "slice(900, 950, None)\n",
      "slice(950, 1000, None)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, n, bs): print(slice(i, min(n, bs+i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply adding `bs` to `n` at the `end` parameter for `range` will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice(0, 50, None)\n",
      "slice(50, 100, None)\n",
      "slice(100, 150, None)\n",
      "slice(150, 200, None)\n",
      "slice(200, 250, None)\n",
      "slice(250, 300, None)\n",
      "slice(300, 350, None)\n",
      "slice(350, 400, None)\n",
      "slice(400, 450, None)\n",
      "slice(450, 500, None)\n",
      "slice(500, 550, None)\n",
      "slice(550, 600, None)\n",
      "slice(600, 650, None)\n",
      "slice(650, 700, None)\n",
      "slice(700, 750, None)\n",
      "slice(750, 800, None)\n",
      "slice(800, 850, None)\n",
      "slice(850, 900, None)\n",
      "slice(900, 950, None)\n",
      "slice(950, 1000, None)\n",
      "slice(1000, 1050, None)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, n+bs, bs): print(slice(i, bs+i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters & Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we update our weights by checking whether a layer in our network has a `weight` attribute.\n",
    "\n",
    "```py\n",
    "for epoch in range(epochs):\n",
    "  for i in range(0, n, bs):\n",
    "    s = slice(i, min(n, bs+i))\n",
    "    xb, yb = trn_x[s], trn_y[s]\n",
    "    preds = model(xb)\n",
    "    loss = loss_func(preds, yb)\n",
    "    loss.backward()\n",
    "    with torch.no_grad(): # <<\n",
    "      for l in model.layers: # <<\n",
    "        if hasattr(l, 'weight'): # <<\n",
    "          l.weight -= l.weight.grad * lr # <<\n",
    "          l.bias   -= l.bias.grad * lr # <<\n",
    "          l.weight.grad.zero_() # <<\n",
    "          l.bias  .grad.zero_() # <<\n",
    "  report(loss, preds, yb)\n",
    "```\n",
    "\n",
    "PyTorch actually keeps track which layers have weights. Let us explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, PyTorch knows that our model has a linear layer with 3 inputs and 4 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Module(\n",
       "  (foo): Linear(in_features=3, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = nn.Module()\n",
    "m1.foo = nn.Linear(3, 4); m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('foo', Linear(in_features=3, out_features=4, bias=True))]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(m1.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_children of Module(\n",
       "  (foo): Linear(in_features=3, out_features=4, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| include: false\n",
    "m1.named_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_children at 0x1509edcb0>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| include: false\n",
    "m1.named_children()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar manner, we can access the layer's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.37,  0.20, -0.39],\n",
       "         [-0.47,  0.00,  0.18],\n",
       "         [ 0.51, -0.35,  0.36],\n",
       "         [ 0.12,  0.10, -0.03]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.31, -0.42,  0.35,  0.16], requires_grad=True)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(m1.foo.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this approach will require us to loop through all layers to access all parameters. PyTorch instead provides a way to directly return the parameters of all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.37,  0.20, -0.39],\n",
       "         [-0.47,  0.00,  0.18],\n",
       "         [ 0.51, -0.35,  0.36],\n",
       "         [ 0.12,  0.10, -0.03]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.31, -0.42,  0.35,  0.16], requires_grad=True)]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(m1.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, n_inps, nh, n_outs):\n",
    "    super().__init__()\n",
    "    self.l1 = nn.Linear(n_inps, nh)\n",
    "    self.l2 = nn.Linear(nh, n_outs)\n",
    "    self.relu = nn.ReLU()\n",
    "  \n",
    "  def forward(self, x): return self.l2(self.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784, 50, tensor(10))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, m, nh, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=50, bias=True)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(m, nh, c); model.l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1: Linear(in_features=784, out_features=50, bias=True)\n",
      "l2: Linear(in_features=50, out_features=10, bias=True)\n",
      "relu: ReLU()\n"
     ]
    }
   ],
   "source": [
    "for name, l in model.named_children(): print(f'{name}: {l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 784])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters(): print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.03, -0.02,  0.03,  ...,  0.00,  0.03,  0.03],\n",
      "        [-0.03, -0.01, -0.00,  ...,  0.00, -0.02, -0.01],\n",
      "        [ 0.03,  0.01, -0.02,  ..., -0.03,  0.01,  0.01],\n",
      "        ...,\n",
      "        [ 0.03,  0.02,  0.02,  ...,  0.02,  0.03, -0.03],\n",
      "        [ 0.01,  0.03,  0.01,  ...,  0.02, -0.02, -0.01],\n",
      "        [-0.01, -0.02, -0.02,  ...,  0.03,  0.01,  0.03]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.00,  0.03,  0.03, -0.01,  0.03,  0.03,  0.00, -0.02,  0.02, -0.04,  0.00, -0.01,  0.03, -0.01, -0.02,  0.01,  0.01,  0.03, -0.03,\n",
      "         0.04,  0.03,  0.04, -0.02, -0.01, -0.03, -0.02,  0.02,  0.01, -0.02,  0.01, -0.03, -0.00,  0.01,  0.02, -0.02, -0.00,  0.00,  0.02,\n",
      "         0.03,  0.01,  0.01, -0.01,  0.00, -0.00,  0.00,  0.02,  0.01, -0.02, -0.01, -0.02], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.04, -0.05, -0.10,  0.12,  0.01,  0.11, -0.14, -0.12,  0.08, -0.13,  0.14, -0.06, -0.11,  0.05, -0.11,  0.06, -0.07, -0.12,\n",
      "         -0.05, -0.10, -0.06, -0.04,  0.11,  0.05,  0.13,  0.03,  0.08, -0.02,  0.11,  0.03,  0.08,  0.10, -0.13, -0.04, -0.00, -0.08,\n",
      "          0.08,  0.13, -0.07, -0.13,  0.05, -0.03,  0.04, -0.07, -0.11,  0.06,  0.09, -0.01,  0.11, -0.09],\n",
      "        [-0.01,  0.05, -0.09,  0.07,  0.05, -0.08,  0.01, -0.09, -0.10, -0.06,  0.13, -0.14, -0.07, -0.10, -0.13,  0.13,  0.01,  0.07,\n",
      "          0.08,  0.14, -0.12, -0.10, -0.09,  0.13, -0.00, -0.11,  0.09,  0.05,  0.05,  0.09, -0.11, -0.11,  0.14, -0.12, -0.07,  0.06,\n",
      "         -0.13,  0.00, -0.10, -0.06, -0.02,  0.09,  0.07, -0.02, -0.00, -0.01, -0.04, -0.14,  0.10, -0.03],\n",
      "        [-0.01, -0.05,  0.01,  0.07, -0.04, -0.09,  0.07,  0.12,  0.07, -0.09, -0.10,  0.03, -0.10,  0.04, -0.11, -0.12, -0.10,  0.01,\n",
      "         -0.09,  0.07, -0.05,  0.01, -0.03,  0.07, -0.04, -0.13, -0.11,  0.12,  0.01,  0.13, -0.02, -0.00, -0.09, -0.07,  0.14, -0.05,\n",
      "          0.07,  0.01, -0.11, -0.10, -0.07, -0.12, -0.13, -0.05, -0.11, -0.04,  0.04, -0.04, -0.06, -0.02],\n",
      "        [-0.07,  0.12,  0.01,  0.07,  0.01,  0.09, -0.02,  0.13,  0.10, -0.12,  0.01,  0.08,  0.01,  0.09, -0.04,  0.10, -0.06,  0.06,\n",
      "         -0.06,  0.02,  0.05, -0.02, -0.01, -0.13, -0.14,  0.04, -0.03,  0.06,  0.05, -0.07, -0.01, -0.04,  0.06,  0.08, -0.06,  0.00,\n",
      "          0.08, -0.14, -0.01, -0.13, -0.11, -0.00,  0.02, -0.10,  0.12, -0.14, -0.11, -0.01,  0.03, -0.09],\n",
      "        [ 0.09, -0.08, -0.10, -0.11, -0.04, -0.02,  0.04, -0.08, -0.12,  0.01,  0.10,  0.00, -0.11,  0.02, -0.10, -0.09, -0.00,  0.03,\n",
      "         -0.04,  0.14, -0.00, -0.07,  0.13,  0.12,  0.07,  0.03, -0.10,  0.13, -0.02, -0.14, -0.10,  0.13, -0.10, -0.09,  0.08, -0.03,\n",
      "         -0.10, -0.13, -0.04,  0.06,  0.09, -0.06,  0.13, -0.09, -0.03, -0.08,  0.06,  0.00, -0.08, -0.12],\n",
      "        [ 0.11,  0.03, -0.02,  0.04,  0.10, -0.09, -0.01, -0.12,  0.11,  0.03,  0.01, -0.05, -0.08, -0.08,  0.10,  0.03, -0.10, -0.12,\n",
      "         -0.12, -0.09,  0.04, -0.07, -0.12, -0.08,  0.07,  0.03,  0.07, -0.12,  0.08, -0.05, -0.11,  0.08, -0.14, -0.13, -0.12, -0.06,\n",
      "         -0.12, -0.04,  0.05,  0.12,  0.02,  0.01,  0.08, -0.10, -0.08,  0.08,  0.09, -0.02, -0.09,  0.14],\n",
      "        [ 0.03, -0.12,  0.09, -0.04, -0.06, -0.10, -0.02, -0.01, -0.02,  0.10,  0.03,  0.05,  0.00,  0.03,  0.08, -0.06, -0.11,  0.03,\n",
      "         -0.02, -0.08,  0.09, -0.07,  0.07, -0.02, -0.13,  0.01,  0.08,  0.13, -0.05, -0.04,  0.10,  0.12,  0.13, -0.08,  0.04, -0.08,\n",
      "         -0.05, -0.03,  0.14,  0.14,  0.13,  0.08, -0.01, -0.10,  0.14,  0.14, -0.04,  0.04,  0.13,  0.02],\n",
      "        [-0.05,  0.08, -0.05,  0.13,  0.09,  0.02,  0.08,  0.03, -0.11,  0.01, -0.07,  0.03,  0.00,  0.09,  0.02,  0.07, -0.11, -0.01,\n",
      "         -0.08, -0.03,  0.10, -0.06, -0.05, -0.03,  0.02, -0.09,  0.13, -0.01,  0.05,  0.11, -0.11, -0.13,  0.06, -0.12,  0.10,  0.04,\n",
      "          0.02, -0.07, -0.06, -0.09, -0.02, -0.11, -0.05,  0.08,  0.07,  0.13, -0.03, -0.06,  0.01,  0.10],\n",
      "        [-0.02,  0.00, -0.02, -0.05,  0.04, -0.14, -0.00,  0.05,  0.11,  0.11, -0.02, -0.13, -0.10, -0.02,  0.12,  0.06,  0.14, -0.05,\n",
      "          0.02,  0.05,  0.08, -0.08,  0.05,  0.14,  0.13, -0.11,  0.11,  0.08,  0.11, -0.00,  0.08, -0.01, -0.07,  0.11,  0.14, -0.11,\n",
      "         -0.07,  0.11,  0.03, -0.13, -0.03, -0.02, -0.11,  0.06,  0.02,  0.13, -0.09, -0.04, -0.01,  0.11],\n",
      "        [ 0.14,  0.01,  0.09,  0.02,  0.06,  0.13,  0.07, -0.05,  0.08,  0.01, -0.11,  0.11, -0.06,  0.12, -0.07, -0.02,  0.11, -0.13,\n",
      "         -0.01,  0.02, -0.14,  0.11,  0.01,  0.00, -0.06,  0.02, -0.04, -0.01, -0.06, -0.01,  0.09, -0.01,  0.08,  0.11, -0.09,  0.12,\n",
      "          0.07,  0.10, -0.05, -0.02, -0.11, -0.01, -0.03, -0.01, -0.01, -0.03,  0.01,  0.13, -0.08,  0.06]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.03,  0.05,  0.05, -0.01,  0.11,  0.00,  0.07,  0.08, -0.07,  0.04], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "for p in model.parameters(): print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we can directly access the parameters, we do not need to check whether a certain parameter exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| source-line-numbers: \"9-12\"\n",
    "def fit():\n",
    "  for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "      s = slice(i, min(n, bs+i))\n",
    "      xb, yb = trn_x[s], trn_y[s]\n",
    "      preds = model(xb)\n",
    "      loss = loss_func(preds, yb)\n",
    "      loss.backward()\n",
    "      with torch.no_grad():\n",
    "        for p in model.parameters(): p -= p.grad * lr\n",
    "        model.zero_grad() # <1>\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `torch.zero_grad()` can also be called directly on the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.84; Accuracy: 0.74\n",
      "Loss: 0.45; Accuracy: 0.88\n",
      "Loss: 0.37; Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement this functionalityâ€“where the model itself knows what its layers and parameters areâ€“ourselves.\n",
    "\n",
    "To do so, we will need to define the `__setattr__` dunder method, where any submodules defined are registered as parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule:\n",
    "  def __init__(self, n_inps, nh, n_outs):\n",
    "    self._modules = {}\n",
    "    self.l1 = nn.Linear(n_inps, nh)\n",
    "    self.l2 = nn.Linear(nh, n_outs)\n",
    "  \n",
    "  def __setattr__(self, k, v):\n",
    "    if not k.startswith('_'): self._modules[k] = v\n",
    "    super().__setattr__(k, v) # <1>\n",
    "  \n",
    "  def __repr__(self): return f'{self._modules}'\n",
    "\n",
    "  def parameters(self):\n",
    "    for l in self._modules.values(): yield from l.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `class MyModule` is actually `class MyModule(object)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)},\n",
       " MLP(\n",
       "   (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "   (l2): Linear(in_features=50, out_features=10, bias=True)\n",
       "   (relu): ReLU()\n",
       " ))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = MyModule(m, nh, c); mdl, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 784])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in mdl.parameters(): print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use our original approach, where a list of layers are specified, we can use the `add_module` method provided by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Module'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Adds a child module to the current module.\n",
      "\n",
      "The module can be accessed as an attribute using the given name.\n",
      "\n",
      "Args:\n",
      "    name (str): name of the child module. The child module can be\n",
      "        accessed from this module using the given name\n",
      "    module (Module): child module to be added to the module.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/mambaforge/envs/default/lib/python3.10/site-packages/torch/nn/modules/module.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "?nn.Module.add_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, layers):\n",
    "    super().__init__()\n",
    "    self.layers = layers\n",
    "    for i, l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n",
    "\n",
    "  def forward(self, x): return reduce(lambda val, layer: layer(val), self.layers, x) # <1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In essence, `reduce` uses the output of the function as input to the same function in the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "reduce(function, iterable[, initial]) -> value\n",
      "\n",
      "Apply a function of two arguments cumulatively to the items of a sequence\n",
      "or iterable, from left to right, so as to reduce the iterable to a single\n",
      "value.  For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates\n",
      "((((1+2)+3)+4)+5).  If initial is present, it is placed before the items\n",
      "of the iterable in the calculation, and serves as a default when the\n",
      "iterable is empty.\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "#| column: margin\n",
    "?reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x,y: x+y, [1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer_1): ReLU()\n",
       "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(layers); model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 10])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xb).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, `nn.ModuleList` can do the registration for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Holds submodules in a list.\n",
      "\n",
      ":class:`~torch.nn.ModuleList` can be indexed like a regular Python list, but\n",
      "modules it contains are properly registered, and will be visible by all\n",
      ":class:`~torch.nn.Module` methods.\n",
      "\n",
      "Args:\n",
      "    modules (iterable, optional): an iterable of modules to add\n",
      "\n",
      "Example::\n",
      "\n",
      "    class MyModule(nn.Module):\n",
      "        def __init__(self):\n",
      "            super().__init__()\n",
      "            self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n",
      "\n",
      "        def forward(self, x):\n",
      "            # ModuleList can act as an iterable, or be indexed using ints\n",
      "            for i, l in enumerate(self.linears):\n",
      "                x = self.linears[i // 2](x) + l(x)\n",
      "            return x\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/mambaforge/envs/default/lib/python3.10/site-packages/torch/nn/modules/container.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     ParametrizationList"
     ]
    }
   ],
   "source": [
    "#| column: margin\n",
    "?nn.ModuleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "  def __init__(self, layers):\n",
    "    super().__init__()\n",
    "    self.layers = nn.ModuleList(layers)\n",
    "  \n",
    "  def forward(self, x): return reduce(lambda x, layer: layer(x), self.layers, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SequentialModel(layers); model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.93; Accuracy: 0.78\n",
      "Loss: 0.52; Accuracy: 0.86\n",
      "Loss: 0.38; Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c)); model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.88; Accuracy: 0.74\n",
      "Loss: 0.48; Accuracy: 0.86\n",
      "Loss: 0.39; Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer is simply the name given to the algorithm that updates the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "  def __init__(self, params, lr=0.5): self.params,self.lr = list(params), lr\n",
    "\n",
    "  def step(self):\n",
    "    with torch.no_grad():\n",
    "      for p in self.params: p -= p.grad * self.lr\n",
    "  \n",
    "  def zero_grad(self):\n",
    "    for p in self.params: p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n",
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight update step can now be cleaned up by using `opt.step()` and `opt.zero_grad()` instead.\n",
    "\n",
    "\n",
    "```py\n",
    "def fit():\n",
    "  for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "      s = slice(i, min(n, i+bs))\n",
    "      xb, yb = trn_x[s], trn_y[s]\n",
    "      preds = model(xb)\n",
    "      loss = loss_func(preds, yb)\n",
    "      loss.backward()\n",
    "      with torch.no_grad(): # <<\n",
    "        for p in model.parameters(): p -= p.grad * lr # <<\n",
    "        model.zero_grad() # <<\n",
    "    report(loss, preds, yb)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "  for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "      s = slice(i, min(n, i+bs))\n",
    "      xb, yb = trn_x[s], trn_y[s]\n",
    "      preds = model(xb)\n",
    "      loss = loss_func(preds, yb)\n",
    "      loss.backward()\n",
    "      opt.step() # <<\n",
    "      opt.zero_grad() # <<\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.89; Accuracy: 0.74\n",
      "Loss: 0.51; Accuracy: 0.88\n",
      "Loss: 0.41; Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "def get_model():\n",
    "  model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n",
    "  return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.32, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "loss_func(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.82; Accuracy: 0.78\n",
      "Loss: 0.42; Accuracy: 0.90\n",
      "Loss: 0.35; Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I sometimes get confuzzled between the two terms, with regard to what each component actually does. The best way to think about these terms is that a dataset simply stores data in a massive warehouse, while a dataloader takes data from the dataset and tosses them into crates known as batches.\n",
    "\n",
    "As it currently is, we iterate through our dataset by obtaining a slice object, and then slicing out some data to form a batch.\n",
    "\n",
    "```py\n",
    "for i in range(0, n, bs):\n",
    "  s = slice(i, min(n, bs+i))\n",
    "  xb, yb = trn_x[s], trn_y[s]\n",
    "```\n",
    "\n",
    "We will now simplify how we approach this logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first point of simplification is to create a single dataset that will return both a sample and its associated target, from a single index. This will prevent us from having to index into two separate tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "  def __init__(self, x, y): self.x, self.y = x, y\n",
    "  def __len__(self): return len(self.x)\n",
    "  def __getitem__(self, i): return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds, vld_ds = Dataset(trn_x, trn_y), Dataset(vld_x, vld_y)\n",
    "assert len(trn_ds) == len(trn_x)\n",
    "assert len(vld_ds) == len(vld_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = trn_ds[0:5]\n",
    "assert xb.shape == (5, 28*28)\n",
    "assert yb.shape == (5,)\n",
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.19; Accuracy: 0.70\n",
      "Loss: 0.50; Accuracy: 0.88\n",
      "Loss: 0.34; Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "for epoch in range(epochs):\n",
    "  for i in range(0, n, bs):\n",
    "    xb, yb = trn_ds[i:min(n, bs+i)] # <<\n",
    "    preds = model(xb)\n",
    "    loss = loss_func(preds, yb)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "  report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now abstract away how the data from our datasets is loaded, by putting the logic that fetches data from the dataset...\n",
    "```py\n",
    "for i in range(0, n, bs):\n",
    "  xb, yb = trn_ds[i:min(n,i+bs)] # <<\n",
    "  ...\n",
    "```\n",
    "...into a class that we can call a dataloader.\n",
    "```py\n",
    "for xb, yb in train_dl: # <<\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "  def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n",
    "  def __iter__(self):\n",
    "    for i in range(0, len(self.ds), self.bs): yield self.ds[i:min(len(self.ds), i+self.bs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl, vld_dl = DataLoader(trn_ds, bs), DataLoader(vld_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([3, 8, 6, 9, 6, 4, 5, 3, 8, 4, 5, 2, 3, 8, 4, 8, 1, 5, 0, 5, 9, 7, 4, 1, 0, 3, 0, 6, 2, 9, 9, 4, 1, 3, 6, 8, 0, 7, 7, 6, 8, 9, 0, 3,\n",
       "         8, 3, 7, 7, 8, 4]))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(vld_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 784])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(vld_dl)); xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(xb[0].view(28, 28)); yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "  for epoch in range(epochs):\n",
    "    for xb, yb in trn_dl: # <<\n",
    "      preds = model(xb)\n",
    "      loss = loss_func(preds, yb)\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "      opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.79; Accuracy: 0.82\n",
      "Loss: 0.49; Accuracy: 0.84\n",
      "Loss: 0.30; Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that, we have abstracted our loading logic from three lines...\n",
    "```py\n",
    "for i in range(0, n, bs):\n",
    "  s = slice(i, min(n, bs+i))\n",
    "  xb, yb = trn_x[s], trn_y[s]\n",
    "  ...\n",
    "```\n",
    "...to a much more readable single line.\n",
    "```py\n",
    "for xb, yb in trn_dl:\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling is the method by which the dataloader selects indices from the dataset to load. Sampling from the training set should be random (due to the nature of our data), but not for the validation set.\n",
    "\n",
    "Therefore, we will need to create an additional class for the our dataloader; a component that tells the dataloader from which indices to load data from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Shuffle list x in place, and return None.\n",
      "\n",
      "Optional argument random is a 0-argument function returning a\n",
      "random float in [0.0, 1.0); if it is the default None, the\n",
      "standard random.random will be used.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/mambaforge/envs/default/lib/python3.10/random.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "import random\n",
    "?random.shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "  def __init__(self, ds, shuffle=False): self.n,self.shuffle = len(ds),shuffle\n",
    "  def __iter__(self):\n",
    "    res = list(range(self.n))\n",
    "    if self.shuffle: random.shuffle(res)\n",
    "    return iter(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Sampler at 0x150dddd80>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = Sampler(trn_ds); ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: margin\n",
    "try: print(next(ss))\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.column-margin}\n",
    "This does not work because `__iter__` is not being called. `__iter__` only gets called when we wrap the class with `iter()`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#| column: margin\n",
    "try: print(next(iter(ss)))\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<list_iterator at 0x150996fe0>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(ss); it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for o in range(5): print(next(it))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Sampler` currently returns a single index in each iteration. We need to change that so a number of indices (equal to our batch size) is returned in each iteration. We can do this through a fancy slicing function known as `islice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "islice(iterable, stop) --> islice object\n",
      "islice(iterable, start, stop[, step]) --> islice object\n",
      "\n",
      "Return an iterator whose next() method returns selected values from an\n",
      "iterable.  If start is specified, will skip all preceding elements;\n",
      "otherwise, start defaults to zero.  Step defaults to one.  If\n",
      "specified as another value, step determines how many values are\n",
      "skipped between successive calls.  Works like a slice() on a list\n",
      "but returns an iterator.\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "?islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iter` returns a single element from an iterable at a time. `islice` is a type of iterator that returns $x$ elements from an iterable at a time. It is an, erm, iterative slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(islice(ss, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an additional class that takes a sampler, and assembles its output into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSampler:\n",
    "  def __init__(self, sampler, bs, drop_last=False): store_attr()\n",
    "  def __iter__(self): yield from chunked(iter(self.sampler), self.bs, drop_last=self.drop_last) # <1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `fastcore`'s `chunked` function has the exact same functionality as `islice`, but with some extra quality of life features. This includes being able to specify how many chunks, or slices, we want back (rather than the number of elements in a chunk), as well as being able to specify whether we would like to drop, or keep, chunks that are smaller than our specified chunk size. This latter option is what we will useâ€“it will abstract away the `min` check we use in our `DataLoader` (`self.ds[i:min(len(self.ds), i+self.bs)]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_sz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_chunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_sz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_chunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"Return batches from iterator `it` of size `chunk_sz` (or return `n_chunks` total)\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32massert\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_sz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m^\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mn_chunks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchunk_sz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_sz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mchunk_sz\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mchunk_sz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/mambaforge/envs/default/lib/python3.10/site-packages/fastcore/basics.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "#| column: margin\n",
    "??chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(islice(ss, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4],\n",
       " [5, 6, 7, 8, 9],\n",
       " [10, 11, 12, 13, 14],\n",
       " [15, 16, 17, 18, 19],\n",
       " [20, 21, 22, 23, 24]]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(chunked(ss, 5))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3],\n",
       " [4, 5, 6, 7],\n",
       " [8, 9, 10, 11],\n",
       " [12, 13, 14, 15],\n",
       " [16, 17, 18, 19]]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = BatchSampler(ss, 4)\n",
    "list(islice(batches, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one last piece of the puzzle left. Each sample in our `Dataset` also stores its associated target. We need to split these apart when dataloading. In other words, we need to split the data and target in each sample into their own batches; into an `x` batch and a `y` batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "  xs, ys = zip(*b)\n",
    "  return torch.stack(xs), torch.stack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "  def __init__(self, ds, batches, collate_fn=collate): store_attr()\n",
    "  def __iter__(self): yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batches) # <<"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's breakdown the latter line and explore what it does, piece by piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_samp = BatchSampler(Sampler(trn_ds, shuffle=True), bs)\n",
    "vld_samp = BatchSampler(Sampler(vld_ds, shuffle=False), bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`for b in self.batches`, we loop through each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(vld_samp)); b[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`self.ds[i] for i in b`; using the indices in each batch, we access the respective samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [vld_ds[i] for i in b]; len(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen below, `p` also stores the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.18, 0.62, 0.76, 0.80, 0.28, 0.34, 0.05, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.05, 0.93, 0.99, 0.99, 0.99,\n",
       "         0.99, 0.99, 0.89, 0.33, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.05, 0.77, 0.69, 0.50, 0.69, 0.81, 0.92, 0.96, 0.87, 0.09, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.08, 0.54, 0.99, 0.37, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.30, 0.99,\n",
       "         0.56, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.07, 0.78, 0.99, 0.66, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.18, 0.85, 0.99, 0.84, 0.11, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.37, 0.88, 0.99, 0.96, 0.25, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.05, 0.50, 0.98, 0.99, 0.92,\n",
       "         0.16, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.67, 0.99, 0.99, 0.66, 0.23, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.81, 0.99, 0.99, 0.25, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.54, 0.99, 0.99, 0.98, 0.57, 0.10, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.04, 0.68, 0.88,\n",
       "         0.99, 0.99, 0.90, 0.28, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.03, 0.05, 0.99, 0.99, 0.99, 0.96, 0.41, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.18, 0.74, 0.99, 0.99, 0.88, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.04, 0.00, 0.00, 0.00, 0.00, 0.00, 0.07, 0.68, 0.99,\n",
       "         0.99, 0.10, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.14, 0.90, 0.61, 0.44,\n",
       "         0.34, 0.73, 0.75, 0.85, 0.99, 0.99, 0.86, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.47, 1.00, 0.99, 0.99, 0.99, 0.99, 1.00, 0.99, 0.99, 0.95, 0.26, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.54, 1.00, 0.99, 0.99, 0.99, 0.99, 1.00, 0.67, 0.18, 0.09, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.02, 0.28, 0.64, 0.74, 0.68, 0.68, 0.26, 0.02,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
       " tensor(3))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we simply run the collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3),\n",
       " tensor(8),\n",
       " tensor(6),\n",
       " tensor(9),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(8),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(8),\n",
       " tensor(4),\n",
       " tensor(8),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(9),\n",
       " tensor(7),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(6),\n",
       " tensor(2),\n",
       " tensor(9),\n",
       " tensor(9),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(6),\n",
       " tensor(8),\n",
       " tensor(0),\n",
       " tensor(7),\n",
       " tensor(7),\n",
       " tensor(6),\n",
       " tensor(8),\n",
       " tensor(9),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(8),\n",
       " tensor(3),\n",
       " tensor(7),\n",
       " tensor(7),\n",
       " tensor(8),\n",
       " tensor(4))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys = zip(*p); ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have our collated `x` and `y` batches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 8, 6, 9, 6, 4, 5, 3, 8, 4, 5, 2, 3, 8, 4, 8, 1, 5, 0, 5, 9, 7, 4, 1, 0, 3, 0, 6, 2, 9, 9, 4, 1, 3, 6, 8, 0, 7, 7, 6, 8, 9, 0, 3,\n",
       "        8, 3, 7, 7, 8, 4])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_samp = BatchSampler(Sampler(trn_ds, shuffle=True), bs)\n",
    "vld_samp = BatchSampler(Sampler(vld_ds, shuffle=False), bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = DataLoader(trn_ds, batches=trn_samp)\n",
    "vld_dl = DataLoader(vld_ds, batches=vld_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(vld_dl))\n",
    "plt.imshow(xb[0].view(28, 28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 784]), torch.Size([50]))"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.03; Accuracy: 0.74\n",
      "Loss: 0.46; Accuracy: 0.82\n",
      "Loss: 0.30; Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need to update the `fit()` function, as its logic remains the same despite our changes to the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrn_dl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# <<\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      /var/folders/fy/vg316qk1001227svr6d4d8l40000gn/T/ipykernel_52843/769712355.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "??fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can speed up how quickly data is loaded by using multiple CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227 ns Â± 1.45 ns per loop (mean Â± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "it = iter(trn_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "class DataLoader():\n",
    "  def __init__(self, ds, batches, n_workers=1, collate_fun=collate): store_attr()\n",
    "  def __iter__(self):\n",
    "    with mp.Pool(self.n_workers) as ex: yield from ex.map(self.ds.__getitem__, iter(self.batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = DataLoader(trn_ds, batches=trn_samp, n_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197 ns Â± 0.557 ns per loop (mean Â± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "it = iter(trn_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down how exactly our `__iter__` method works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We slice batches by specifying a list of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([1, 1, 1, 0]))"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds[[3, 6, 8, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, the square bracket notation calls the `__getitem__` dunder method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mtrn_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mSource:\u001b[0m      \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      /var/folders/fy/vg316qk1001227svr6d4d8l40000gn/T/ipykernel_52843/694427655.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "??trn_ds.__getitem__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we can index directly using `__getitem__`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([1, 1, 1, 0]))"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds.__getitem__([3, 6, 8, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, by dividing our batches into smaller sets, we can take advantage of the `__getitem__` dunder method to allow each CPU core to handle a separate set of items.\n",
    "\n",
    "So we can divide our batches into smaller sets that each CPU core can manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(map(trn_ds.__getitem__, ([3, 6], [8, 1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 1]))\n",
      "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 0]))\n"
     ]
    }
   ],
   "source": [
    "for o in map(trn_ds.__getitem__, ([3, 6], [8, 1])): print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mBatchSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSampler\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdrop_last\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Wraps another sampler to yield a mini-batch of indices.\n",
      "\n",
      "Args:\n",
      "    sampler (Sampler or Iterable): Base sampler. Can be any iterable object\n",
      "    batch_size (int): Size of mini-batch.\n",
      "    drop_last (bool): If ``True``, the sampler will drop the last batch if\n",
      "        its size would be less than ``batch_size``\n",
      "\n",
      "Example:\n",
      "    >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n",
      "    [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n",
      "    >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n",
      "    [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
      "\u001b[0;31mFile:\u001b[0m           ~/mambaforge/envs/default/lib/python3.10/site-packages/torch/utils/data/sampler.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler\n",
    "?BatchSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides a wrapper which assembles the indices, sampled by our desired sampler, into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdata_source\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreplacement\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_samples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Samples elements randomly. If without replacement, then sample from a shuffled dataset.\n",
      "If with replacement, then user can specify :attr:`num_samples` to draw.\n",
      "\n",
      "Args:\n",
      "    data_source (Dataset): dataset to sample from\n",
      "    replacement (bool): samples are drawn on-demand with replacement if ``True``, default=``False``\n",
      "    num_samples (int): number of samples to draw, default=`len(dataset)`.\n",
      "    generator (Generator): Generator used in sampling.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/mambaforge/envs/default/lib/python3.10/site-packages/torch/utils/data/sampler.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "?RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_samp = BatchSampler(    RandomSampler(trn_ds), bs, drop_last=False)\n",
    "vld_samp = BatchSampler(SequentialSampler(vld_ds), bs, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a dataloader with PyTorch, we have to provide the dataset and a sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mT_co\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_sampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSampler\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_workers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcollate_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdrop_last\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mworker_init_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmultiprocessing_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprefetch_factor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpersistent_workers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpin_memory_device\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Data loader. Combines a dataset and a sampler, and provides an iterable over\n",
      "the given dataset.\n",
      "\n",
      "The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
      "iterable-style datasets with single- or multi-process loading, customizing\n",
      "loading order and optional automatic batching (collation) and memory pinning.\n",
      "\n",
      "See :py:mod:`torch.utils.data` documentation page for more details.\n",
      "\n",
      "Args:\n",
      "    dataset (Dataset): dataset from which to load the data.\n",
      "    batch_size (int, optional): how many samples per batch to load\n",
      "        (default: ``1``).\n",
      "    shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
      "        at every epoch (default: ``False``).\n",
      "    sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
      "        samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
      "        implemented. If specified, :attr:`shuffle` must not be specified.\n",
      "    batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
      "        returns a batch of indices at a time. Mutually exclusive with\n",
      "        :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
      "        and :attr:`drop_last`.\n",
      "    num_workers (int, optional): how many subprocesses to use for data\n",
      "        loading. ``0`` means that the data will be loaded in the main process.\n",
      "        (default: ``0``)\n",
      "    collate_fn (Callable, optional): merges a list of samples to form a\n",
      "        mini-batch of Tensor(s).  Used when using batched loading from a\n",
      "        map-style dataset.\n",
      "    pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
      "        into device/CUDA pinned memory before returning them.  If your data elements\n",
      "        are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
      "        see the example below.\n",
      "    drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
      "        if the dataset size is not divisible by the batch size. If ``False`` and\n",
      "        the size of dataset is not divisible by the batch size, then the last batch\n",
      "        will be smaller. (default: ``False``)\n",
      "    timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
      "        from workers. Should always be non-negative. (default: ``0``)\n",
      "    worker_init_fn (Callable, optional): If not ``None``, this will be called on each\n",
      "        worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
      "        input, after seeding and before data loading. (default: ``None``)\n",
      "    generator (torch.Generator, optional): If not ``None``, this RNG will be used\n",
      "        by RandomSampler to generate random indexes and multiprocessing to generate\n",
      "        `base_seed` for workers. (default: ``None``)\n",
      "    prefetch_factor (int, optional, keyword-only arg): Number of batches loaded\n",
      "        in advance by each worker. ``2`` means there will be a total of\n",
      "        2 * num_workers batches prefetched across all workers. (default value depends\n",
      "        on the set value for num_workers. If value of num_workers=0 default is ``None``.\n",
      "        Otherwise if value of num_workers>0 default is ``2``).\n",
      "    persistent_workers (bool, optional): If ``True``, the data loader will not shutdown\n",
      "        the worker processes after a dataset has been consumed once. This allows to\n",
      "        maintain the workers `Dataset` instances alive. (default: ``False``)\n",
      "    pin_memory_device (str, optional): the data loader will copy Tensors\n",
      "        into device pinned memory before returning them if pin_memory is set to true.\n",
      "\n",
      "\n",
      ".. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
      "             cannot be an unpicklable object, e.g., a lambda function. See\n",
      "             :ref:`multiprocessing-best-practices` on more details related\n",
      "             to multiprocessing in PyTorch.\n",
      "\n",
      ".. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
      "             When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
      "             it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
      "             rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
      "             configurations. This represents the best guess PyTorch can make because PyTorch\n",
      "             trusts user :attr:`dataset` code in correctly handling multi-process\n",
      "             loading to avoid duplicate data.\n",
      "\n",
      "             However, if sharding results in multiple workers having incomplete last batches,\n",
      "             this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
      "             be broken into multiple ones and (2) more than one batch worth of samples can be\n",
      "             dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
      "             cases in general.\n",
      "\n",
      "             See `Dataset Types`_ for more details on these two types of datasets and how\n",
      "             :class:`~torch.utils.data.IterableDataset` interacts with\n",
      "             `Multi-process data loading`_.\n",
      "\n",
      ".. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n",
      "             :ref:`data-loading-randomness` notes for random seed related questions.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/mambaforge/envs/default/lib/python3.10/site-packages/torch/utils/data/dataloader.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "?DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = DataLoader(trn_ds, batch_sampler=trn_samp, collate_fn=collate)\n",
    "vld_dl = DataLoader(vld_dl, batch_sampler=vld_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.05; Accuracy: 0.64\n",
      "Loss: 0.69; Accuracy: 0.72\n",
      "Loss: 0.55; Accuracy: 0.84\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.02, grad_fn=<NllLossBackward0>), tensor(0.66))"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of separately wrapping the `RandomSampler` and `SequentialSampler` classes, we can let the `DataLoader` class do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = DataLoader(trn_ds, bs, sampler=    RandomSampler(trn_ds), collate_fn=collate)\n",
    "vld_dl = DataLoader(vld_ds, bs, sampler=SequentialSampler(trn_ds), collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we don't even need to specify the sampler. All we have to do is toggle and set some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = DataLoader(trn_ds, bs, shuffle=True, drop_last=True, num_workers=2)\n",
    "vld_dl = DataLoader(vld_ds, bs, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.80; Accuracy: 0.80\n",
      "Loss: 0.27; Accuracy: 0.94\n",
      "Loss: 0.40; Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model(); fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.84, grad_fn=<NllLossBackward0>), tensor(0.68))"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our dataset already knows how to sample a batch of indices all at once, we can actually skip the `batch_sampler` and `collate_fn` entirely. ðŸ™ƒ\n",
    "\n",
    "```py\n",
    "class Dataset():\n",
    "  def __init__(self, x, y): self.x, self.y = x, y\n",
    "  def __len__(self): return len(self.x)\n",
    "  def __getitem__(self, i): return self.x[i], self.y[i]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([9, 1, 3]))"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds[[4, 6, 7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = DataLoader(trn_ds, sampler=trn_samp)\n",
    "vld_dl = DataLoader(vld_ds, sampler=vld_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 50, 784]), torch.Size([1, 50]))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(trn_dl)); xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training and evaluating a model, `model.train()` and `model.eval()` need to be called respectively. These methods are used by layers such as `nn.BatchNorm2d` and `nn.Dropout` to ensure appropriate behaviour during different phases of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Sets the module in training mode.\n",
      "\n",
      "This has any effect only on certain modules. See documentations of\n",
      "particular modules for details of their behaviors in training/evaluation\n",
      "mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "etc.\n",
      "\n",
      "Args:\n",
      "    mode (bool): whether to set training mode (``True``) or evaluation\n",
      "                 mode (``False``). Default: ``True``.\n",
      "\n",
      "Returns:\n",
      "    Module: self\n",
      "\u001b[0;31mFile:\u001b[0m      ~/mambaforge/envs/default/lib/python3.10/site-packages/torch/nn/modules/module.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "?model.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Sets the module in evaluation mode.\n",
      "\n",
      "This has any effect only on certain modules. See documentations of\n",
      "particular modules for details of their behaviors in training/evaluation\n",
      "mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "etc.\n",
      "\n",
      "This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "\n",
      "See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "`.eval()` and several similar mechanisms that may be confused with it.\n",
      "\n",
      "Returns:\n",
      "    Module: self\n",
      "\u001b[0;31mFile:\u001b[0m      ~/mambaforge/envs/default/lib/python3.10/site-packages/torch/nn/modules/module.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "?model.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "      preds = model(xb)\n",
    "      loss = loss_func(preds, yb)\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "      opt.zero_grad()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      tot_loss, tot_acc, count = (0.,) * 3\n",
    "      for xb, yb in valid_dl:\n",
    "        preds = model(xb)\n",
    "        n = len(xb)\n",
    "        count += n\n",
    "        tot_loss += loss_func(preds, yb).item() * n\n",
    "        tot_acc  += accuracy (preds, yb).item() * n\n",
    "    print(epoch, tot_loss/count, tot_acc/count)\n",
    "  return tot_loss/count, tot_acc/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(trainn_ds, valid_ds, bs, **kwargs):\n",
    "  return (DataLoader(trn_ds, batch_size=bs,   shuffle=True, **kwargs),\n",
    "          DataLoader(vld_ds, batch_size=bs*2,               **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl, vld_dl = get_dls(trn_ds, vld_ds, bs)\n",
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.3015430688858032 0.6180000007152557\n",
      "1 0.7089294970035553 0.7680000007152558\n",
      "2 0.6260120451450348 0.7990000009536743\n",
      "3 0.501511612534523 0.8490000128746032\n",
      "4 0.5909725487232208 0.8119999945163727\n",
      "CPU times: user 1.55 s, sys: 41.8 ms, total: 1.59 s\n",
      "Wall time: 358 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss, acc = fit(5, model, loss_func, opt, trn_dl, vld_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
