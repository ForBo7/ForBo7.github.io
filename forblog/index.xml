<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>ForBo7 // Salman Naqvi</title>
<link>https://forbo7.github.io/forblog/index.html</link>
<atom:link href="https://forbo7.github.io/forblog/index.xml" rel="self" type="application/rss+xml"/>
<description>The world of ForBo7!</description>
<image>
<url>https://forbo7.github.io/images/profile.png</url>
<title>ForBo7 // Salman Naqvi</title>
<link>https://forbo7.github.io/forblog/index.html</link>
<height>144</height>
<width>144</width>
</image>
<generator>quarto-1.2.475</generator>
<lastBuildDate>Thu, 13 Apr 2023 00:00:00 GMT</lastBuildDate>
<item>
  <title>My Musings Through Stable Diffusion</title>
  <dc:creator>Salman Naqvi</dc:creator>
  <link>https://forbo7.github.io/forblog/posts/11_musings_through_stable_diffusion.html</link>
  <description><![CDATA[ 



<blockquote class="blockquote">
<p><strong>Quick tip:</strong> Click or tap the images to view them up close.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../images/11_musings_through_stable_diffusion/thumbnail.png" class="lightbox" title="An antique 18th century painting of a gorilla eating a plate of chips." data-gallery="quarto-lightbox-gallery-1"><img src="https://forbo7.github.io/forblog/images/11_musings_through_stable_diffusion/thumbnail.png" class="img-fluid figure-img" alt="An antique 18th century painting of a gorilla eating a plate of chips."></a></p>
</figure>
</div>
<p>I recently began <a href="https://course.fast.ai/Lessons/part2.html">fastai Course Part 2</a>: a course where one dives into the deeper workings of deep learning by fully implementing stable diffusion.</p>
<p>In the first lesson, we play around with diffusers using the <a href="https://huggingface.co/docs/diffusers/index">Hugging Face Diffusers</a> library. Below are things I have noticed; my musings.</p>
<section id="steps" class="level2">
<h2 class="anchored" data-anchor-id="steps">Steps</h2>
<p>Diffusion is simply a process whereby noise is progressively removed from a noisy image. A single step can be thought of a single portion of noise being removed.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><a href="../images/11_musings_through_stable_diffusion/serpent_ring.png" class="lightbox" title="A depiction of a ring comprised of interwined serpents, topped with a single jewel of emerald." data-gallery="quarto-lightbox-gallery-2"><img src="https://forbo7.github.io/forblog/images/11_musings_through_stable_diffusion/serpent_ring.png" class="img-fluid figure-img" alt="A depiction of a ring comprised of interwined serpents, topped with a single jewel of emerald."></a></p>
<p></p><figcaption class="figure-caption">A depiction of a ring comprised of interwined serpents, topped with a single jewel of emerald.</figcaption><p></p>
</figure>
</div>
<p>Below is the evolution of the image above in 48 steps. Each new image has less and less noise (what the diffuser thinks is noise).</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><a href="../images/11_musings_through_stable_diffusion/serpent_ring.gif" class="lightbox" title="The gif itself has artefacts due to compression…" data-gallery="quarto-lightbox-gallery-3"><img src="https://forbo7.github.io/forblog/images/11_musings_through_stable_diffusion/serpent_ring.gif" class="img-fluid figure-img" alt="A gif showing how the diffuser came to generating the image 'A depiction of a ring comprised of interwined serpents, topped with a single jewel of emerald.'"></a></p>
<p></p><figcaption class="figure-caption">The gif itself has artefacts due to compression…</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><a href="../images/11_musings_through_stable_diffusion/serpent_ring_grid.jpeg" class="lightbox" title="This image displays all images in the gif above in a grid." data-gallery="quarto-lightbox-gallery-4"><img src="https://forbo7.github.io/forblog/images/11_musings_through_stable_diffusion/serpent_ring_grid.jpeg" class="img-fluid figure-img" alt="This image displays all images in the gif above in a grid."></a></p>
</figure>
</div>
<p>It still managed to generate a pretty good image despite the misspelling of “intertwined”!</p>
</section>
<section id="when-it-doesnt-work-well" class="level2">
<h2 class="anchored" data-anchor-id="when-it-doesnt-work-well">When It Doesn’t Work Well</h2>
<p>I’ve found that a diffuser doesn’t work well when one prompts it for things, which I assume, it hasn’t “seen” or hasn’t been trained on before. It sounds obvious, but it’s really interesting when you see the result of it.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><a href="../images/11_musings_through_stable_diffusion/grasshopper_and_bunny_1.png" class="lightbox" title="A grasshopper riding a bunny." data-gallery="quarto-lightbox-gallery-5"><img src="https://forbo7.github.io/forblog/images/11_musings_through_stable_diffusion/grasshopper_and_bunny_1.png" class="img-fluid figure-img" alt="A grasshopper riding a bunny."></a></p>
<p></p><figcaption class="figure-caption">A grasshopper riding a bunny.</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><a href="../images/11_musings_through_stable_diffusion/grasshopper_and_bunny_2.png" class="lightbox" title="A grasshopper riding a bunny." data-gallery="quarto-lightbox-gallery-6"><img src="https://forbo7.github.io/forblog/images/11_musings_through_stable_diffusion/grasshopper_and_bunny_2.png" class="img-fluid figure-img" alt="A grasshopper riding a bunny."></a></p>
</figure>
</div>
<p>A quick Google search also doesn’t return any images matching the prompt in the top results.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><a href="../images/11_musings_through_stable_diffusion/grasshopper_and_bunny_3.png" class="lightbox" title="A screenshot of Google Image Search results showing no picture of a grasshopper riding a bunny." data-gallery="quarto-lightbox-gallery-7"><img src="https://forbo7.github.io/forblog/images/11_musings_through_stable_diffusion/grasshopper_and_bunny_3.png" class="img-fluid figure-img" alt="A screenshot of Google Image Search results showing no picture of a grasshopper riding a bunny."></a></p>
</figure>
</div>
</section>
<section id="cfg-classifier-free-guidance" class="level2">
<h2 class="anchored" data-anchor-id="cfg-classifier-free-guidance">CFG (Classifier Free Guidance)</h2>
<p>Or simply known as guidance, CFG is a value which tells the diffuser how much it should stick to the prompt.</p>
<p>A lower guidance leads to more varied and random images that are loosely related to the prompt. A higher guidance produces more relevant images.</p>
<p>I’ve found that too high of a guidenace leads to images having too much contrast.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><a href="../images/11_musings_through_stable_diffusion/guidance.png" class="lightbox" title="An antique 18th century painting of a gorilla eating a plate of chips." data-gallery="quarto-lightbox-gallery-8"><img src="https://forbo7.github.io/forblog/images/11_musings_through_stable_diffusion/guidance.png" class="img-fluid figure-img" alt="A grid of images generated from the prompt, 'An antique 18th century painting of a gorilla eating a plate of chips.' Each rows shows images generated with increased guidance."></a></p>
<p></p><figcaption class="figure-caption">An antique 18th century painting of a gorilla eating a plate of chips.</figcaption><p></p>
</figure>
</div>
<p>The image above shows rows with increasing levels of guidance (1, 2.5, 5, 7.5, 10, 25, 50). 7.5 is the sweetspot.</p>
</section>
<section id="negative-prompts" class="level2">
<h2 class="anchored" data-anchor-id="negative-prompts">Negative Prompts</h2>
<p>The best way to think about negative prompts is that a negative prompt <em>guides</em> a diffuser away from generating a certain entity.</p>
<p>Take the image below as an example.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><a href="../images/11_musings_through_stable_diffusion/negative_prompt_1.png" class="lightbox" title="An antique 18th century painting of a gorilla eating a plate of chips." data-gallery="quarto-lightbox-gallery-9"><img src="https://forbo7.github.io/forblog/images/11_musings_through_stable_diffusion/negative_prompt_1.png" class="img-fluid figure-img" alt="Another variation generated from the prompt, 'An antique 18th century painting of a gorilla eating a plate of chips.' There is a yellow circle around the gorilla."></a></p>
<p></p><figcaption class="figure-caption">An antique 18th century painting of a gorilla eating a plate of chips.</figcaption><p></p>
</figure>
</div>
<p>I generated the image again using the exact same seed and prompt, but also used the following negative prompt, “yellow circle”.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><a href="../images/11_musings_through_stable_diffusion/negative_prompt_2.png" class="lightbox" title="Prompt: An antique 18th century painting of a gorilla eating a plate of chips. | Negative Prompt: yellow circle" data-gallery="quarto-lightbox-gallery-10"><img src="https://forbo7.github.io/forblog/images/11_musings_through_stable_diffusion/negative_prompt_2.png" class="img-fluid figure-img" alt="The same prompt used again, but the negative prompt now removes the yellow circle and adds a rectangular border around the gorilla."></a></p>
<p></p><figcaption class="figure-caption">Prompt: An antique 18th century painting of a gorilla eating a plate of chips. | Negative Prompt: yellow circle</figcaption><p></p>
</figure>
</div>
</section>
<section id="image-to-image" class="level2">
<h2 class="anchored" data-anchor-id="image-to-image">Image to Image</h2>
<p>Instead of starting from noise, one can make a diffuser begin from an existing image. The diffuser follows the image as guide and doesn’t match it 1 to 1.</p>
<p>I quickly mocked up the following image.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><a href="../images/11_musings_through_stable_diffusion/img_to_img_0.png" class="lightbox" title="Clip art of a bench and a tree behind a sky and on top of grass. There is the sun and a couple of clouds in the sky." data-gallery="quarto-lightbox-gallery-11"><img src="https://forbo7.github.io/forblog/images/11_musings_through_stable_diffusion/img_to_img_0.png" class="img-fluid figure-img" alt="Clip art of a bench and a tree behind a sky and on top of grass. There is the sun and a couple of clouds in the sky."></a></p>
</figure>
</div>
<p>I input it to a diffuser with a prompt, and it output the following.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><a href="../images/11_musings_through_stable_diffusion/img_to_img_1.png" class="lightbox" title="A bench under a tree in a park" data-gallery="quarto-lightbox-gallery-12"><img src="https://forbo7.github.io/forblog/images/11_musings_through_stable_diffusion/img_to_img_1.png" class="img-fluid figure-img" alt="A bench under a tree in a park"></a></p>
<p></p><figcaption class="figure-caption">A bench under a tree in a park</figcaption><p></p>
</figure>
</div>
<p>I then further generated another image from this one.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><a href="../images/11_musings_through_stable_diffusion/img_to_img_2.png" class="lightbox" title="A low poly 3D render of a bench under a tree in a park" data-gallery="quarto-lightbox-gallery-13"><img src="https://forbo7.github.io/forblog/images/11_musings_through_stable_diffusion/img_to_img_2.png" class="img-fluid figure-img" alt="A low poly 3D render of a bench under a tree in a park"></a></p>
<p></p><figcaption class="figure-caption">A low poly 3D render of a bench under a tree in a park</figcaption><p></p>
</figure>
</div>
</section>
<section id="further-adapting-a-diffuser" class="level2">
<h2 class="anchored" data-anchor-id="further-adapting-a-diffuser">Further Adapting a Diffuser</h2>
<p>There are two ways one can further customize a diffuser to produce desired images: textual inversion and dreambooth.</p>
<section id="textual-inversion" class="level3">
<h3 class="anchored" data-anchor-id="textual-inversion">Textual Inversion</h3>
<p>A diffuser contains a text encoder. This encoder is responsible for parsing the prompt and giving it a mathematical representation.</p>
<p>A text encoder can only parse according to its vocabulary. If it encounters words not in its vocabulary, the diffuser will be unable to produce an image relevant to the prompt.</p>
<p>In a nutshell, textual inversion adds new words to the vocabulary of the text encoder so it can parse prompts with those new words.</p>
<p>I managed to generate the image below by adding the word “<a href="https://mrdoodle.com">Mr Doodle</a>” to the vocabulary of the diffuser’s text encoder.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><a href="../images/11_musings_through_stable_diffusion/textual_inversion.png" class="lightbox" title="An antique 18th century painting of a gorilla eating a plate of chips in the style of Mr Doodle" data-gallery="quarto-lightbox-gallery-14"><img src="https://forbo7.github.io/forblog/images/11_musings_through_stable_diffusion/textual_inversion.png" class="img-fluid figure-img" alt="An antique 18th century painting of a gorilla eating a plate of chips in the style of Mr Doodle"></a></p>
<p></p><figcaption class="figure-caption">An antique 18th century painting of a gorilla eating a plate of chips in the style of Mr Doodle</figcaption><p></p>
</figure>
</div>
</section>
<section id="dreambooth" class="level3">
<h3 class="anchored" data-anchor-id="dreambooth">Dreambooth</h3>
<p>Dreambooth is more akin to traditional fine-tuning methods. A diffuser is further trained on images one supplies to it.</p>
</section>
</section>
<section id="so-end-my-musings" class="level2">
<h2 class="anchored" data-anchor-id="so-end-my-musings">So End my Musings</h2>
<p>If you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!</p>


</section>

 ]]></description>
  <category>Diffusion</category>
  <guid>https://forbo7.github.io/forblog/posts/11_musings_through_stable_diffusion.html</guid>
  <pubDate>Thu, 13 Apr 2023 00:00:00 GMT</pubDate>
  <media:content url="https://forbo7.github.io/forblog/images/11_musings_through_stable_diffusion/thumbnail.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Stable Diffusion, Summarized</title>
  <dc:creator>Salman Naqvi</dc:creator>
  <link>https://forbo7.github.io/forblog/posts/12_stable_diffusion_summarized.html</link>
  <description><![CDATA[ 



<p><img src="https://forbo7.github.io/forblog/images/12_stable_diffusion_summarized/thumbnail.jpeg" class="img-fluid"></p>
<p>Here, I explain the workings of stable diffusion at a high level.</p>
<section id="components" class="level2">
<h2 class="anchored" data-anchor-id="components">Components</h2>
<p>A diffuser contains four main components</p>
<ul>
<li>The text encoder</li>
<li>The image encoder</li>
<li>The autoencoder (VAE autoencoder)</li>
<li>The neural network (U-net)</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div>
<p>
</p><pre class="mermaid mermaid-js" data-tooltip-selector="#mermaid-tooltip-1">flowchart TB
    A{{Diffuser}}
    B([U-net])
    C([VAE Autoencoder])
    D([Text Encoder])
    E([Image Encoder])

    A --&gt; D &amp; E &amp; C &amp; B
</pre>
<div id="mermaid-tooltip-1" class="mermaidTooltip">

</div>
<p></p>
</div>
</div>
</div>
</section>
<section id="training" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="training">Training</h2>
<p>I’ll explain the training process in terms of a single image.</p>
<p>When all components shown above are put into their respective places, the overall training process looks like this.</p>
<div class="column-screen-inset">
<div class="cell">
<div class="cell-output-display">
<div>
<p>
</p><pre class="mermaid mermaid-js" data-tooltip-selector="#mermaid-tooltip-2">flowchart LR
    subgraph A [Feature Vector Creation]
        id1([Text Encoder])
        id2([Image Encoder])
    end

    subgraph B [Image Compression]
        id3([VAE Autoencoder])
    end

    subgraph C [Noise Removal]
        id4([U-net])
    end

    subgraph D [Image Decompression]
        id5([VAE Autoencoder])
    end

    id7[Input Image Description] &amp; id6[Input Image] --&gt; A --&gt; id9[Feature Vector]
    id6 --noise added to image--&gt; B --&gt; id10[Noisy Latent]
    id9 &amp; id10 --&gt; C --&gt; id11[Less Noisy Latent] --&gt;  C
    id11 --&gt; D --&gt; id8[Generated Image]
</pre>
<div id="mermaid-tooltip-2" class="mermaidTooltip">

</div>
<p></p>
</div>
</div>
</div>
</div>
<p>Let’s break it down.</p>
<section id="feature-vector-creation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="feature-vector-creation">Feature Vector Creation</h3>
<div class="column-body-outset-right">
<div class="cell">
<div class="cell-output-display">
<div>
<p>
</p><pre class="mermaid mermaid-js" data-tooltip-selector="#mermaid-tooltip-3">flowchart TB
    subgraph B [ ]
        direction LR
        id1[Input Image]
        id2[Input Image Description]
        subgraph A [Feature Vector Creation]
            id3([Text Encoder])
            id4([Image Encoder])
        end
        id2 &amp; id1 --&gt; A --&gt; id11[Feature Vector]
    end
    style B fill:#FFF, stroke:#333,stroke-width:3px

    subgraph C [ ]
        direction LR
        id5[Input Image]
        id7[Input Image Descripton]
        id5 --&gt; id6
        id7 --&gt; id8
        subgraph D [Feature Vector Creation]
            id6([Image Encoder])
            id8([Text Encoder])
            id6 &amp; id8 --&gt; id9[CLIP Embedding]
        end
        id9 --&gt; id10[Feature Vector]
    end
    style C fill:#FFF, stroke:#333,stroke-width:3px

    B --&gt; C
    B --&gt; C
    B --&gt; C
</pre>
<div id="mermaid-tooltip-3" class="mermaidTooltip">

</div>
<p></p>
</div>
</div>
</div>
</div>
<p>We start with an image and its description. The image encoder takes the image and produces a feature vector — a vector with numerical values that describe the image in some way. The text encoder takes the image’s description and similarly produces a feature vector.</p>
<p>These two feature vectors are then stored in what’s known as a CLIP embedding. An embedding is simply a table where each row is an item and each column describes the items in some way. I this case, the rows represent feature vectors, and the columns are each feature in the vector.</p>
<p>Both encoders keep producing feature vectors until they are as similar as possible.</p>
</section>
<section id="image-compression" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="image-compression">Image Compression</h3>
<div class="column-body-outset-right">
<div class="cell">
<div class="cell-output-display">
<div>
<p>
</p><pre class="mermaid mermaid-js" data-tooltip-selector="#mermaid-tooltip-4">flowchart TB
    subgraph A [ ]
        id2[Input Image] --&gt; id1
        subgraph B [Image Compression]
            direction LR
            id1([VAE Autoencoder])
        end
        id1 --&gt; id7[Latent]
    end
    style A fill:#FFF, stroke:#333,stroke-width:3px

    subgraph C[ ]
        direction LR
        id3[Input Image]
        subgraph D [Image Compression]
            id4([VAE Encoder])
            id5([VAE Decoder])
        end
        id3 --&gt; id4 --&gt; id6[Latent]
    end
    style C fill:#FFF, stroke:#333,stroke-width:3px

    A &amp; A &amp; A --&gt; C
</pre>
<div id="mermaid-tooltip-4" class="mermaidTooltip">

</div>
<p></p>
</div>
</div>
</div>
</div>
<p>Once the feature vectors have been produced, some noise is tossed onto the image. This noisy image is then compressed by the VAE autoencoder.</p>
<p>The VAE autoencoder contains an encoder and a decoder. The encoder handles compression whereas the decoder handles decompression.</p>
<p>The compressed noisy image is now known as the latent. The image is compressed for faster computation, as there would be fewer pixels to compute on.</p>
</section>
<section id="noise-removal" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="noise-removal">Noise Removal</h3>
<div class="column-body-outset-right">
<div class="cell">
<div class="cell-output-display">
<div>
<p>
</p><pre class="mermaid mermaid-js" data-tooltip-selector="#mermaid-tooltip-5">flowchart TB
    subgraph A [ ]
        id1[Feature Vector] &amp; id2[Noisy Latent] --&gt; id3
        subgraph B [Noise Removal]
            direction LR
            id3([U-net]) --&gt; id4[Noise]
        end
        id4 --with learning rate--&gt; id5[Less Noisy Latent] --&gt; id3
    end
    style A fill:#FFF, stroke:#333,stroke-width:3px 

    subgraph C [ ]
        id6[Feature Vector] &amp; id7[Noisy Latent] --&gt; id8
        subgraph Noise Removal
            direction LR
            id8([U-net])
        end
        id8 --&gt; id9([Less Noisy Latent]) --&gt; id8
    end
    C &amp; C &amp; C --&gt; A
    style C fill:#FFF, stroke:#333,stroke-width:3px
</pre>
<div id="mermaid-tooltip-5" class="mermaidTooltip">

</div>
<p></p>
</div>
</div>
</div>
</div>
<p>The latent, together with its feature vector, is now input to the U-net. Instead of predicting what the original, un-noisy image was, the U-net predicts the noise that was tossed onto the image.</p>
<p>Once it outputs the predicted noise, that noise is subtracted from the latent in conjunction with the learning rate. This new, less noisy latent is now input again and the process repeats until desired.</p>
</section>
<section id="image-decompression" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="image-decompression">Image Decompression</h3>
<div class="column-body-outset-right">
<div class="cell">
<div class="cell-output-display">
<div>
<p>
</p><pre class="mermaid mermaid-js" data-tooltip-selector="#mermaid-tooltip-6">flowchart TB
    subgraph A [ ]
        direction LR
        id2[Input Image] --&gt; id1
        subgraph B [Image Decompression]
            id1([VAE Autoencoder])
        end
        id1 --&gt; id7[Latent]
    end
    style A fill:#FFF, stroke:#333,stroke-width:3px

    subgraph C [ ]
        direction LR
        id3[Less Noisy Latent] --&gt; id5
        subgraph D [Image Decompression]
            id4([VAE Encoder])
            id5([VAE Decoder])
        end
        id5 --&gt; id6[Generated Image]
    end
    style C fill:#FFF, stroke:#333,stroke-width:3px

    A &amp; A &amp; A --&gt; C
</pre>
<div id="mermaid-tooltip-6" class="mermaidTooltip">

</div>
<p></p>
</div>
</div>
</div>
</div>
<p>The latent is now decompressed through the VAE autoencoder’s decoder.</p>
<p>We now have a generated image!</p>
</section>
</section>
<section id="inference" class="level2">
<h2 class="anchored" data-anchor-id="inference">Inference</h2>
<p>When using a diffuser for inference, the diffuser <em>typically</em> begins with a purely noisey image. The diffuser uses the input prompt to guide the removal of noise from the noisy image, until the image resembles what is desired.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>And that’s all there is to it!</p>
<p>We take an image and its prompt, and create a feature vector out of them. Noise is then added to the image, which is then compressed. The latent and the feature vector are input to a U-net which then predicts the noise in the latent. The predicted noise is subtracted from the latent, which is then input back to the U-net. After the desired number of steps has lapsed, the latent is decompressed and the generated image is ready!</p>
<p>If you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!</p>


</section>

 ]]></description>
  <category>Diffusion</category>
  <category>Creating Models</category>
  <guid>https://forbo7.github.io/forblog/posts/12_stable_diffusion_summarized.html</guid>
  <pubDate>Thu, 13 Apr 2023 00:00:00 GMT</pubDate>
  <media:content url="https://forbo7.github.io/forblog/images/12_stable_diffusion_summarized/thumbnail.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>How to Convert Audio to Spectrogram Images</title>
  <dc:creator>Salman Naqvi</dc:creator>
  <link>https://forbo7.github.io/forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html</link>
  <description><![CDATA[ 



<p><strong>You can find this notebook on Kaggle <a href="https://www.kaggle.com/code/forbo7/how-to-convert-audio-to-spectrogram-images">here</a>.</strong></p>
<blockquote class="blockquote">
<p>This notebook follows the <a href="https://docs.fast.ai/dev/style.html#style-guide">fastai style conventions</a>.</p>
</blockquote>
<p><img src="https://forbo7.github.io/forblog/images/10_how_to_convert_audio_to_spectrogram_images/thumbnail.jpg" class="img-fluid" alt="A generated image of a colorful East African bird."></p>
<p>In this to-the-point notebook, I go over how one can create images of spectrograms from audio files using the PyTorch torchaudio module.</p>
<p>The notebook also goes over how I created the spectrogram images for the BirdCLEF 2023 competition, and how one can create and push a dataset right on Kaggle (useful if your local machine doesn’t have enough storage).</p>
<p>You can view the dataset that was generated from this notebook <a href="https://www.kaggle.com/datasets/forbo7/spectrograms-birdclef-2023">here</a>.</p>
<section id="setup" class="level2">
<h2 class="anchored" data-anchor-id="setup">Setup</h2>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:15:52.222983Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:15:52.221975Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:06.187026Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:06.185564Z&quot;}" data-papermill="{&quot;duration&quot;:13.988997,&quot;end_time&quot;:&quot;2023-04-05T11:16:06.190265&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:15:52.201268&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="cf" style="color: #003B4F;">try</span>: <span class="im" style="color: #00769E;">from</span> fastkaggle <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span>
<span id="cb1-2"><span class="cf" style="color: #003B4F;">except</span> ModuleNotFoundError:</span>
<span id="cb1-3">    <span class="op" style="color: #5E5E5E;">!</span> pip install <span class="op" style="color: #5E5E5E;">-</span>Uqq fastkaggle</span>
<span id="cb1-4">    <span class="im" style="color: #00769E;">from</span> fastkaggle <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span>
<span id="cb1-5"></span>
<span id="cb1-6">iskaggle</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>'Batch'</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:06.219791Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:06.217738Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:19.085766Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:19.083703Z&quot;}" data-papermill="{&quot;duration&quot;:12.886205,&quot;end_time&quot;:&quot;2023-04-05T11:16:19.089359&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:06.203154&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">comp <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'birdclef-2023'</span></span>
<span id="cb3-2">d_path <span class="op" style="color: #5E5E5E;">=</span> setup_comp(comp, install<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'nbdev'</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:19.119731Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:19.119242Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:25.763802Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:25.761810Z&quot;}" data-papermill="{&quot;duration&quot;:6.663933,&quot;end_time&quot;:&quot;2023-04-05T11:16:25.766895&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:19.102962&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;">from</span> fastai.imports <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span>
<span id="cb4-2"><span class="im" style="color: #00769E;">from</span> fastai.vision.<span class="bu" style="color: null;">all</span> <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span></code></pre></div>
</div>
</section>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">Data</h2>
<section id="paths" class="level3">
<h3 class="anchored" data-anchor-id="paths">Paths</h3>
<p>Let’s see all the files and directories we have.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:25.878998Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:25.878488Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:25.888345Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:25.887118Z&quot;}" data-papermill="{&quot;duration&quot;:0.028396,&quot;end_time&quot;:&quot;2023-04-05T11:16:25.891571&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:25.863175&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">d_path.ls()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>(#5) [Path('../input/birdclef-2023/sample_submission.csv'),Path('../input/birdclef-2023/train_audio'),Path('../input/birdclef-2023/eBird_Taxonomy_v2021.csv'),Path('../input/birdclef-2023/train_metadata.csv'),Path('../input/birdclef-2023/test_soundscapes')]</code></pre>
</div>
</div>
<p>Let’s get the path to the audio files.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:25.947818Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:25.947204Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:25.953261Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:25.951813Z&quot;}" data-papermill="{&quot;duration&quot;:0.022977,&quot;end_time&quot;:&quot;2023-04-05T11:16:25.956052&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:25.933075&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">aud_files <span class="op" style="color: #5E5E5E;">=</span> d_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'train_audio'</span></span></code></pre></div>
</div>
<p>And create a directory to store the spectrogram images.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:26.012862Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:26.012021Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:26.022510Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:26.020324Z&quot;}" data-papermill="{&quot;duration&quot;:0.030052,&quot;end_time&quot;:&quot;2023-04-05T11:16:26.026393&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:25.996341&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">mkdir(<span class="st" style="color: #20794D;">'/kaggle/train_images'</span>, exist_ok<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)<span class="op" style="color: #5E5E5E;">;</span> Path(<span class="st" style="color: #20794D;">'/kaggle/train_images'</span>).exists()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>True</code></pre>
</div>
</div>
</section>
</section>
<section id="single-image" class="level2">
<h2 class="anchored" data-anchor-id="single-image">Single Image</h2>
<p>It’s always a good idea to try things out on a smaller scale; so let’s begin by converting only a single audio file.</p>
<p>Let’s get the first audio file.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:26.113335Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:26.112824Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:26.142126Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:26.140480Z&quot;}" data-papermill="{&quot;duration&quot;:0.048533,&quot;end_time&quot;:&quot;2023-04-05T11:16:26.145465&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:26.096932&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">aud_files.ls()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>(#264) [Path('../input/birdclef-2023/train_audio/yetgre1'),Path('../input/birdclef-2023/train_audio/moccha1'),Path('../input/birdclef-2023/train_audio/rostur1'),Path('../input/birdclef-2023/train_audio/walsta1'),Path('../input/birdclef-2023/train_audio/ratcis1'),Path('../input/birdclef-2023/train_audio/norfis1'),Path('../input/birdclef-2023/train_audio/macshr1'),Path('../input/birdclef-2023/train_audio/brrwhe3'),Path('../input/birdclef-2023/train_audio/crefra2'),Path('../input/birdclef-2023/train_audio/pabspa1')...]</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:26.174836Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:26.174343Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:26.192704Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:26.190872Z&quot;}" data-papermill="{&quot;duration&quot;:0.035581,&quot;end_time&quot;:&quot;2023-04-05T11:16:26.195419&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:26.159838&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">aud_files.ls()[<span class="dv" style="color: #AD0000;">0</span>].ls()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>(#27) [Path('../input/birdclef-2023/train_audio/yetgre1/XC247367.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC574558.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC403259.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC498854.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC289493.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC716763.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC498853.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC338717.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC349660.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC403543.ogg')...]</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:26.226795Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:26.225378Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:26.237298Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:26.236003Z&quot;}" data-papermill="{&quot;duration&quot;:0.031014,&quot;end_time&quot;:&quot;2023-04-05T11:16:26.240594&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:26.209580&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">aud <span class="op" style="color: #5E5E5E;">=</span> aud_files.ls()[<span class="dv" style="color: #AD0000;">0</span>].ls()[<span class="dv" style="color: #AD0000;">0</span>]<span class="op" style="color: #5E5E5E;">;</span> aud</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>Path('../input/birdclef-2023/train_audio/yetgre1/XC247367.ogg')</code></pre>
</div>
</div>
<p>Now it’s time to load it in. What we get in return is the waveform and the sample rate.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:26.298265Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:26.297796Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:27.258464Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:27.256550Z&quot;}" data-papermill="{&quot;duration&quot;:0.980217,&quot;end_time&quot;:&quot;2023-04-05T11:16:27.261269&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:26.281052&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="im" style="color: #00769E;">import</span> torchaudio</span>
<span id="cb16-2">wvfrm, sr <span class="op" style="color: #5E5E5E;">=</span> torchaudio.load(aud)<span class="op" style="color: #5E5E5E;">;</span> wvfrm</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.0518e-05, 0.0000e+00,
         0.0000e+00]])</code></pre>
</div>
</div>
<hr>
<section id="birdclef-2023-clipping-the-audio-files" class="level4">
<h4 class="anchored" data-anchor-id="birdclef-2023-clipping-the-audio-files">BirdCLEF 2023 — Clipping the Audio Files</h4>
<p>This competition requires predictions to be submitted of all 5 second intervals in each audio clip. This means the audio files need to be clipped.</p>
<p>Below is an easy way this can be done. We clip the first 5 seconds of the audio file.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:27.375336Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:27.374863Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:27.383346Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:27.381728Z&quot;}" data-papermill="{&quot;duration&quot;:0.027009,&quot;end_time&quot;:&quot;2023-04-05T11:16:27.386202&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:27.359193&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="11">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">start_sec <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb18-2">end_sec <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">5</span></span>
<span id="cb18-3">wvfrm <span class="op" style="color: #5E5E5E;">=</span> wvfrm[:, start_sec<span class="op" style="color: #5E5E5E;">*</span>sr:end_sec<span class="op" style="color: #5E5E5E;">*</span>sr]</span>
<span id="cb18-4">wvfrm.shape[<span class="dv" style="color: #AD0000;">1</span>] <span class="op" style="color: #5E5E5E;">/</span> sr</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>5.0</code></pre>
</div>
</div>
<p>Sample rate is simply the number of frames recorded per second. The waveform that torchaudio returns is a tensor of frames. Therefore, we can easily select the desired range of frames by multiplying the sample rate with the desired start and end seconds.</p>
<hr>
<p>Now let’s create the spectrogram.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:27.503378Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:27.502883Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:27.586982Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:27.585599Z&quot;}" data-papermill="{&quot;duration&quot;:0.103329,&quot;end_time&quot;:&quot;2023-04-05T11:16:27.589754&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:27.486425&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="12">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="im" style="color: #00769E;">import</span> torchaudio.transforms <span class="im" style="color: #00769E;">as</span> T</span>
<span id="cb20-2">spec <span class="op" style="color: #5E5E5E;">=</span> T.Spectrogram()(wvfrm)<span class="op" style="color: #5E5E5E;">;</span> spec</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>tensor([[[4.3970e-08, 8.2461e-09, 4.7306e-11,  ..., 7.8266e-08,
          1.7642e-08, 1.5016e-03],
         [6.3310e-09, 6.5514e-10, 1.6958e-08,  ..., 4.3492e-09,
          3.6019e-08, 1.5231e-03],
         [1.1548e-08, 1.7308e-08, 6.5956e-08,  ..., 2.9340e-06,
          1.2277e-06, 1.4124e-03],
         ...,
         [2.4446e-07, 6.1277e-09, 1.4932e-09,  ..., 1.4665e-08,
          1.0110e-08, 1.8980e-05],
         [3.1582e-07, 1.4777e-09, 1.2275e-08,  ..., 1.3213e-08,
          1.9035e-09, 2.0009e-05],
         [3.1673e-07, 1.1897e-10, 2.7457e-09,  ..., 1.0001e-08,
          6.0452e-14, 1.9979e-05]]])</code></pre>
</div>
</div>
<p>Let’s scale it logarithmically. This allows for better viewing.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:27.649631Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:27.648891Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:27.674057Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:27.672720Z&quot;}" data-papermill="{&quot;duration&quot;:0.043955,&quot;end_time&quot;:&quot;2023-04-05T11:16:27.676877&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:27.632922&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="13">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">spec <span class="op" style="color: #5E5E5E;">=</span> T.AmplitudeToDB()(spec)<span class="op" style="color: #5E5E5E;">;</span> spec</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>tensor([[[ -73.5684,  -80.8375, -100.0000,  ...,  -71.0643,  -77.5346,
           -28.2346],
         [ -81.9853,  -91.8367,  -77.7063,  ...,  -83.6159,  -74.4347,
           -28.1726],
         [ -79.3751,  -77.6177,  -71.8074,  ...,  -55.3254,  -59.1092,
           -28.5005],
         ...,
         [ -66.1180,  -82.1270,  -88.2588,  ...,  -78.3373,  -79.9524,
           -47.2170],
         [ -65.0056,  -88.3043,  -79.1097,  ...,  -78.7899,  -87.2045,
           -46.9877],
         [ -64.9931,  -99.2458,  -85.6135,  ...,  -79.9997, -100.0000,
           -46.9943]]])</code></pre>
</div>
</div>
<p>The PyTorch tensor needs to be converted into a NumPy array so it can then further be converted to an image. I’m using the <code>squeeze</code> method to remove the uneeded axis of length 1, as seen below.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:27.740341Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:27.739507Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:27.749453Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:27.748268Z&quot;}" data-papermill="{&quot;duration&quot;:0.029094,&quot;end_time&quot;:&quot;2023-04-05T11:16:27.752190&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:27.723096&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="14">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">spec.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>torch.Size([1, 201, 801])</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:27.785837Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:27.784455Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:27.793653Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:27.792659Z&quot;}" data-papermill="{&quot;duration&quot;:0.028469,&quot;end_time&quot;:&quot;2023-04-05T11:16:27.796037&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:27.767568&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="15">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">spec <span class="op" style="color: #5E5E5E;">=</span> spec.squeeze().numpy()<span class="op" style="color: #5E5E5E;">;</span> spec</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>array([[ -73.56842 ,  -80.83749 , -100.      , ...,  -71.064285,
         -77.53463 ,  -28.234562],
       [ -81.98525 ,  -91.83666 ,  -77.706314, ...,  -83.615875,
         -74.43467 ,  -28.172626],
       [ -79.37505 ,  -77.61765 ,  -71.80743 , ...,  -55.32541 ,
         -59.109177,  -28.500452],
       ...,
       [ -66.118   ,  -82.127014,  -88.25883 , ...,  -78.33732 ,
         -79.95244 ,  -47.21705 ],
       [ -65.00563 ,  -88.30426 ,  -79.10974 , ...,  -78.78989 ,
         -87.20445 ,  -46.987743],
       [ -64.99306 ,  -99.24575 ,  -85.61355 , ...,  -79.99969 ,
        -100.      ,  -46.994343]], dtype=float32)</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:27.828702Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:27.827779Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:27.834763Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:27.833666Z&quot;}" data-papermill="{&quot;duration&quot;:0.026709,&quot;end_time&quot;:&quot;2023-04-05T11:16:27.837709&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:27.811000&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="16">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">spec.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>(201, 801)</code></pre>
</div>
</div>
<p>The array now needs to be normalized so it contains integers between 0 and 255: the values needed for images.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:27.900797Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:27.900280Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:27.913110Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:27.912123Z&quot;}" data-papermill="{&quot;duration&quot;:0.033373,&quot;end_time&quot;:&quot;2023-04-05T11:16:27.916342&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:27.882969&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="17">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">spec <span class="op" style="color: #5E5E5E;">=</span> (spec <span class="op" style="color: #5E5E5E;">-</span> spec.<span class="bu" style="color: null;">min</span>()) <span class="op" style="color: #5E5E5E;">/</span> (spec.<span class="bu" style="color: null;">max</span>() <span class="op" style="color: #5E5E5E;">-</span> spec.<span class="bu" style="color: null;">min</span>()) <span class="op" style="color: #5E5E5E;">*</span> <span class="dv" style="color: #AD0000;">255</span><span class="op" style="color: #5E5E5E;">;</span> spec</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>array([[ 53.000538 ,  38.424625 ,   0.       , ...,  58.021824 ,
         45.047504 , 143.90388  ],
       [ 36.123127 ,  16.369104 ,  44.703243 , ...,  32.853405 ,
         51.263535 , 144.02808  ],
       [ 41.35709  ,  44.881027 ,  56.53168  , ...,  89.581375 ,
         81.99417  , 143.37071  ],
       ...,
       [ 67.94011  ,  35.838867 ,  23.543371 , ...,  43.437954 ,
         40.199318 , 105.84024  ],
       [ 70.17062  ,  23.452267 ,  41.889095 , ...,  42.530468 ,
         25.6576   , 106.30005  ],
       [ 70.19584  ,   1.5124193,  28.847677 , ...,  40.104576 ,
          0.       , 106.28681  ]], dtype=float32)</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:27.951841Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:27.949505Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:27.961525Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:27.960579Z&quot;}" data-papermill="{&quot;duration&quot;:0.032237,&quot;end_time&quot;:&quot;2023-04-05T11:16:27.964842&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:27.932605&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="18">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">spec <span class="op" style="color: #5E5E5E;">=</span> spec.astype(<span class="st" style="color: #20794D;">'uint8'</span>)<span class="op" style="color: #5E5E5E;">;</span> spec</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>array([[ 53,  38,   0, ...,  58,  45, 143],
       [ 36,  16,  44, ...,  32,  51, 144],
       [ 41,  44,  56, ...,  89,  81, 143],
       ...,
       [ 67,  35,  23, ...,  43,  40, 105],
       [ 70,  23,  41, ...,  42,  25, 106],
       [ 70,   1,  28, ...,  40,   0, 106]], dtype=uint8)</code></pre>
</div>
</div>
<p>Now we can finally convert the array to an image!</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:28.031056Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:28.029390Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:28.084834Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:28.083568Z&quot;}" data-papermill="{&quot;duration&quot;:0.07604,&quot;end_time&quot;:&quot;2023-04-05T11:16:28.088714&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:28.012674&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="19">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">img <span class="op" style="color: #5E5E5E;">=</span> Image.fromarray(spec)</span>
<span id="cb34-2"><span class="bu" style="color: null;">print</span>(img.shape)</span>
<span id="cb34-3">img</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(201, 801)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="19">
<p><img src="https://forbo7.github.io/forblog/posts/10_how_to_convert_audio_to_spectrogram_images_files/figure-html/cell-20-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Cool, hey? We’ve just visualized audio!</p>
<hr>
</section>
<section id="birdclef-2023-resizing-the-images" class="level4">
<h4 class="anchored" data-anchor-id="birdclef-2023-resizing-the-images">BirdCLEF 2023 — Resizing the Images</h4>
<p>To allow the images to easily be used by various models, I resized the spectrograms to be 512 by 512 pixels as shown below.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:28.274501Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:28.273673Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:28.326587Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:28.324898Z&quot;}" data-papermill="{&quot;duration&quot;:0.078347,&quot;end_time&quot;:&quot;2023-04-05T11:16:28.331168&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:28.252821&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="20">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">img_size <span class="op" style="color: #5E5E5E;">=</span> (<span class="dv" style="color: #AD0000;">512</span>, <span class="dv" style="color: #AD0000;">512</span>)</span>
<span id="cb36-2">img <span class="op" style="color: #5E5E5E;">=</span> img.resize(img_size)</span>
<span id="cb36-3"><span class="bu" style="color: null;">print</span>(img.shape)<span class="op" style="color: #5E5E5E;">;</span> img</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(512, 512)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="20">
<p><img src="https://forbo7.github.io/forblog/posts/10_how_to_convert_audio_to_spectrogram_images_files/figure-html/cell-21-output-2.png" class="img-fluid"></p>
</div>
</div>
<hr>
<p>To save the image, we can simply use the <code>save</code> method.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:28.459626Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:28.459199Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:28.503231Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:28.501434Z&quot;}" data-papermill="{&quot;duration&quot;:0.069824,&quot;end_time&quot;:&quot;2023-04-05T11:16:28.506489&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:28.436665&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="21">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1">img.save(<span class="st" style="color: #20794D;">'img.png'</span>)</span></code></pre></div>
</div>
</section>
</section>
<section id="all-the-images" class="level2">
<h2 class="anchored" data-anchor-id="all-the-images">All the Images</h2>
<p>Now that we have verified that our algorithm works fine, we can extend it to convert all audio files.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:28.643386Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:28.642940Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:28.655463Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:28.653781Z&quot;}" data-papermill="{&quot;duration&quot;:0.039189,&quot;end_time&quot;:&quot;2023-04-05T11:16:28.658495&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:28.619306&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="22">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><span class="kw" style="color: #003B4F;">def</span> create_imgs(duration, f):</span>
<span id="cb39-2">    <span class="cf" style="color: #003B4F;">for</span> step <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, duration, <span class="dv" style="color: #AD0000;">5</span>):</span>
<span id="cb39-3">        wvfrm, sr <span class="op" style="color: #5E5E5E;">=</span> torchaudio.load(f)</span>
<span id="cb39-4">        wvfrm <span class="op" style="color: #5E5E5E;">=</span> cut_wvfrm(wvfrm, sr, step)</span>
<span id="cb39-5">        spec <span class="op" style="color: #5E5E5E;">=</span> create_spec(wvfrm)</span>
<span id="cb39-6">        img <span class="op" style="color: #5E5E5E;">=</span> spec2img(spec)</span>
<span id="cb39-7">        end_sec <span class="op" style="color: #5E5E5E;">=</span> step <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">5</span></span>
<span id="cb39-8">        img.save(<span class="ss" style="color: #20794D;">f'/kaggle/train_images/</span><span class="sc" style="color: #5E5E5E;">{</span>bird<span class="sc" style="color: #5E5E5E;">.</span>stem<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">/</span><span class="sc" style="color: #5E5E5E;">{</span>f<span class="sc" style="color: #5E5E5E;">.</span>stem<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">_</span><span class="sc" style="color: #5E5E5E;">{</span>end_sec<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">.png'</span>)</span>
<span id="cb39-9"></span>
<span id="cb39-10"><span class="kw" style="color: #003B4F;">def</span> cut_wvfrm(wvfrm, sr, step):</span>
<span id="cb39-11">    start_sec, end_sec <span class="op" style="color: #5E5E5E;">=</span> step, step <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">5</span></span>
<span id="cb39-12">    <span class="cf" style="color: #003B4F;">return</span> wvfrm[:, start_sec <span class="op" style="color: #5E5E5E;">*</span> sr: end_sec <span class="op" style="color: #5E5E5E;">*</span> sr]</span>
<span id="cb39-13">            </span>
<span id="cb39-14"><span class="kw" style="color: #003B4F;">def</span> create_spec(wvfrm):</span>
<span id="cb39-15">    spec <span class="op" style="color: #5E5E5E;">=</span> T.Spectrogram()(wvfrm)</span>
<span id="cb39-16">    <span class="cf" style="color: #003B4F;">return</span> T.AmplitudeToDB()(spec)</span>
<span id="cb39-17">        </span>
<span id="cb39-18"><span class="kw" style="color: #003B4F;">def</span> spec2img(spec, img_size<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">512</span>, <span class="dv" style="color: #AD0000;">512</span>)):</span>
<span id="cb39-19">    spec <span class="op" style="color: #5E5E5E;">=</span> np.real(spec.squeeze().numpy())</span>
<span id="cb39-20">    spec <span class="op" style="color: #5E5E5E;">=</span> ((spec <span class="op" style="color: #5E5E5E;">-</span> spec.<span class="bu" style="color: null;">min</span>()) <span class="op" style="color: #5E5E5E;">/</span> (spec.<span class="bu" style="color: null;">max</span>() <span class="op" style="color: #5E5E5E;">-</span> spec.<span class="bu" style="color: null;">min</span>()) <span class="op" style="color: #5E5E5E;">*</span> <span class="dv" style="color: #AD0000;">255</span>).astype(<span class="st" style="color: #20794D;">'uint8'</span>)</span>
<span id="cb39-21">    <span class="cf" style="color: #003B4F;">return</span> Image.fromarray(spec).resize(img_size)</span></code></pre></div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:28.704642Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:28.704142Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:28.713257Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:28.711504Z&quot;}" data-papermill="{&quot;duration&quot;:0.036435,&quot;end_time&quot;:&quot;2023-04-05T11:16:28.716300&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:28.679865&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="23">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> iskaggle:</span>
<span id="cb40-2">    <span class="cf" style="color: #003B4F;">for</span> bird <span class="kw" style="color: #003B4F;">in</span> aud_files.ls().<span class="bu" style="color: null;">sorted</span>():</span>
<span id="cb40-3">        mkdir(<span class="ss" style="color: #20794D;">f'/kaggle/train_images/</span><span class="sc" style="color: #5E5E5E;">{</span>bird<span class="sc" style="color: #5E5E5E;">.</span>stem<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>, exist_ok<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb40-4">        <span class="cf" style="color: #003B4F;">for</span> f <span class="kw" style="color: #003B4F;">in</span> bird.ls().<span class="bu" style="color: null;">sorted</span>():</span>
<span id="cb40-5">            info <span class="op" style="color: #5E5E5E;">=</span> torchaudio.info(f)</span>
<span id="cb40-6">            duration <span class="op" style="color: #5E5E5E;">=</span> info.num_frames <span class="op" style="color: #5E5E5E;">/</span> info.sample_rate</span>
<span id="cb40-7">            <span class="cf" style="color: #003B4F;">if</span> duration <span class="op" style="color: #5E5E5E;">&gt;=</span> <span class="dv" style="color: #AD0000;">5</span>:</span>
<span id="cb40-8">                create_imgs(<span class="bu" style="color: null;">round</span>(duration<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">5</span>)<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">5</span>, f)</span>
<span id="cb40-9">            <span class="cf" style="color: #003B4F;">else</span>: <span class="cf" style="color: #003B4F;">continue</span></span></code></pre></div>
</div>
<blockquote class="blockquote">
<p><strong>Note:</strong> Ignore the <code>if not iskaggle</code> statement when replicating. I added it since I edited this notebook and needed to save changes without reproducing the entire dataset.</p>
</blockquote>
<p>In the first <code>for</code> loop below, we loop through all the bird folders. For each folder, a folder with the same name is created in the directory where we want to store the images.</p>
<p>In the second <code>for</code> loop, we loop through all audio files within the folder and then convert them to spectrogram images through the <code>create_images</code> function I defined.</p>
<hr>
<section id="birdclef-2023-clipping-the-audio-files-1" class="level4">
<h4 class="anchored" data-anchor-id="birdclef-2023-clipping-the-audio-files-1">BirdCLEF 2023 — Clipping the Audio Files</h4>
<p>Some audio files in the training set are of different durations. Therefore, we obtain the duration of the audio file so it can correctly be clipped into 5 second intervals.</p>
<div class="sourceCode" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">info <span class="op" style="color: #5E5E5E;">=</span> torchaudio.info(f)</span>
<span id="cb41-2">duration <span class="op" style="color: #5E5E5E;">=</span> info.num_frames <span class="op" style="color: #5E5E5E;">/</span> info.sample_rate</span>
<span id="cb41-3"><span class="cf" style="color: #003B4F;">if</span> duration <span class="op" style="color: #5E5E5E;">&gt;=</span> <span class="dv" style="color: #AD0000;">5</span>:</span>
<span id="cb41-4">    create_images(<span class="bu" style="color: null;">round</span>(duration<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">5</span>)<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">5</span>, f)</span>
<span id="cb41-5"><span class="cf" style="color: #003B4F;">else</span>: <span class="cf" style="color: #003B4F;">continue</span></span></code></pre></div>
<p>Again, since sample rate is the number of frames recorded per second, we can divide the total number of frames by the sample rate to obtain the duration in seconds of a clip.</p>
<p><code>duration = info.num_frames / info.sample_rate</code></p>
<p>Then we round the duration to the nearest 5 for easy clipping.</p>
<p><code>round(duration/5)*5</code></p>
<hr>
<p>The images now created! The rest of this notebook covers how one can generate a dataset in a Kaggle Notebook and push it directly to Kaggle within it.</p>
</section>
</section>
<section id="api-setup" class="level2">
<h2 class="anchored" data-anchor-id="api-setup">API Setup</h2>
<p>We need to configure the user keys so we can push to the correct account.</p>
<p>To do this, first obtain your Kaggle API key. Then, while in the notebook editor, click Add-ons -&gt; Secrets -&gt; Add a New Secret…</p>
<p><img src="https://forbo7.github.io/forblog/posts/https:/sat02pap002files.storage.live.com/y4mY_6mu6XXxmpkR_MwTxHxfEGRV9QOqlxkCF8i6T90WkRadthJc9yeOxUG3k21bT-QQ4CxJcP1W-MHuAhX_tKvf-gcZqUvkLONiKPeumqZCTezKhwXJXs0OFuTDBnedbcViCOeeIkzjVjYJiBc3J7ib0AqG5v247vovXQP1JYv-8RmfWMFytuyCgwvLNt_5MSP?width=2880&amp;height=1800&amp;cropmode=none" width="75%" height="100%"></p>
<p><img src="https://forbo7.github.io/forblog/posts/https:/sat02pap002files.storage.live.com/y4mBCKZg2xbmSClGOAWdxf6vXcrgLKV0TNe-SuLaS37nzKAuHpzMZsXL0lEAuauWYrrKnZVx2qGStH3ZjVn-E0xARzTUW7UN9Em3gx_X3qivCHIX_85Z0aQc0xQYrl2dJPvyPOHwuCm0ML5188bFuemgoQik3jn_TVih1YKxYw9Z79fDahzG8wD9Yu-zbizRsVS?width=1024&amp;height=640&amp;cropmode=none" width="75%" height="100%"></p>
<p><img src="https://forbo7.github.io/forblog/posts/https:/sat02pap002files.storage.live.com/y4mgCDqODGoakfC1zbSK1tEGy1gcUPMpzzw5DEFHJnQPZIWZpWn2WKLT48EGRky-kXSmBYt-BYSgVxO7jMVT321_gqOYjX0daxumANws9LpNOvwdG3ce9nKJFT8ZkSEjtb5Q93leY75dKLk2CumNpmRGi4WoZr5jsTPPEV42yTlNPURAMrSsl09XeKqGaBjJO0u?width=2880&amp;height=1800&amp;cropmode=none" width="75%" height="100%"></p>
<p>…input your key and give it a name…</p>
<p><img src="https://forbo7.github.io/forblog/posts/https:/sat02pap002files.storage.live.com/y4mz3E2oKRG9H8n6cSwx-uk8ZAuIB8WSExJX92_w8ZoqYkX4f72QXKM2AFjjn7n_UaS30btHk6ueSi9VTRD_us_pUp3CSo3MkBM0mRBZqeqYDrSY4GRGamkrYnNKtxfZ-PQKJEvjOr8r8N97cftsJ3-9fEUcO3jv8kVGniKNPHWx_aVpIElx4_qgVq-aRt30PSD?width=2880&amp;height=1800&amp;cropmode=none" width="75%" height="100%"></p>
<p>…and click save. Then click the checkbox next to the secret to activate it for your notebook.</p>
<p><img src="https://forbo7.github.io/forblog/posts/https:/sat02pap002files.storage.live.com/y4m3RfkLD7cbynZIPmI7X17qmHTxar-hX5p-w5G70Z3YiRiIPzkzo3Z9h1Z4eWecrmmU88yl-HXUSRU7xUQ0LN2C-_wn_qmKDJFJyXSjNfhV7XjKpGbyhxXU9JCaTOxZhRwHGl4ghkykkhpMBTpoG-dxe7YlhPI40DQJT2dvCcuv1KZQkYkcOHIqiUI8SvwhL4Q?width=2880&amp;height=1800&amp;cropmode=none" width="75%" height="100%"></p>
<p>Repeat for your Kaggle username.</p>
<p>Now we can set the keys for the notebook as shown below (input the name of your key into <code>get_secret</code>).</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:29.196185Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:29.195768Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:29.201245Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:29.199576Z&quot;}" data-papermill="{&quot;duration&quot;:0.032545,&quot;end_time&quot;:&quot;2023-04-05T11:16:29.203954&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:29.171409&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="24">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><span class="im" style="color: #00769E;">import</span> os</span>
<span id="cb42-2"><span class="im" style="color: #00769E;">from</span> kaggle_secrets <span class="im" style="color: #00769E;">import</span> UserSecretsClient</span></code></pre></div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:29.247229Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:29.246767Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:29.732755Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:29.730826Z&quot;}" data-papermill="{&quot;duration&quot;:0.511238,&quot;end_time&quot;:&quot;2023-04-05T11:16:29.735938&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:29.224700&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="25">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">secrets <span class="op" style="color: #5E5E5E;">=</span> UserSecretsClient()</span>
<span id="cb43-2">os.environ[<span class="st" style="color: #20794D;">'KAGGLE_USERNAME'</span>] <span class="op" style="color: #5E5E5E;">=</span> secrets.get_secret(<span class="st" style="color: #20794D;">'KAGGLE_USERNAME'</span>)</span>
<span id="cb43-3">os.environ[<span class="st" style="color: #20794D;">'KAGGLE_KEY'</span>] <span class="op" style="color: #5E5E5E;">=</span> secrets.get_secret(<span class="st" style="color: #20794D;">'KAGGLE_KEY'</span>)</span></code></pre></div>
</div>
</section>
<section id="push-dataset" class="level2">
<h2 class="anchored" data-anchor-id="push-dataset">Push Dataset</h2>
<p>The <a href="fastkaggle">fastkaggle</a> library offers a convenient way to easily create and push a dataset to Kaggle.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:29.865172Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:29.864701Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:30.396398Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:30.394702Z&quot;}" data-papermill="{&quot;duration&quot;:0.558069,&quot;end_time&quot;:&quot;2023-04-05T11:16:30.399675&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:29.841606&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="26">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">doc(mk_dataset)</span></code></pre></div>
<div class="cell-output cell-output-display">
<hr>
<h3 class="anchored">mk_dataset</h3>
<blockquote class="blockquote"><pre><code>mk_dataset(dataset_path, title, force=False, upload=True)</code></pre></blockquote><p>Creates minimal dataset metadata needed to push new dataset to kaggle</p>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Note:</strong> Ignore the <code>if not iskaggle</code> statement when replicating. I added it since I edited this notebook and needed to save changes without reproducing the entire dataset.</p>
</blockquote>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:30.488813Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:30.487791Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:30.495264Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:30.494072Z&quot;}" data-papermill="{&quot;duration&quot;:0.034606,&quot;end_time&quot;:&quot;2023-04-05T11:16:30.498618&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:30.464012&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="27">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> iskaggle:</span>
<span id="cb45-2">    mk_dataset(<span class="st" style="color: #20794D;">'/kaggle/train_images'</span>, <span class="st" style="color: #20794D;">'spectrograms-birdclef-2023'</span>, force<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, upload<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<p>And we can verify our dataset has been created by having a look at the generated metadata file.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-04-05T11:16:30.591890Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-04-05T11:16:30.591361Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-04-05T11:16:30.597529Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-04-05T11:16:30.596042Z&quot;}" data-papermill="{&quot;duration&quot;:0.032864,&quot;end_time&quot;:&quot;2023-04-05T11:16:30.600152&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-04-05T11:16:30.567288&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="28">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> iskaggle:</span>
<span id="cb46-2">    <span class="op" style="color: #5E5E5E;">!</span> cat <span class="op" style="color: #5E5E5E;">/</span>kaggle<span class="op" style="color: #5E5E5E;">/</span>train_images<span class="op" style="color: #5E5E5E;">/</span>dataset<span class="op" style="color: #5E5E5E;">-</span>metadata.json</span></code></pre></div>
</div>
<p>From here, we can go directly to the dataset page on Kaggle and fill out the rest of the details.</p>
</section>
<section id="and-there-you-have-it" class="level2">
<h2 class="anchored" data-anchor-id="and-there-you-have-it">And there you have it!</h2>
<p>In summary, you saw how to: * Generate spectrogram images from audio files using torchaudio and fastai * How to cut audio tracks * And how to create and push a dataset directly on Kaggle</p>
<p>You can view the dataset that was generated from this notebook <a href="https://www.kaggle.com/datasets/forbo7/spectrograms-birdclef-2023">here</a>.</p>
<p>If you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!</p>


</section>

 ]]></description>
  <category>Computer Vision</category>
  <category>Data</category>
  <category>PyTorch</category>
  <guid>https://forbo7.github.io/forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html</guid>
  <pubDate>Wed, 05 Apr 2023 00:00:00 GMT</pubDate>
  <media:content url="https://forbo7.github.io/forblog/images/10_how_to_convert_audio_to_spectrogram_images/thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Transformers, Simply Explained</title>
  <dc:creator>Salman Naqvi</dc:creator>
  <link>https://forbo7.github.io/forblog/posts/9_transformers_explained.html</link>
  <description><![CDATA[ 



<p><img src="https://forbo7.github.io/forblog/images/9_transformers_explained/thumbnail.png" class="img-fluid" alt="A picture of Transformers — the ones that transform from a robot into cars — posing as English alphabets."></p>
<section id="at-a-high-view" class="level2">
<h2 class="anchored" data-anchor-id="at-a-high-view">At a High View</h2>
<p>Transformers are all the rage right now. They’re what’s powering the current wave of chat bots. Here’s a high level view of how transformers work, so you know how these bots really work.</p>
<p>Simply put, a transformer is a type of architecture used for Natural Language Processing (NLP) tasks that either fills-in-the-blanks or autocompletes.</p>
<p>Transformers consist of either an encoder, decoder, or both. Encoders and decoders contain attention layers.</p>
<p>Language models need numbers to work. Text is given a numerical representation after breaking it down into smaller pieces. To keep this explanation simple, these pieces are words.</p>
<p>The numerical representation given to a word describes the word itself and its relation to the surrounding words.</p>
</section>
<section id="encoders" class="level2">
<h2 class="anchored" data-anchor-id="encoders">Encoders</h2>
<p>Encoder-only transformers are good for “understanding” text, such as classifying sentences by sentiment or figuring out what parts of a sentence refers, for example, to a person or location.</p>
<p>When training encoders, words are given a numerical representation by the attention layers considering adjacent words. For example, let’s say we have the sentence, “I am really hungry.”. The attention layers consider the words ‘am’ and ‘hungry’ when giving the word ‘really’ a numerical representation.</p>
<p>The goal of training encoders is to predict words omitted from text (e.g., “I … really hungry.”). This is how encoders can “understand” text.</p>
</section>
<section id="decoders" class="level2">
<h2 class="anchored" data-anchor-id="decoders">Decoders</h2>
<p>Decoder-only transformers are good for text generation. An example is the autocomplete feature on a smartphone’s keyboard.</p>
<p>Decoders similary give text a numerical representation, except that the attention layers consider only the previous words. When giving ‘am’ a numerical representation from “I am hungry.”, the attention layers will only consider the word ‘I’. When giving ‘hungry’ a numerical representation, the attention layers will consider the words ‘I’ and ‘am’.</p>
<p>The goal of training decoders is to predict the most likely word to continue a piece of text (e.g., “I am ….”). All generated words are used in conjunction to generate the next word.</p>
</section>
<section id="encoders-and-decoders" class="level2">
<h2 class="anchored" data-anchor-id="encoders-and-decoders">Encoders and Decoders</h2>
<p>Transformers that use both encoders and decoders are known as encoder-decoder models or sequence-to-sequence models. Such models are good for translation and summarization.</p>
<p>Encoder-decoder models are trained by first letting the encoder give the input text a numerical representation. Next, this representation is input to the decoder which generates text as described above. The encoder part of the model provides the “understanding”, while the decoder part of the model generates based off of this “understanding”.</p>
</section>
<section id="closing-words" class="level2">
<h2 class="anchored" data-anchor-id="closing-words">Closing Words</h2>
<p>And there you have it! It’s as simple as that!</p>
<p>If you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!</p>


</section>

 ]]></description>
  <category>LLMs</category>
  <category>Transformers</category>
  <guid>https://forbo7.github.io/forblog/posts/9_transformers_explained.html</guid>
  <pubDate>Tue, 28 Feb 2023 00:00:00 GMT</pubDate>
  <media:content url="https://forbo7.github.io/forblog/images/9_transformers_explained/thumbnail.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch</title>
  <dc:creator>Salman Naqvi</dc:creator>
  <link>https://forbo7.github.io/forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html</link>
  <description><![CDATA[ 



<p><em>This blog post was updated on <strong>Saturday, 28 January 2023</strong>.</em></p>
<p><img src="https://forbo7.github.io/forblog/images/8_how_to_use_apple_gpu_with_pytorch/thumbnail.png" class="img-fluid" alt="A picture of a snake that has taken a bite out of an apple, and whose tail is a burning torch."></p>
<p>If you have one of those fancy Macs with an M-Series chip (M1/M2, etc.), here’s how to make use of its GPU in PyTorch for increased performance.</p>
<p>It’s a bit annoying and a little tedious, but here we go.</p>
<section id="requirements" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="requirements"><span class="header-section-number">1</span> Requirements</h2>
<ul>
<li>Have an M-Series chip</li>
<li>Have at least PyTorch 1.12</li>
<li>Have at least macOS Monterey 12.3</li>
</ul>
</section>
<section id="installing-pytorch" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="installing-pytorch"><span class="header-section-number">2</span> Installing PyTorch</h2>
<p>Install PyTorch as you usually would. Just make sure it’s PyTorch 1.12.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="co" style="color: #5E5E5E;"># Installing with Pip.</span></span>
<span id="cb1-2"><span class="ex" style="color: null;">$</span> pip3 install torch torchvision torchaudio</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;"># Installing using Conda.</span></span>
<span id="cb1-5"><span class="ex" style="color: null;">$</span> conda install pytorch torchvision torchaudio <span class="at" style="color: #657422;">-c</span> pytorch</span></code></pre></div>
<p>By using these commands, the latest version of the library is installed so there is no need to specify the version number.</p>
<p>However, if you have an existing installation, you can run the following Pip command instead.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><span class="ex" style="color: null;">$</span> pip3 install <span class="at" style="color: #657422;">--upgrade</span> torch torchvision torchaudio</span></code></pre></div>
</section>
<section id="import-pytorch" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="import-pytorch"><span class="header-section-number">3</span> Import PyTorch</h2>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;">import</span> torch</span></code></pre></div>
</div>
</section>
<section id="check-requirements-are-met" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="check-requirements-are-met"><span class="header-section-number">4</span> Check Requirements are Met</h2>
<p>Below is a convenient code snippet taken from the PyTorch documentation that checks whether requirements are met.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> torch.backends.mps.is_available():</span>
<span id="cb4-2">    <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> torch.backends.mps.is_built():</span>
<span id="cb4-3">        <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"MPS not available because the current PyTorch install was not built with MPS enabled."</span>)</span>
<span id="cb4-4">    <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb4-5">        <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine."</span>)</span></code></pre></div>
</div>
<p>If neither of the two above messages print, you’re good to go!</p>
</section>
<section id="the-annoying-part-enabling-the-gpu" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="the-annoying-part-enabling-the-gpu"><span class="header-section-number">5</span> The Annoying Part: Enabling the GPU</h2>
<p>As far as I know, you must explicitly enable the use of the GPU for whatever model or tensor you wish to use the GPU for.</p>
<p>There are different ways you can do this.</p>
<p><strong>Use a string.</strong></p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">t <span class="op" style="color: #5E5E5E;">=</span> torch.tensor([<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">3</span>], device<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'mps'</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="5">
<div class="cell-output cell-output-display" data-execution_count="5">
<blockquote class="blockquote">
<p><code>tensor([1, 2, 3], device='mps:0')</code></p>
</blockquote>
</div>
</div>
<p><strong>Store as a variable.</strong></p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">device<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'mps'</span></span>
<span id="cb6-2">t <span class="op" style="color: #5E5E5E;">=</span> torch.tensor([<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">3</span>], device<span class="op" style="color: #5E5E5E;">=</span>device)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="7">
<div class="cell-output cell-output-display" data-execution_count="7">
<blockquote class="blockquote">
<p><code>tensor([1, 2, 3], device='mps:0')</code></p>
</blockquote>
</div>
</div>
<p><strong>Convert existing objects.</strong></p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">t <span class="op" style="color: #5E5E5E;">=</span> torch.tensor([<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">3</span>])</span>
<span id="cb7-2">t.to(device)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="9">
<div class="cell-output cell-output-display" data-execution_count="9">
<blockquote class="blockquote">
<p><code>tensor([1, 2, 3], device='mps:0')</code></p>
</blockquote>
</div>
</div>
<p>Note that converting existing objects creates a copy and does not modify the original.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">t</span></code></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="cell-output cell-output-display" data-execution_count="11">
<blockquote class="blockquote">
<p><code>tensor([1, 2, 3])</code></p>
</blockquote>
</div>
</div>
<p>Though the above operations have been performed on tensors, they can also be performed on models.</p>
</section>
<section id="points-to-note" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="points-to-note"><span class="header-section-number">6</span> Points to Note</h2>
<ul>
<li><p>GPU enabled means operations are done on the GPU.</p></li>
<li><p>A GPU enabled tensor can only perform operations with another GPU enabled tensor.</p></li>
<li><p>As of writing this, GPU support is still in its early stages. So certain features are unsupported and further optimizations await.</p></li>
</ul>
</section>
<section id="relevant-links" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="relevant-links">Relevant Links</h2>
<p>Relevant links:</p>
<ul>
<li><p>Installing PyTorch: <a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a></p></li>
<li><p>Docs on using GPU: <a href="https://pytorch.org/docs/stable/notes/mps.html">https://pytorch.org/docs/stable/notes/mps.html</a></p></li>
<li><p>Performance gains (note that nightly builds are no longer needed): <a href="https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/">https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/</a></p></li>
</ul>
</section>
<section id="closing-words" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="closing-words">Closing Words</h2>
<p>If you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!</p>


</section>

 ]]></description>
  <category>PyTorch</category>
  <guid>https://forbo7.github.io/forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html</guid>
  <pubDate>Thu, 26 Jan 2023 00:00:00 GMT</pubDate>
  <media:content url="https://forbo7.github.io/forblog/images/8_how_to_use_apple_gpu_with_pytorch/thumbnail.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Adding Subscriptions to a Quarto Site</title>
  <dc:creator>Isaac Flath</dc:creator>
  <dc:creator>Salman Naqvi</dc:creator>
  <link>https://forbo7.github.io/forblog/posts/7_blog_subscriptions.html</link>
  <description><![CDATA[ 



<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/thumbnail.png" class="img-fluid"></p>
<p>The <a href="https://quarto.org/docs/websites/website-blog.html#subscriptions">Quarto Documenation</a> covers how to implement website subscriptions at a surface level. This guide goes into the details on how one could do so, with three different options. <strong>That said</strong>, this guide can also be helpful for sites that do not use Quarto.</p>
<p>The three ways this guide will cover:</p>
<ul>
<li><p><strong>Readymade Services</strong></p>
<p>These are services that handle and automate everything for you. MailChimp is mentioned in the Quarto Docs as one option, but is not covered in this guide as it appears they are depracting the RSS email feed function which is necessary. Instead, we have found MailerLite to be a suitable alternative that is easy to setup and use.</p></li>
<li><p><strong>Online Forms</strong></p>
<p>Though more manual, it’s good for just getting started or if you do not have an alternative address — many services like MailerLite require you to include a physical address in your emails. This options will dive into embedding forms, and gathering emails from there.</p></li>
<li><p><strong>HTML/JS</strong></p>
<p>For when you want to handle the frontend and the backend.</p></li>
</ul>
<p>Switch between the tabs below to view the steps for each option.</p>
<div class="panel-tabset">
<ul class="nav nav-tabs"><li class="nav-item"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" aria-controls="tabset-1-1" aria-selected="true">Services</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" aria-controls="tabset-1-2" aria-selected="false">Online Forms</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" aria-controls="tabset-1-3" aria-selected="false">HTML &amp; JS</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" aria-labelledby="tabset-1-1-tab">
<p>The first thing we need to do is create a MailerLite Campaign. That is what will actually send the email.</p>
<section id="quarto-setup" class="level3">
<h3 class="anchored" data-anchor-id="quarto-setup">Quarto Setup</h3>
<p>Make sure RSS feeds are enabled on your blog. Instructions for how to do this are in <a href="https://quarto.org/docs/websites/website-blog.html#rss-feed">the Quarto Documentation</a>.</p>
</section>
<section id="mailerlite-campaign-setup" class="level3">
<h3 class="anchored" data-anchor-id="mailerlite-campaign-setup">MailerLite Campaign Setup</h3>
<p><a href="https://www.mailerlite.com/signup">Create a MailerLite account</a></p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/mailerlite/MailerLite1_signup.png" class="img-fluid"></p>
<p>Once you have an account and are logged in, <a href="https://dashboard.mailerlite.com/campaigns/create">create an RSS Campaign</a>.</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/mailerlite/MailerLite2_NewCampaign.png" class="img-fluid"></p>
<p>As you complete the Campaign creation process there are a few key options to look out for.</p>
<p>As you progress through the signup form you will need to fill in some information and, including the URL of your RSS feed. It should be a URL that ends with <code>.xml</code>.</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/mailerlite/MailerLite3_RssFeedUrl.png" class="img-fluid"></p>
<p>I recommend setting the email to only be sent when you have new blog posts. This ensures that an email is only sent if you’ve published a new post. Otherwise, an email is sent on a regular interval with the latest posts regardless of whether there is new content.</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/mailerlite/MailerLite4_NewPostsOnly.png" class="img-fluid"></p>
<p>On the content page, choose start from scratch (free tier) or select a template (paid) and design your email that will go out.</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/mailerlite/MailerLite5_EmailDesign.png" class="img-fluid"></p>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>When you create an RSS campaign there are templates that can be used in the content tab for designing this email. These are paid features that you get for free for the first 30 days. Only use the templates if you intend to pay as they are not included in the free plan.</p>
</div>
</div>
<p>Select All Active Subscribers to send to.</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/mailerlite/MailerLite6_CampaignRecipients.png" class="img-fluid"></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Advanced Subscriber Settings
</div>
</div>
<div class="callout-body-container callout-body">
<p>You can create individual subscriber groups to have different campaigns go to different groups to give subscribers more options. <a href="https://isaac-flath.tech/blog.html">Example here</a>.</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/mailerlite/MailerLite7_Groups.png" class="img-fluid"></p>
</div>
</div>
<p>Continue through to schedule your campaign!</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/mailerlite/MailerLite8_Schedule.png" class="img-fluid"></p>
</section>
<section id="create-subscribe.html" class="level3">
<h3 class="anchored" data-anchor-id="create-subscribe.html">Create subscribe.html</h3>
<p>Now that the campaign is set up and will go out to all subscribers, we need to create the widget that allows users to subscribe to the blog. In other words we need a way for users to sign up! In Quarto, this is defined in the <code>subscribe.html</code> file. First, we need to use MailerLite to create the contents.</p>
<p>In MailerLite this is called an <code>embedded form</code>. We can use their GUI to <a href="https://dashboard.mailerlite.com/forms/create?type=embedded">Create an embedded form</a>.</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/mailerlite/MailerLite9_NewForm.png" class="img-fluid"></p>
<p>Once we start the form we can use the GUI form editor to design what we want the form to look like.</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/mailerlite/MailerLite11_FormDesign.png" class="img-fluid"></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>I recommend leaving it with the default design for now, you can always come back and re-style it later if you don’t like how it looks on your blog. But it’s much easier to get something working then improve upon it once it’s working than to try to make something perfect the first time through!</p>
</div>
</div>
<p>Once you created the form it will take you to that forms <code>Overview</code> page. Scroll down to look for the <code>Embed form into your website</code> section. In that section select <code>HTML Code</code> and copy the code provided.</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/mailerlite/MailerLite12_FormHtml.png" class="img-fluid"></p>
<p>Paste this code into a <code>subscribe.html</code> file at the top level of your blog’s directory.</p>
</section>
<section id="modify-_quarto.yml" class="level3">
<h3 class="anchored" data-anchor-id="modify-_quarto.yml">Modify _quarto.yml</h3>
<p>Add the <code>subscribe.html</code> file to your <code>_quarto.yml</code> file by adding it to the <code>margin-header</code> attribute. This option will look like this in your <code>_quarto.yml</code> file.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb1-1"><span class="fu" style="color: #4758AB;">website</span><span class="kw" style="color: #003B4F;">:</span></span>
<span id="cb1-2"><span class="co" style="color: #5E5E5E;">  # (additional metadata excluded for brevity)</span></span>
<span id="cb1-3"><span class="at" style="color: #657422;">  </span><span class="fu" style="color: #4758AB;">margin-header</span><span class="kw" style="color: #003B4F;">:</span><span class="at" style="color: #657422;"> subscribe.html</span></span></code></pre></div>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>That is all it takes to get subscriptions working on your blog with MailerLite! Everything you just set up is editable so if you don’t like how the email or the subscription widget looks, you can go in and edit your templates.</p>
</section>
<section id="live-example" class="level3">
<h3 class="anchored" data-anchor-id="live-example">Live example</h3>
<p>Check out <a href="https://isaac-flath.tech">Isaac Flath’s blog</a> to see the MailerLite widget in action!</p>
</section>
</div>
<div id="tabset-1-2" class="tab-pane" aria-labelledby="tabset-1-2-tab">
<p>Perhaps you don’t have an alternative address. Or perhaps you just want something simple to get started with. There’s still a way to implement blog subscriptions! It requires more manual effort, but gets the job done: embedding online forms (e.g., Google Forms, Microsoft Forms, etc.).</p>
<p>It involves embedding a form in your website, collecting responses from it, creating a mailing list from those responses, and then composing and sending an email with the list.</p>
<p>The example in the steps below use Google Forms, though it would be very similar to Microsoft Forms. The steps below should also generally apply to any other online forms service.</p>
<section id="step-1-create-the-form." class="level3">
<h3 class="anchored" data-anchor-id="step-1-create-the-form.">Step 1: Create the form.</h3>
<p>Using your online form provider of choice, create your form! A simple form would include a text box for inputting an email, with a simple check to see if the entered email is valid.</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/manual/1.png" class="img-fluid"></p>
<p>On Google Forms, you have an option to implement email checking with the following option.</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/manual/2.png" class="img-fluid"></p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/manual/3.png" class="img-fluid"></p>
<div class="callout-note callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>You may want to allow responses to be edited after submission, create a confirmation message, and disable a link to submit another response.</p>
<p>In Google Forms, these options can be toggled under the ‘Settings’ tab.</p>
</div>
</div>
</div>
</section>
<section id="step-2-obtain-the-embed-snippet." class="level3">
<h3 class="anchored" data-anchor-id="step-2-obtain-the-embed-snippet.">Step 2: Obtain the embed snippet.</h3>
<p>Obtain the HTML snippet which you can paste into your website’s source.</p>
<p>To do this, press send…</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/manual/4.png" class="img-fluid"></p>
<p>…go to the embed tab…</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/manual/5.png" class="img-fluid"></p>
<p>…and copy the snippet.</p>
</section>
<section id="step-3-embed-the-embed" class="level3">
<h3 class="anchored" data-anchor-id="step-3-embed-the-embed">Step 3: Embed the embed</h3>
<p>Paste the snippet whereever you want to put the form on your site!</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/manual/6.png" class="img-fluid"></p>
<p>You can adjust the size of the embed by tweaking these values.</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/manual/7.png" class="img-fluid"></p>
</section>
<section id="step-4-unsubscribing." class="level3">
<h3 class="anchored" data-anchor-id="step-4-unsubscribing.">Step 4: Unsubscribing.</h3>
<p>Repeat steps 1-3 above and create a form that would allow subscribers to unsubscribe from receiving notifications. Make sure this form is clearly accessible in your site.</p>
</section>
<section id="step-5-gathering-emails." class="level3">
<h3 class="anchored" data-anchor-id="step-5-gathering-emails.">Step 5: Gathering emails.</h3>
<p>Head to the responses tab of your form.</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/manual/8.png" class="img-fluid"></p>
<p>You can take these email addresses and create a mailing list in the email service of your choice.</p>
<p>You can also download a CSV file containing the responses.</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/manual/9.png" class="img-fluid"></p>
<p>Alternatively, you can also create a spreadsheet by clicking on the spreadsheet icon.</p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/manual/10.png" class="img-fluid"></p>
<p><img src="https://forbo7.github.io/forblog/images/7_blog_subscriptions/manual/11.png" class="img-fluid"></p>
<p><strong>At the same time,</strong> check the responses in your unsubscribe form and tally them against the responses received in your subscribe form. Remove any email addresses that need to be removed.</p>
</section>
<section id="step-6-compose-and-send" class="level3">
<h3 class="anchored" data-anchor-id="step-6-compose-and-send">Step 6: Compose and send!</h3>
<p>Now compose the email how you would like to, and hit that send button!</p>
<div class="callout-warning callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Before you hit that send button!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Make sure you include a clearly visible link in your email that would allow recipients to unsubscribe.</p>
</div>
</div>
</section>
<section id="step-0-extras" class="level3">
<h3 class="anchored" data-anchor-id="step-0-extras">Step 0: Extras</h3>
<div class="callout-tip callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>You could combine the subscribe and unsubscribe forms into a single form so it would be easier to manage. The form would initially ask if a user would like to subscribe or unsubscribe. Based on their input, the form would take them to the appropriate section.</p>
<p>Further expanding on this, if your site has multiple feeds, the form could also ask which feed the user would like to subscribe to or unsubscribe from.</p>
</div>
</div>
</div>
</section>
<section id="live-example-1" class="level3">
<h3 class="anchored" data-anchor-id="live-example-1">Live example</h3>
<p>Check out <a href="https://forbo7.github.io/forblog/">Salman Naqvi’s ForBlog</a> to see embedded forms in action!</p>
</section>
</div>
<div id="tabset-1-3" class="tab-pane" aria-labelledby="tabset-1-3-tab">
<section id="option-3" class="level2">
<h2 class="anchored" data-anchor-id="option-3">Option 3</h2>
<p>Perhaps you know some HTML and JS, or even only JS, and don’t have an alternative address. Instead of creating the frontend with HTML, try using the <a href="https://github.com/jlgraves-ubc/forms">Quarto HTML Forms</a> extension by <a href="https://github.com/jlgraves-ubc">Jonathan Graves</a>.</p>
<p>This extension allows you to implement HTML forms through <a href="">Quarto Shortcodes</a> and YAML Options. However, you still will need to handle the backend with JavaScript and perhaps a few other technologies. If you’re interested in implementing it this way, you probably already know how to. If not, there are plenty of great guides online!.</p>
</section>
</div>
</div>
</div>
<section id="acknowledgements" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgements">Acknowledgements</h2>
<p>Thanks to Isaac Flath for collaborating with me on this guide! You can view his blog, works, and contact <a href="https://isaac-flath.tech">here</a>.</p>


</section>

 ]]></description>
  <category>Quarto</category>
  <category>Collaboration</category>
  <guid>https://forbo7.github.io/forblog/posts/7_blog_subscriptions.html</guid>
  <pubDate>Fri, 23 Dec 2022 00:00:00 GMT</pubDate>
  <media:content url="https://forbo7.github.io/forblog/images/7_blog_subscriptions/thumbnail.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>AI in a Nutshell</title>
  <dc:creator>Salman Naqvi</dc:creator>
  <link>https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell.html</link>
  <description><![CDATA[ 



<p><em>This blog post was updated on <strong>Saturday, 12 November 2022</strong>.</em></p>
<p><img src="https://forbo7.github.io/forblog/images/6_ai_in_a_nutshell/thumbnail.png" class="img-fluid" alt="A circuit board inside a walnut."></p>
<p>Artificial Intelligence. Machine Learning. Neural Networks. Deep Learning. Fancy Words. Deceptively Simple. All really the same.</p>
<p>The basic workflow to create such a system is below.</p>
<div class="grid">
<div class="g-col-4">

</div>
<div class="g-col-4">
<div class="cell" data-fig-width="2">
<div class="cell-output-display">
<div>
<p>
</p><pre class="mermaid mermaid-js" data-tooltip-selector="#mermaid-tooltip-1">flowchart TB
    A[Function] -- fits --&gt; B[Data]
</pre>
<div id="mermaid-tooltip-1" class="mermaidTooltip">

</div>
<p></p>
</div>
</div>
</div>
</div>
<div class="g-col-4">

</div>
</div>
<p>Very simple, eh? Of course, it’s a very high level abstraction, but this high level view will make this seemingly complex topic very simple.</p>
<p>First, what’s the main thing modern AI methods try to do? They try to make predictions about certain things.</p>
<p>So a <strong>function</strong> of sorts is needed to achieve this. A function that can make these predictions. Think of a function as a machine. You put something into the machine and then, with whatever was input, the machine then produces an output.</p>
<p>The machine that we will be working with has two input slots: one slot is for <strong>training</strong> and the other slot is for predictions.</p>
<p>To create a function that produces predictions, we need to tell the function what sort of predictions it needs to make.</p>
<p>To do that, we can pour some data into the <strong>training</strong> slot. This data will tell the function what sort of predictions to output. This process is known as <strong>fitting</strong> the function to the data.</p>
<p>To fit the function onto data, you <strong>train</strong> the function.</p>
<section id="simple-case-quadratic-function" class="level2">
<h2 class="anchored" data-anchor-id="simple-case-quadratic-function">Simple Case: Quadratic Function</h2>
<p><em>Gasp!</em> A quadratic?? What’s this nonsense!</p>
<p>A quadratic is a very simple equation. When shown on a graph, it looks like this.</p>
<div class="cell" data-execution_count="5">
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-5-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We’ll be using this equation to demonstrate a very simple example.</p>
<p>The basic workflow for fitting a function to data is below.</p>
<div class="grid">
<div class="g-col-2">

</div>
<div class="g-col-8">
<div class="cell" data-fig-width="3.5">
<div class="cell-output-display">
<div>
<p>
</p><pre class="mermaid mermaid-js" data-tooltip-selector="#mermaid-tooltip-2">flowchart TB
    B[Calculate Loss] --&gt; C[Calculate Gradients] --&gt; D[Update Parameters] --&gt; B
</pre>
<div id="mermaid-tooltip-2" class="mermaidTooltip">

</div>
<p></p>
</div>
</div>
</div>
</div>
<div class="g-col-2">

</div>
</div>
<p>It can seem like a lot at first glance; quite a few new terms too.</p>
<p>We’ll break this down by going over the very simple example.</p>
<p>Let’s say we have the following data points that describe, say, the speed of an object with respect to time. We want to predict what the speed of an object would be outside these data points.</p>
<p>The horizontal axis is time and the vertical axis is the object’s speed.</p>
<div class="cell" data-tags="[]" data-execution_count="7">
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We can see that the data looks like the quadratic function shown above! Therefore, we could use the quadratic to predict what the speed of the object would be after 2.0 s and before -2.0 s.</p>
<p>A quadratic equation includes three numbers which we will call <img src="https://latex.codecogs.com/png.latex?a">, <img src="https://latex.codecogs.com/png.latex?b">, and <img src="https://latex.codecogs.com/png.latex?c">. These three numbers affect or control how our quadratic function will end up looking. <img src="https://latex.codecogs.com/png.latex?a">, <img src="https://latex.codecogs.com/png.latex?b">, and <img src="https://latex.codecogs.com/png.latex?c"> are our <strong>parameters</strong>.</p>
<p>Let’s let <img src="https://latex.codecogs.com/png.latex?a">, <img src="https://latex.codecogs.com/png.latex?b">, and <img src="https://latex.codecogs.com/png.latex?c"> all equal <img src="https://latex.codecogs.com/png.latex?1"> to begin with.</p>
<div class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-8-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Hmm, not a very good fit.</p>
<p>Let’s try another set of values for the parameters: <img src="https://latex.codecogs.com/png.latex?2">, <img src="https://latex.codecogs.com/png.latex?1">, <img src="https://latex.codecogs.com/png.latex?1.5">.</p>
<div class="cell" data-tags="[]" data-execution_count="9">
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-9-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Looking much better now!</p>
<p>Let’s see what <img src="https://latex.codecogs.com/png.latex?2">, <img src="https://latex.codecogs.com/png.latex?0">, and <img src="https://latex.codecogs.com/png.latex?1.5"> gives us.</p>
<div class="cell" data-execution_count="10">
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Eyeballing this is difficult. A certain set of parameters we use may be good by looking at the resulting graph, but in reality, it may not be.</p>
<p>What we need is something that can tell us how good our function is; something that tells us whether the changes we are making are actually good or not. To do this, we can calculate a number called the <strong>loss</strong>. The smaller the loss, the better the function is.</p>
<p>There are many different ways loss can be calculated. The way we will be doing it is known as <strong>mean absolute error (MAE)</strong>. In simple terms, it tells us how far off each prediction is from the actual value. For example, if we have a MAE of 1, this means that, on average, each prediction we make is 1 unit off from the real value.</p>
<p>In our case, a MAE of 1 would mean that each prediction is on average 1 m/s off from the real value.</p>
<p>Let’s repeat what we did above, but this time, we’ll also see what the MAE is.</p>
<div class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-12-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Again, this means that on average, each prediction we will make is 2.61 m/s off from the real value.</p>
<div class="cell" data-execution_count="13">
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>That’s a big jump!</p>
<div class="cell" data-execution_count="14">
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Hmm, things got worse.</p>
<p>Doing this process by hand is very tedious. How do we know if the new set of parameters we are using would improve the function? There needs to be a way to automate this so we don’t have to sit down and do this by hand.</p>
<p>What we can do is update the parameters based on the loss. This would in turn create new parameters that would decrease the loss.</p>
<div class="grid">
<div class="g-col-4">

</div>
<div class="g-col-4">
<div class="cell" data-fig-width="2.2">
<div class="cell-output-display">
<div>
<p>
</p><pre class="mermaid mermaid-js" data-tooltip-selector="#mermaid-tooltip-3">flowchart TB
    A[Loss] -- Updates ---&gt; B[Parameters] -- Updates ---&gt; A
</pre>
<div id="mermaid-tooltip-3" class="mermaidTooltip">

</div>
<p></p>
</div>
</div>
</div>
</div>
<div class="g-col-4">

</div>
</div>
<p>Let’s give <img src="https://latex.codecogs.com/png.latex?a">, <img src="https://latex.codecogs.com/png.latex?b">, and <img src="https://latex.codecogs.com/png.latex?c"> an arbitrary set of parameters <img src="https://latex.codecogs.com/png.latex?1.1">, <img src="https://latex.codecogs.com/png.latex?1.1">, and <img src="https://latex.codecogs.com/png.latex?1.1">.</p>
<p>Now let’s create a quadratic with this set of parameters and calculate its mean absolute error.</p>
<div class="cell" data-execution_count="16">
<div class="cell-output cell-output-display">
<div class="callout-tip callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Code Output
</div>
</div>
<div class="callout-body-container callout-body">
<p>The MAE is 2.42.</p>
</div>
</div>
</div>
</div>
<p>Now comes the next step: how do we update the parameters based on this loss we have calculated?</p>
<p>To do this, we calculate a new set of quantities known as the gradients. Each parameter has its own gradient.</p>
<p>Let’s say <img src="https://latex.codecogs.com/png.latex?a"> has the value of <img src="https://latex.codecogs.com/png.latex?1">. If <img src="https://latex.codecogs.com/png.latex?a"> has a gradient of value <img src="https://latex.codecogs.com/png.latex?0.5">, this would mean that if we increase <img src="https://latex.codecogs.com/png.latex?a"> by <img src="https://latex.codecogs.com/png.latex?1">, the loss would increase by <img src="https://latex.codecogs.com/png.latex?0.5">. Therefore, if we <em>decrease</em> <img src="https://latex.codecogs.com/png.latex?a"> by <img src="https://latex.codecogs.com/png.latex?1">, this would mean the loss would decrease by <img src="https://latex.codecogs.com/png.latex?0.5">, which is what we want!</p>
<p>Read over this once more and it’ll make sense!</p>
<p>Let’s quickly go over the inverse: if <img src="https://latex.codecogs.com/png.latex?a"> has a gradient of value <img src="https://latex.codecogs.com/png.latex?-0.5">, increasing <img src="https://latex.codecogs.com/png.latex?a"> by <img src="https://latex.codecogs.com/png.latex?1"> would decrease the loss by <img src="https://latex.codecogs.com/png.latex?0.5"> — again, this is what we want! Similarly, decreasing <img src="https://latex.codecogs.com/png.latex?a"> by <img src="https://latex.codecogs.com/png.latex?1"> would increase the loss by <img src="https://latex.codecogs.com/png.latex?0.5">.</p>
<p>The gradients are calculated from the loss. Then the gradients, the current parameters, and along with another value, the parameters are updated to new values. The “another value” is known as the <strong>learning rate</strong>. The learning rate controls how much the gradients update the parameters.</p>
<div class="grid">
<div class="g-col-2">

</div>
<div class="g-col-8">
<div class="cell" data-fig-width="5">
<div class="cell-output-display">
<div>
<p>
</p><pre class="mermaid mermaid-js" data-tooltip-selector="#mermaid-tooltip-4">flowchart TB
    A[Gradients]
    B[Current Parameters]
    C[Learning Rate]
    D[Magical Box]
    E[Updated Paramters]
    A &amp; B &amp; C ---&gt; D ---&gt; E
</pre>
<div id="mermaid-tooltip-4" class="mermaidTooltip">

</div>
<p></p>
</div>
</div>
</div>
</div>
<div class="g-col-2">

</div>
</div>
<p>Lets see this tangibly.</p>
<div class="cell" data-execution_count="17">
<div class="cell-output cell-output-display">
<div class="callout-tip callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Code Output
</div>
</div>
<div class="callout-body-container callout-body">
<p>The gradients for each parameter respectively are [-1.35, -0.03, -0.5].</p>
</div>
</div>
</div>
</div>
<p>Okay, let’s break this down. The gradient for the first parameter <img src="https://latex.codecogs.com/png.latex?a"> is <img src="https://latex.codecogs.com/png.latex?-1.35">. This tells us that if we increase the parameter <img src="https://latex.codecogs.com/png.latex?a"> by <img src="https://latex.codecogs.com/png.latex?1">, our loss will decrease by <img src="https://latex.codecogs.com/png.latex?-1.35">. Similary, if we increase the parameter <img src="https://latex.codecogs.com/png.latex?b"> by <img src="https://latex.codecogs.com/png.latex?1">, this will result in the loss being decreased by <img src="https://latex.codecogs.com/png.latex?-0.03">. The same logic holds for <img src="https://latex.codecogs.com/png.latex?c">.</p>
<p>Let’s now update the parameters. Remember, the current set of parameters, their gradients, and the learning rate all update the current set of parameters to new values.</p>
<div class="cell" data-execution_count="18">
<div class="cell-output cell-output-display">
<div class="callout-tip callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Code Output
</div>
</div>
<div class="callout-body-container callout-body">
<p>The new parameters are [1.11, 1.1, 1.11].</p>
</div>
</div>
</div>
</div>
<p>We can now repeat the process as many times as desired. Let’s do it 4 times.</p>
<div class="cell" data-execution_count="19">
<div class="cell-output cell-output-stdout">
<pre><code>Pass: 0; Loss: 2.4010409560416095
Pass: 1; Loss: 1.9847692009423128
Pass: 2; Loss: 1.498316818239171
Pass: 3; Loss: 1.171195547258246</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="callout-tip callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Code Output
</div>
</div>
<div class="callout-body-container callout-body">
<p>The MAE after 4 passes is 1.17.</p>
</div>
</div>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-19-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>And there you go! An even better fitting quadratic!</p>
<p>Let’s see what the object’s speed is at 1 second.</p>
<div class="cell" data-execution_count="20">
<div class="cell-output cell-output-display">
<div class="callout-tip callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Code Output
</div>
</div>
<div class="callout-body-container callout-body">
<p>The object’s velocity at 1 seconds is 5.65 m/s.</p>
</div>
</div>
</div>
</div>
<p>That roughly seems right!</p>
<p>Let’s see what the object’s speed would be at 3 seconds.</p>
<div class="cell" data-execution_count="21">
<div class="cell-output cell-output-display">
<div class="callout-tip callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Code Output
</div>
</div>
<div class="callout-body-container callout-body">
<p>The object’s velocity at 1 seconds is 30.31 m/s.</p>
</div>
</div>
</div>
</div>
<p>And now, the diagram below should make sense!</p>
<div class="grid">
<div class="g-col-2">

</div>
<div class="g-col-8">
<div class="cell" data-fig-width="3.5">
<div class="cell-output-display">
<div>
<p>
</p><pre class="mermaid mermaid-js" data-tooltip-selector="#mermaid-tooltip-5">flowchart TB
    B[Calculate Loss] --&gt; C[Calculate Gradients] --&gt; D[Update Parameters] --&gt; B
</pre>
<div id="mermaid-tooltip-5" class="mermaidTooltip">

</div>
<p></p>
</div>
</div>
</div>
</div>
<div class="g-col-2">

</div>
</div>
</section>
<section id="the-cool-case-relus" class="level2">
<h2 class="anchored" data-anchor-id="the-cool-case-relus">The Cool Case: ReLUs</h2>
<p>The quadratic example above is a nice, simple way to get a grasp of things. However, you may be wondering, “What if the data doesn’t follow a quadratic shape? What do we do then?”</p>
<p>And that’s a good question! What if our data doesn’t follow any sort of mathematical shape? What if we don’t even know the shape the data will follow? How do we know what function to use in that case?</p>
<p>There is a solution to that! There is an easy way to create a function that bends and twists itself to fit the data; an “unbound” function of sorts, as I like to call it.</p>
<p>This can be achieved by using another equation known as the <strong>ReLU</strong>. Another fancy word that can make you sound like a professional, while also being really simple. ReLU is short for <strong>Rectified Linear Unit</strong>.</p>
<p>The ReLU takes any value that is less than 0, and converts to 0.</p>
<p>Let’s see this.</p>
<p>Take the following line. It has both positive and negative values on the vertical axis.</p>
<div class="cell" data-execution_count="22">
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-22-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>When we use a ReLU, all negative values are converted to zero.</p>
<div class="cell" data-execution_count="23">
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-23-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s return to our original data.</p>
<div class="cell" data-execution_count="24">
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-24-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Now a single ReLU won’t work as seen below.</p>
<div class="cell" data-execution_count="25">
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-25-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Even after we try to fit it.</p>
<div class="cell" data-execution_count="26">
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-26-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>But look at what happens when two ReLUs are, literally, added together!</p>
<div class="cell" data-execution_count="27">
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-27-output-1.png" class="img-fluid"></p>
</div>
</div>
<!-- - Seed 22; 196 passes; lr=0.01
- Seed 33; 49 passes; lr=0.1
- Seed 16; 20 passes; lr=0.5 -->
<p>Pretty neat, hey?</p>
<p>Let’s add a third ReLU to the mix.</p>
<!-- #| output: false
- Seed 16; 35 passes; lr=0.5
- Seed 33: 35 passes; lr=0.1
- Seed 10: 45 passes; lr=0.1
- Seed 33; 50 passes; lr=0.05
- Seed 38, 196 passes; lr=0.01 -->
<div class="cell" data-execution_count="31">
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-31-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>You can see here how the function is adapting to the shape of the data.</p>
<p>With some extra experimentation, I was able to get the loss down to 1.08!</p>
<div class="cell" data-execution_count="32">
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-32-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>That said, it’s not too much of a difference when compared to two ReLUs.</p>
<p>What if we add 5 more to the mix, for a total of 8?</p>
<div class="cell" data-execution_count="35">
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-35-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Nice! The MAE has gone below 1!</p>
<p>It’s even beat the quadratic function from before! With some expermimenting, I had managed to get the quadratic’s loss down to 1.03.</p>
<div class="cell" data-execution_count="36">
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell_files/figure-html/cell-36-output-1.png" class="img-fluid"></p>
</div>
</div>
<!-- - Seed 16; 36 passes; lr=0.1
- Seed 42; 37 passes; lr=0.5 -->
<p>Let’s use the model that has 8 ReLUs to predict what the object’s velocity would be at 1 second.</p>
<div class="cell" data-execution_count="39">
<div class="cell-output cell-output-display">
<div class="callout-tip callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Code Output
</div>
</div>
<div class="callout-body-container callout-body">
<p>The object’s speed at 1 s is 4.9 m/s.</p>
</div>
</div>
</div>
</div>
<p>Hmm, yes, that is a bit off. But that is fine because overall, the function is a lot more accurate for all the datapoints.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>See how easy this stuff all is? All those fancy terms makes this feel complex when in reality, it’s all really simple.</p>
<p>Why not now go and venture off to learn more and implement your own models!</p>
<p>Below are two free courses I can recommend:</p>
<ul>
<li><p><a href="https://www.elementsofai.com">Elements of AI</a></p>
<p>A great primer into AI. The course goes over the history, the implementations, and the implications of this field, all without needing the knowledge of programming or complex mathematics.</p></li>
<li><p><a href="https://course.fast.ai">Practical Deep Learning for Coders</a></p>
<p>This course is different from other AI courses you’ll find. How? Because instead of starting off with the nitty gritty basics, you begin by actually implementing your own simple image classifier (a model that can tell what thing is in an image). You’ll be surprised at how simple it is to implement models with minimal code, and how little you need to know to get started (hint: you only really need high-school maths).</p></li>
</ul>
<p><strong>If you have any questions, comments, suggestions, or feedback, please do post them down in the comment section below!</strong></p>
</section>
<section id="acknowledgements" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgements">Acknowledgements</h2>
<p>This article was inspired by the <a href="https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work/notebook">How does a neural net really work</a> Kaggle Notebook by <a href="https://www.kaggle.com/jhoward">Jeremy Howard</a>, and lesson 3 of <a href="https://course.fast.ai/Lessons/lesson3.html">Practical Deep Learning for Coders</a>.</p>


</section>

 ]]></description>
  <category>Creating Models</category>
  <guid>https://forbo7.github.io/forblog/posts/6_ai_in_a_nutshell.html</guid>
  <pubDate>Tue, 04 Oct 2022 00:00:00 GMT</pubDate>
  <media:content url="https://forbo7.github.io/forblog/images/6_ai_in_a_nutshell/thumbnail.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Detecting Floods for Disaster Relief</title>
  <dc:creator>Salman Naqvi</dc:creator>
  <link>https://forbo7.github.io/forblog/posts/5_detecting_floods_for_disaster_relief.html</link>
  <description><![CDATA[ 



<p><strong>You can find this notebook on Kaggle <a href="https://www.kaggle.com/code/forbo7/flood-classifier">here</a>.</strong></p>
<p><em>This article was updated on <strong>Friday, 11 November 2022</strong>.</em></p>
<!-- TODO: Rotate image. -->
<p><img src="https://forbo7.github.io/forblog/images/5_detecting_floods_for_disaster_relief/thumbnail.png" class="img-fluid" alt="An image of a house on top of a pinnacle of rock surrounded by water."></p>
<p>The model that will be created in this notebook can detect whether an area shown in an image is flooded or not. The idea for creating this model has been spurred from the recent floodings in Pakistan.</p>
<p>Such models can prove useful in flood relief, helping to detect which areas need immediate focus.</p>
<p>The dataset used to train this model is <strong>Louisiana flood 2016</strong>, uploaded by Kaggle user <strong>Rahul T P</strong>, which you can view <a href="https://www.kaggle.com/datasets/rahultp97/louisiana-flood-2016">here</a>.</p>
<p>The fastai library, a high level PyTorch library, has been used.</p>
<p>One of the points of this notebook is to showcase how simple it is to create powerful models. That said, this notebook is <strong>not</strong> a tutorial or guide.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:18:05.493309Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:18:05.492447Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:18:08.742561Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:18:08.741405Z&quot;}" data-papermill="{&quot;duration&quot;:3.270399,&quot;end_time&quot;:&quot;2022-09-11T07:18:08.745299&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:18:05.474900&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> fastai.vision.<span class="bu" style="color: null;">all</span> <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span></code></pre></div>
</div>
<section id="sort-data." class="level2">
<h2 class="anchored" data-anchor-id="sort-data.">Sort data.</h2>
<p>The data in the dataset needs to be organized into <em>train</em> and <em>valid</em> folders. Each will contain the same subfolders, <em>0</em> and <em>1</em>, which will be used to label the data. A label of <code>0</code> indicates the area shown in the image is not flooded, while a label of <code>1</code> indicates the area shown in the image is flooded.</p>
<p>The images in the dataset itself has been organized as follows:</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;If no underscore is in the file name, the image shows the area before or after the flood.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;If an underscore is in the file name, the image shows the area during the flood:</p>
<ul>
<li>If a zero follows the underscore, the area was not flooded.</li>
<li>If a one follows the underscore, the area was flooded.</li>
</ul>
<p>Creating the necessary paths.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:18:08.829498Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:18:08.828333Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:18:08.834978Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:18:08.833986Z&quot;}" data-papermill="{&quot;duration&quot;:0.021492,&quot;end_time&quot;:&quot;2022-09-11T07:18:08.838729&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:18:08.817237&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">working_path <span class="op" style="color: #5E5E5E;">=</span> Path.cwd()<span class="op" style="color: #5E5E5E;">;</span> <span class="bu" style="color: null;">print</span>(working_path)</span>
<span id="cb2-2">folders <span class="op" style="color: #5E5E5E;">=</span> (<span class="st" style="color: #20794D;">'train'</span>, <span class="st" style="color: #20794D;">'valid'</span>)</span>
<span id="cb2-3">labels <span class="op" style="color: #5E5E5E;">=</span> (<span class="st" style="color: #20794D;">'0'</span>, <span class="st" style="color: #20794D;">'1'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>/kaggle/working</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:18:08.860991Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:18:08.859510Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:18:08.944909Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:18:08.943807Z&quot;}" data-papermill="{&quot;duration&quot;:0.099093,&quot;end_time&quot;:&quot;2022-09-11T07:18:08.947775&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:18:08.848682&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">input_path <span class="op" style="color: #5E5E5E;">=</span> Path(<span class="st" style="color: #20794D;">'/kaggle/input'</span>)</span>
<span id="cb4-2">train_image_paths <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">sorted</span>(input_path.rglob(<span class="st" style="color: #20794D;">'train/*.png'</span>))</span>
<span id="cb4-3">valid_image_paths <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">sorted</span>(input_path.rglob(<span class="st" style="color: #20794D;">'test/*.png'</span>))</span>
<span id="cb4-4"><span class="bu" style="color: null;">len</span>(train_image_paths), <span class="bu" style="color: null;">len</span>(valid_image_paths)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>(270, 52)</code></pre>
</div>
</div>
<p>Creating the necessary directories.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:18:08.990714Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:18:08.990359Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:18:08.996844Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:18:08.995770Z&quot;}" data-papermill="{&quot;duration&quot;:0.019674,&quot;end_time&quot;:&quot;2022-09-11T07:18:08.998954&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:18:08.979280&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="cf" style="color: #003B4F;">for</span> folder <span class="kw" style="color: #003B4F;">in</span> folders:</span>
<span id="cb6-2">    <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> (working_path<span class="op" style="color: #5E5E5E;">/</span>folder).exists():</span>
<span id="cb6-3">        (working_path<span class="op" style="color: #5E5E5E;">/</span>folder).mkdir()</span>
<span id="cb6-4">    <span class="cf" style="color: #003B4F;">for</span> label <span class="kw" style="color: #003B4F;">in</span> labels:</span>
<span id="cb6-5">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> (working_path<span class="op" style="color: #5E5E5E;">/</span>folder<span class="op" style="color: #5E5E5E;">/</span>label).exists():</span>
<span id="cb6-6">            (working_path<span class="op" style="color: #5E5E5E;">/</span>folder<span class="op" style="color: #5E5E5E;">/</span>label).mkdir()</span></code></pre></div>
</div>
<p>Move images to new directories.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:18:09.042253Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:18:09.041352Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:18:12.002238Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:18:12.001094Z&quot;}" data-papermill="{&quot;duration&quot;:2.974352,&quot;end_time&quot;:&quot;2022-09-11T07:18:12.004531&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:18:09.030179&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb7-2">    <span class="cf" style="color: #003B4F;">for</span> image_path <span class="kw" style="color: #003B4F;">in</span> train_image_paths:</span>
<span id="cb7-3">        <span class="cf" style="color: #003B4F;">if</span> <span class="st" style="color: #20794D;">'_1'</span> <span class="kw" style="color: #003B4F;">in</span> image_path.stem:</span>
<span id="cb7-4">            <span class="cf" style="color: #003B4F;">with</span> (working_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'train'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'1'</span><span class="op" style="color: #5E5E5E;">/</span>image_path.name).<span class="bu" style="color: null;">open</span>(mode<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xb'</span>) <span class="im" style="color: #00769E;">as</span> f:</span>
<span id="cb7-5">                f.write(image_path.read_bytes())</span>
<span id="cb7-6">        <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb7-7">            <span class="cf" style="color: #003B4F;">with</span> (working_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'train'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'0'</span><span class="op" style="color: #5E5E5E;">/</span>image_path.name).<span class="bu" style="color: null;">open</span>(mode<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xb'</span>) <span class="im" style="color: #00769E;">as</span> f:</span>
<span id="cb7-8">                f.write(image_path.read_bytes())</span>
<span id="cb7-9"><span class="cf" style="color: #003B4F;">except</span> <span class="pp" style="color: #AD0000;">FileExistsError</span>:</span>
<span id="cb7-10">    <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Training images have already been moved."</span>)</span>
<span id="cb7-11"><span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb7-12">    <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Training images moved."</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training images moved.</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:18:12.028069Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:18:12.026523Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:18:12.395600Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:18:12.394388Z&quot;}" data-papermill="{&quot;duration&quot;:0.382827,&quot;end_time&quot;:&quot;2022-09-11T07:18:12.398173&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:18:12.015346&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb9-2">    <span class="cf" style="color: #003B4F;">for</span> image_path <span class="kw" style="color: #003B4F;">in</span> valid_image_paths:</span>
<span id="cb9-3">        <span class="cf" style="color: #003B4F;">if</span> <span class="st" style="color: #20794D;">'_1'</span> <span class="kw" style="color: #003B4F;">in</span> image_path.stem:</span>
<span id="cb9-4">            <span class="cf" style="color: #003B4F;">with</span> (working_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'valid'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'1'</span><span class="op" style="color: #5E5E5E;">/</span>image_path.name).<span class="bu" style="color: null;">open</span>(mode<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xb'</span>) <span class="im" style="color: #00769E;">as</span> f:</span>
<span id="cb9-5">                f.write(image_path.read_bytes())</span>
<span id="cb9-6">        <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb9-7">            <span class="cf" style="color: #003B4F;">with</span> (working_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'valid'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'0'</span><span class="op" style="color: #5E5E5E;">/</span>image_path.name).<span class="bu" style="color: null;">open</span>(mode<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xb'</span>) <span class="im" style="color: #00769E;">as</span> f:</span>
<span id="cb9-8">                f.write(image_path.read_bytes())</span>
<span id="cb9-9"><span class="cf" style="color: #003B4F;">except</span> <span class="pp" style="color: #AD0000;">FileExistsError</span>:</span>
<span id="cb9-10">    <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Testing images have already been moved."</span>)</span>
<span id="cb9-11"><span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb9-12">    <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Testing images moved."</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Testing images moved.</code></pre>
</div>
</div>
<p>Check that images have been moved.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:18:12.442516Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:18:12.441495Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:18:12.452149Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:18:12.450721Z&quot;}" data-papermill="{&quot;duration&quot;:0.024352,&quot;end_time&quot;:&quot;2022-09-11T07:18:12.454314&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:18:12.429962&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">training_images <span class="op" style="color: #5E5E5E;">=</span> get_image_files(working_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'train'</span>)<span class="op" style="color: #5E5E5E;">;</span> <span class="bu" style="color: null;">print</span>(<span class="bu" style="color: null;">len</span>(training_images))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>270</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:18:12.477823Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:18:12.477117Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:18:12.688255Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:18:12.687359Z&quot;}" data-papermill="{&quot;duration&quot;:0.226811,&quot;end_time&quot;:&quot;2022-09-11T07:18:12.691568&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:18:12.464757&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">Image.<span class="bu" style="color: null;">open</span>(training_images[<span class="dv" style="color: #AD0000;">0</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<p><img src="https://forbo7.github.io/forblog/posts/5_detecting_floods_for_disaster_relief_files/figure-html/cell-9-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:18:12.719927Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:18:12.719594Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:18:12.728327Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:18:12.727274Z&quot;}" data-papermill="{&quot;duration&quot;:0.025912,&quot;end_time&quot;:&quot;2022-09-11T07:18:12.730913&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:18:12.705001&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">validation_images <span class="op" style="color: #5E5E5E;">=</span> get_image_files(working_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'valid'</span>)<span class="op" style="color: #5E5E5E;">;</span> <span class="bu" style="color: null;">print</span>(<span class="bu" style="color: null;">len</span>(validation_images))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>52</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:18:12.760100Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:18:12.759783Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:18:12.880274Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:18:12.879306Z&quot;}" data-papermill="{&quot;duration&quot;:0.139112,&quot;end_time&quot;:&quot;2022-09-11T07:18:12.884616&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:18:12.745504&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">Image.<span class="bu" style="color: null;">open</span>(validation_images[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<p><img src="https://forbo7.github.io/forblog/posts/5_detecting_floods_for_disaster_relief_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="load-data" class="level2">
<h2 class="anchored" data-anchor-id="load-data">Load data</h2>
<p>Create the training and validation dataloaders through fastai’s quick and easy <code>DataBlock</code> class.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:18:12.985360Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:18:12.985000Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:18:16.619229Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:18:16.618265Z&quot;}" data-papermill="{&quot;duration&quot;:3.654376,&quot;end_time&quot;:&quot;2022-09-11T07:18:16.621871&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:18:12.967495&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">dataloaders <span class="op" style="color: #5E5E5E;">=</span> DataBlock(</span>
<span id="cb17-2">    blocks <span class="op" style="color: #5E5E5E;">=</span> (ImageBlock, CategoryBlock),</span>
<span id="cb17-3">    get_items <span class="op" style="color: #5E5E5E;">=</span> get_image_files,</span>
<span id="cb17-4">    splitter <span class="op" style="color: #5E5E5E;">=</span> GrandparentSplitter(),</span>
<span id="cb17-5">    get_y <span class="op" style="color: #5E5E5E;">=</span> parent_label,</span>
<span id="cb17-6">    item_tfms <span class="op" style="color: #5E5E5E;">=</span> [Resize(<span class="dv" style="color: #AD0000;">192</span>, method<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'squish'</span>)]</span>
<span id="cb17-7">).dataloaders(working_path, bs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">32</span>)</span></code></pre></div>
</div>
<p>Check that data has been loaded correctly.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:18:16.690449Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:18:16.689337Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:18:17.569325Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:18:17.568486Z&quot;}" data-papermill="{&quot;duration&quot;:0.904393,&quot;end_time&quot;:&quot;2022-09-11T07:18:17.576236&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:18:16.671843&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">dataloaders.show_batch(max_n<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">8</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/5_detecting_floods_for_disaster_relief_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="instantiate-and-train-model" class="level2">
<h2 class="anchored" data-anchor-id="instantiate-and-train-model">Instantiate and Train Model</h2>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:18:17.666206Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:18:17.665830Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:18:58.064050Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:18:58.062950Z&quot;}" data-papermill="{&quot;duration&quot;:40.424136,&quot;end_time&quot;:&quot;2022-09-11T07:18:58.066398&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:18:17.642262&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="13">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">learner <span class="op" style="color: #5E5E5E;">=</span> vision_learner(dataloaders, resnet18, metrics<span class="op" style="color: #5E5E5E;">=</span>error_rate)</span>
<span id="cb19-2">learner.fine_tune(<span class="dv" style="color: #AD0000;">9</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"bdd89b090c3944d5a5462767fc8bf29d","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.919323</td>
      <td>1.118264</td>
      <td>0.365385</td>
      <td>00:09</td>
    </tr>
  </tbody>
</table>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.490039</td>
      <td>0.628054</td>
      <td>0.250000</td>
      <td>00:02</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.367996</td>
      <td>0.411558</td>
      <td>0.192308</td>
      <td>00:02</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.266664</td>
      <td>0.472146</td>
      <td>0.192308</td>
      <td>00:02</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.203069</td>
      <td>0.256436</td>
      <td>0.115385</td>
      <td>00:03</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.158453</td>
      <td>0.127106</td>
      <td>0.076923</td>
      <td>00:03</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.124499</td>
      <td>0.095927</td>
      <td>0.038462</td>
      <td>00:02</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.098409</td>
      <td>0.089279</td>
      <td>0.038462</td>
      <td>00:03</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.079600</td>
      <td>0.093277</td>
      <td>0.038462</td>
      <td>00:02</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.064886</td>
      <td>0.090372</td>
      <td>0.038462</td>
      <td>00:02</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<p>Nice! A relatively low error rate for no tweaking.</p>
</section>
<section id="visualizing-mistakes" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-mistakes">Visualizing Mistakes</h2>
<p>We have to see how the model is getting confuzzled.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:18:58.241827Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:18:58.241361Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:18:59.575155Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:18:59.573757Z&quot;}" data-papermill="{&quot;duration&quot;:1.361381,&quot;end_time&quot;:&quot;2022-09-11T07:18:59.579075&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:18:58.217694&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="14">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">interp <span class="op" style="color: #5E5E5E;">=</span> ClassificationInterpretation.from_learner(learner)</span>
<span id="cb21-2">interp.plot_confusion_matrix()</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/5_detecting_floods_for_disaster_relief_files/figure-html/cell-15-output-5.png" class="img-fluid"></p>
</div>
</div>
<p>Only a couple of mistakes. Let’s see what they are.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:18:59.724320Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:18:59.723919Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:19:00.338604Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:19:00.337405Z&quot;}" data-papermill="{&quot;duration&quot;:0.648114,&quot;end_time&quot;:&quot;2022-09-11T07:19:00.341972&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:18:59.693858&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="15">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">interp.plot_top_losses(<span class="dv" style="color: #AD0000;">5</span>, nrows<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/5_detecting_floods_for_disaster_relief_files/figure-html/cell-16-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>Nothing has been mislabeled, but the first one is especially tricky to determine, even for human eyes.</p>
</section>
<section id="model-inference" class="level2">
<h2 class="anchored" data-anchor-id="model-inference">Model Inference</h2>
<p>Let’s test the model on some images of the recent flooding in Pakistan.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:19:00.558615Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:19:00.558198Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:19:00.565882Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:19:00.565018Z&quot;}" data-papermill="{&quot;duration&quot;:0.038169,&quot;end_time&quot;:&quot;2022-09-11T07:19:00.567989&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:19:00.529820&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="16">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="kw" style="color: #003B4F;">def</span> infer_image(image_path):</span>
<span id="cb23-2">    display(Image.<span class="bu" style="color: null;">open</span>(image_path))</span>
<span id="cb23-3">    label, _, probabilities <span class="op" style="color: #5E5E5E;">=</span> learner.predict(PILImage(PILImage.create(image_path)))</span>
<span id="cb23-4">    <span class="cf" style="color: #003B4F;">if</span> label <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'0'</span>:</span>
<span id="cb23-5">        <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"The area shown in the image is not flooded with probability </span><span class="sc" style="color: #5E5E5E;">{</span>probabilities[<span class="dv" style="color: #AD0000;">0</span>]<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">100</span><span class="sc" style="color: #5E5E5E;">:.2f}</span><span class="ss" style="color: #20794D;">%."</span>)</span>
<span id="cb23-6">    <span class="cf" style="color: #003B4F;">elif</span> label <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'1'</span>:</span>
<span id="cb23-7">        <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"The area shown in the image is flooded with probability </span><span class="sc" style="color: #5E5E5E;">{</span>probabilities[<span class="dv" style="color: #AD0000;">1</span>]<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">100</span><span class="sc" style="color: #5E5E5E;">:.2f}</span><span class="ss" style="color: #20794D;">%."</span>)</span>
<span id="cb23-8">    <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb23-9">        <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Unknown label assigned to image."</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:19:00.622862Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:19:00.622520Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:19:00.688753Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:19:00.687512Z&quot;}" data-papermill="{&quot;duration&quot;:0.096393,&quot;end_time&quot;:&quot;2022-09-11T07:19:00.690854&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:19:00.594461&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="17">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">infer_image(input_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'floodclassifiertestset'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'1'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'1.jpeg'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/5_detecting_floods_for_disaster_relief_files/figure-html/cell-18-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-stdout">
<pre><code>The area shown in the image is not flooded with probability 65.65%.</code></pre>
</div>
</div>
<p>Not bad!</p>
<p>Let’s try it on another image.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:19:00.808329Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:19:00.807968Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:19:02.432693Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:19:02.431630Z&quot;}" data-papermill="{&quot;duration&quot;:1.658645,&quot;end_time&quot;:&quot;2022-09-11T07:19:02.435988&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:19:00.777343&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="18">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">infer_image(input_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'floodclassifiertestset'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'1'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'2.jpg'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/5_detecting_floods_for_disaster_relief_files/figure-html/cell-19-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-stdout">
<pre><code>The area shown in the image is flooded with probability 99.90%.</code></pre>
</div>
</div>
<p>The label for this image is kind of meaningless. This is an image of a vast area of land, so certain areas could be flooded, while others are not. That said, it could be used to determine whether there is flooding in the image.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:19:03.158164Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:19:03.157795Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:19:03.326060Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:19:03.324479Z&quot;}" data-papermill="{&quot;duration&quot;:0.313347,&quot;end_time&quot;:&quot;2022-09-11T07:19:03.329205&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:19:03.015858&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="19">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">infer_image(input_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'floodclassifiertestset'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'1'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'3.jpg'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/5_detecting_floods_for_disaster_relief_files/figure-html/cell-20-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-stdout">
<pre><code>The area shown in the image is flooded with probability 99.99%.</code></pre>
</div>
</div>
<p>The model performed really well in this case: the input image is shown at a different angle. The images in the training set only show areas from a top-down view.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:19:03.915708Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:19:03.915118Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:19:04.297396Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:19:04.296252Z&quot;}" data-papermill="{&quot;duration&quot;:0.532781,&quot;end_time&quot;:&quot;2022-09-11T07:19:04.300294&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:19:03.767513&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="20">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">infer_image(input_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'floodclassifiertestset'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'1'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'4.jpg'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/5_detecting_floods_for_disaster_relief_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-stdout">
<pre><code>The area shown in the image is not flooded with probability 64.56%.</code></pre>
</div>
</div>
<p>Over here, the limitations of the current state of the model can be seen. The model is not performing well on images where the view is more parallel to the ground, since the images in the training set are all top-down.</p>
<p>Let’s do two more images.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:19:05.417373Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:19:05.416795Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:19:05.473225Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:19:05.471793Z&quot;}" data-papermill="{&quot;duration&quot;:0.252769,&quot;end_time&quot;:&quot;2022-09-11T07:19:05.475284&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:19:05.222515&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="21">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">infer_image(input_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'floodclassifiertestset'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'1'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'5.jpg'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/5_detecting_floods_for_disaster_relief_files/figure-html/cell-22-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-stdout">
<pre><code>The area shown in the image is flooded with probability 99.94%.</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:19:05.835986Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:19:05.835396Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:19:06.151206Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:19:06.150029Z&quot;}" data-papermill="{&quot;duration&quot;:0.504279,&quot;end_time&quot;:&quot;2022-09-11T07:19:06.154063&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:19:05.649784&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="22">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">infer_image(input_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'floodclassifiertestset'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'1'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'6.jpg'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/5_detecting_floods_for_disaster_relief_files/figure-html/cell-23-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-stdout">
<pre><code>The area shown in the image is flooded with probability 100.00%.</code></pre>
</div>
</div>
<p>The model is working well with images of different sizes too, and has given this image a very high, correct confidence.</p>
</section>
<section id="improving-the-model." class="level2">
<h2 class="anchored" data-anchor-id="improving-the-model.">Improving the model.</h2>
<p>Let’s see if we can get the model’s performance to improve on the following image through augmenting the training set.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:19:07.793911Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:19:07.793281Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:19:08.090600Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:19:08.089689Z&quot;}" data-papermill="{&quot;duration&quot;:0.526936,&quot;end_time&quot;:&quot;2022-09-11T07:19:08.109805&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:19:07.582869&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="23">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">Image.<span class="bu" style="color: null;">open</span>(input_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'floodclassifiertestset'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'1'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'4.jpg'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<p><img src="https://forbo7.github.io/forblog/posts/5_detecting_floods_for_disaster_relief_files/figure-html/cell-24-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:19:09.399017Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:19:09.398632Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:19:12.073251Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:19:12.072257Z&quot;}" data-papermill="{&quot;duration&quot;:3.737701,&quot;end_time&quot;:&quot;2022-09-11T07:19:12.075717&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:19:08.338016&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="24">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1">augmented_dataloaders <span class="op" style="color: #5E5E5E;">=</span> DataBlock(</span>
<span id="cb37-2">    blocks <span class="op" style="color: #5E5E5E;">=</span> (ImageBlock, CategoryBlock),</span>
<span id="cb37-3">    get_items <span class="op" style="color: #5E5E5E;">=</span> get_image_files,</span>
<span id="cb37-4">    splitter <span class="op" style="color: #5E5E5E;">=</span> GrandparentSplitter(),</span>
<span id="cb37-5">    get_y <span class="op" style="color: #5E5E5E;">=</span> parent_label,</span>
<span id="cb37-6">    item_tfms <span class="op" style="color: #5E5E5E;">=</span> RandomResizedCrop(<span class="dv" style="color: #AD0000;">192</span>, min_scale<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>),</span>
<span id="cb37-7">    batch_tfms<span class="op" style="color: #5E5E5E;">=</span>aug_transforms()</span>
<span id="cb37-8">).dataloaders(working_path, bs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">32</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:19:12.539701Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:19:12.539118Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:19:13.438651Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:19:13.437466Z&quot;}" data-papermill="{&quot;duration&quot;:1.134797,&quot;end_time&quot;:&quot;2022-09-11T07:19:13.443352&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:19:12.308555&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="25">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1">augmented_dataloaders.show_batch(max_n<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">8</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/5_detecting_floods_for_disaster_relief_files/figure-html/cell-26-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:19:14.119021Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:19:14.118453Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:19:42.786631Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:19:42.785492Z&quot;}" data-papermill="{&quot;duration&quot;:28.901784,&quot;end_time&quot;:&quot;2022-09-11T07:19:42.788960&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:19:13.887176&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="26">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1">augmented_learner <span class="op" style="color: #5E5E5E;">=</span> vision_learner(augmented_dataloaders, resnet18, metrics<span class="op" style="color: #5E5E5E;">=</span>error_rate)</span>
<span id="cb39-2">augmented_learner.fine_tune(<span class="dv" style="color: #AD0000;">9</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.161182</td>
      <td>0.835870</td>
      <td>0.365385</td>
      <td>00:02</td>
    </tr>
  </tbody>
</table>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.442552</td>
      <td>0.686252</td>
      <td>0.288462</td>
      <td>00:03</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.417739</td>
      <td>0.411907</td>
      <td>0.153846</td>
      <td>00:02</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.346400</td>
      <td>0.316388</td>
      <td>0.057692</td>
      <td>00:03</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.306782</td>
      <td>0.213407</td>
      <td>0.076923</td>
      <td>00:02</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.251947</td>
      <td>0.199586</td>
      <td>0.076923</td>
      <td>00:02</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.209951</td>
      <td>0.141818</td>
      <td>0.057692</td>
      <td>00:02</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.188433</td>
      <td>0.116713</td>
      <td>0.057692</td>
      <td>00:03</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.169689</td>
      <td>0.125078</td>
      <td>0.057692</td>
      <td>00:02</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.151843</td>
      <td>0.131188</td>
      <td>0.057692</td>
      <td>00:02</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<p>Let’s try the new model out.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:19:43.735511Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:19:43.733929Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:19:44.110452Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:19:44.109223Z&quot;}" data-papermill="{&quot;duration&quot;:0.619631,&quot;end_time&quot;:&quot;2022-09-11T07:19:44.113360&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:19:43.493729&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="27">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">display(Image.<span class="bu" style="color: null;">open</span>(input_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'floodclassifiertestset'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'1'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'4.jpg'</span>))</span>
<span id="cb40-2">label, _, probabilities <span class="op" style="color: #5E5E5E;">=</span> augmented_learner.predict(PILImage(PILImage.create(input_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'floodclassifiertestset'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'1'</span><span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'4.jpg'</span>)))</span>
<span id="cb40-3"><span class="cf" style="color: #003B4F;">if</span> label <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'0'</span>:</span>
<span id="cb40-4">    <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"The area shown in the image is not flooded with probability </span><span class="sc" style="color: #5E5E5E;">{</span>probabilities[<span class="dv" style="color: #AD0000;">0</span>]<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">100</span><span class="sc" style="color: #5E5E5E;">:.2f}</span><span class="ss" style="color: #20794D;">%."</span>)</span>
<span id="cb40-5"><span class="cf" style="color: #003B4F;">elif</span> label <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'1'</span>:</span>
<span id="cb40-6">    <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"The area shown in the image is flooded with probability </span><span class="sc" style="color: #5E5E5E;">{</span>probabilities[<span class="dv" style="color: #AD0000;">1</span>]<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">100</span><span class="sc" style="color: #5E5E5E;">:.2f}</span><span class="ss" style="color: #20794D;">%."</span>)</span>
<span id="cb40-7"><span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb40-8">    <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Unknown label assigned to image."</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/5_detecting_floods_for_disaster_relief_files/figure-html/cell-28-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-stdout">
<pre><code>The area shown in the image is flooded with probability 99.91%.</code></pre>
</div>
</div>
<p>Dang, impressive! The correct label <em>and</em> with excellent confidence!</p>
<p>Before we get too excited though, we should check the performance on the model with the previous images.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:19:45.714022Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:19:45.713440Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:19:45.727793Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:19:45.726862Z&quot;}" data-papermill="{&quot;duration&quot;:0.283643,&quot;end_time&quot;:&quot;2022-09-11T07:19:45.729947&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:19:45.446304&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="28">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">test_dataloader <span class="op" style="color: #5E5E5E;">=</span> learner.dls.test_dl([image_path <span class="cf" style="color: #003B4F;">for</span> image_path <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">sorted</span>((input_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'floodclassifiertestset'</span>).rglob(<span class="st" style="color: #20794D;">'*.*'</span>))])</span></code></pre></div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:19:46.267204Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:19:46.266630Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:19:47.016009Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:19:47.014674Z&quot;}" data-papermill="{&quot;duration&quot;:1.030615,&quot;end_time&quot;:&quot;2022-09-11T07:19:47.018653&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:19:45.988038&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="29">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">probabilities, _, labels <span class="op" style="color: #5E5E5E;">=</span> augmented_learner.get_preds(dl<span class="op" style="color: #5E5E5E;">=</span>test_dataloader, with_decoded<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-09-11T07:19:47.597219Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-09-11T07:19:47.594938Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-09-11T07:19:48.299583Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-09-11T07:19:48.298350Z&quot;}" data-papermill="{&quot;duration&quot;:1.000961,&quot;end_time&quot;:&quot;2022-09-11T07:19:48.303699&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-09-11T07:19:47.302738&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="30">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Images are numbered horizontally."</span>)</span>
<span id="cb44-2">test_dataloader.show_batch()</span>
<span id="cb44-3"><span class="cf" style="color: #003B4F;">for</span> probability, label, image_number <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">zip</span>(probabilities, labels, <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">7</span>)):</span>
<span id="cb44-4">    <span class="cf" style="color: #003B4F;">if</span> label <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">1</span>:</span>
<span id="cb44-5">        <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Image </span><span class="sc" style="color: #5E5E5E;">{</span>image_number<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;"> is flooded with a probability of </span><span class="sc" style="color: #5E5E5E;">{</span>probability[<span class="dv" style="color: #AD0000;">1</span>]<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">100</span><span class="sc" style="color: #5E5E5E;">:.2f}</span><span class="ss" style="color: #20794D;">%."</span>)</span>
<span id="cb44-6">    <span class="cf" style="color: #003B4F;">elif</span> label <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb44-7">        <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Image </span><span class="sc" style="color: #5E5E5E;">{</span>image_number<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;"> is not flooded with a probability of </span><span class="sc" style="color: #5E5E5E;">{</span>probability[<span class="dv" style="color: #AD0000;">0</span>]<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">100</span><span class="sc" style="color: #5E5E5E;">:.2f}</span><span class="ss" style="color: #20794D;">%."</span>)</span>
<span id="cb44-8">    <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb44-9">        <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Image </span><span class="sc" style="color: #5E5E5E;">{</span>image_number<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;"> has been assigned an unknown label."</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Images are numbered horizontally.
Image 1 is flooded with a probability of 95.94%.
Image 2 is flooded with a probability of 99.92%.
Image 3 is flooded with a probability of 91.34%.
Image 4 is flooded with a probability of 99.71%.
Image 5 is flooded with a probability of 100.00%.
Image 6 is flooded with a probability of 100.00%.</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/5_detecting_floods_for_disaster_relief_files/figure-html/cell-31-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Drastically improved probabilities! A little augmentation can go a long way.</p>
</section>
<section id="takeaways" class="level2">
<h2 class="anchored" data-anchor-id="takeaways">Takeaways</h2>
<p>This model was trained on only 270 images and minimal code. Accessbility and abstraction to the field of machine learning has come a long, long way. Given the right data and the right pretrained model, a powerful model can be produced in less than an hour, if not half.</p>
<p>This is important: in disasters such as floods, the time taken to produce the logistics required for relief can be drastically reduced. It is also important because the barrier of entry to this field is dramatically lowered; more people can create powerful models, in turn producing better solutions.</p>
<p>However, there could be some improvements and additions made to the model:</p>
<ul>
<li><p>Include a third class to the model. Images that are not flooded, but show signs of having been flooded would be assigned this class. The dataset used for this model includes such images.</p></li>
<li><p>Train the model on images that include a variety of geographic locations and dwellings. The current dataset only contains images taken in a lush, green area with plenty of trees; infrastructure looks a certain way; the color of the floodwater is also dependent on the surroundings. All this makes the model good a prediciting whether an image is flooded for images with certain features.</p></li>
</ul>
<p>If you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!</p>


</section>

 ]]></description>
  <category>Creating Models</category>
  <guid>https://forbo7.github.io/forblog/posts/5_detecting_floods_for_disaster_relief.html</guid>
  <pubDate>Mon, 12 Sep 2022 00:00:00 GMT</pubDate>
  <media:content url="https://forbo7.github.io/forblog/images/5_detecting_floods_for_disaster_relief/thumbnail.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Data Quality is Important | Car Classifier</title>
  <dc:creator>Salman Naqvi</dc:creator>
  <link>https://forbo7.github.io/forblog/posts/4_data_quality_is_important.html</link>
  <description><![CDATA[ 



<p><em>This article was updated on <strong>Thursday, 10 November 2022</strong>.</em></p>
<p><img src="https://forbo7.github.io/forblog/images/4_data_quality_is_important/thumbnail.jpg" class="img-fluid" alt="A parking lot filled with cars."></p>
<p>I recently created a car classifier that classified cars into their respective brands.</p>
<p>Despite having almost 5000 images in my training set, I ended up trying out over a hundred layers in my model, and twenty epochs. Even then, I had an error rate of 17.4%.</p>
<p>The culprit? My dataset.</p>
<p>I scraped 5000 images of cars (500 for each company) from DuckDuckGo. Naturally, as expected, the data quality is not so good.</p>
<p>Why? Below are some potential reasons:</p>
<ul>
<li>Noncar images present in dataset</li>
<li>Cars of incorrect company present in dataset</li>
<li>F1 cars present in dataset</li>
<li>A large variety of cars from different time periods present in dataset</li>
<li>Different companys’ cars look similar</li>
<li>Modded cars present in dataset</li>
<li>Concept cars present in dataset</li>
<li>Multiple cars present in a single image</li>
<li>Certain angles of cars appear more than others</li>
<li>Cars appear in certain backgrounds more than others</li>
<li>The search term <code>{car_brand} car</code> could be skewing results</li>
</ul>
<p>I could have absolutely achieved better results with fewer layers and fewer epochs if I trained the model on better quality data — or manually combed through the 5000 images 💀. However, I did use fastai’s GUI for data cleaning. This GUI sorts images by their loss which helps to determine if certain images should be relabeled or deleted.</p>
<p>Below is the confusion matrix for this model.</p>
<p><img src="https://forbo7.github.io/forblog/images/4_data_quality_is_important/confusion_matrix.png" class="img-fluid" style="width:65.0%" alt="A confusion matrix of the model."></p>
<p>It can be seen that this model “confuses” between quite a few different brands: Ford and Chevrolet, Chevrolet and Ford, Jaguar and Aston Martin, Renault and Ford.</p>
<p>But <strong>why</strong> is data quality important? Because without good data, the model will not be able to “see” things the way they actually are, and in turn end up making worse predictions and not generalize to other data.</p>
<p>Let’s say you did not know how, say, a toaster looked like. So I taught you by showing you pictures of a kettle. Then to test you, I showed you a set of pictures depicting various kitchen appliances and told you to find the toaster. You would not be able to.</p>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="https://forbo7.github.io/forblog/images/4_data_quality_is_important/kettle_1.jpg" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="https://forbo7.github.io/forblog/images/4_data_quality_is_important/kettle_2.jpg" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="https://forbo7.github.io/forblog/images/4_data_quality_is_important/toaster_1.jpg" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="https://forbo7.github.io/forblog/images/4_data_quality_is_important/toaster_2.jpg" class="img-fluid"></p>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="https://forbo7.github.io/forblog/images/4_data_quality_is_important/kettle_3.jpg" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="https://forbo7.github.io/forblog/images/4_data_quality_is_important/kettle_4.jpg" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="https://forbo7.github.io/forblog/images/4_data_quality_is_important/toaster_3.jpg" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="https://forbo7.github.io/forblog/images/4_data_quality_is_important/toaster_4.jpg" class="img-fluid"></p>
</div>
</div>
</div>
<p>Extending upon this example, say I showed you toasters only from the last two years and from two brands only. You would not be able to identify toasters older than two years, and toasters from other brands to much success.</p>
<p>Obviously, humans are smarter and can infer. AI methods can only infer to a certain degree, mainly based on what is in their dataset. This talk does start to become more philosophical.</p>
<p>The point of this post is to emphasize the importance of data quality and different aspects to consider as to why data quality may not be good. You can have the best architecture in the world, but it is useless if you do not have good data.</p>
<p>If you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!</p>



 ]]></description>
  <category>Data</category>
  <category>Analyzing Models</category>
  <guid>https://forbo7.github.io/forblog/posts/4_data_quality_is_important.html</guid>
  <pubDate>Sat, 04 Jun 2022 00:00:00 GMT</pubDate>
  <media:content url="https://forbo7.github.io/forblog/images/4_data_quality_is_important/thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>A No Nonsense Guide to Reading a Confusion Matrix</title>
  <dc:creator>Salman Naqvi</dc:creator>
  <link>https://forbo7.github.io/forblog/posts/3_the_confusion_matrix.html</link>
  <description><![CDATA[ 



<p><em>This article was updated on <strong>Thursday, 10 November 2022</strong>.</em></p>
<p><img src="https://forbo7.github.io/forblog/images/3_the_confusion_matrix/thumbnail.jpg" class="img-fluid" alt="A meme from the Matrix film."></p>
<p>Confusion matrices help model designers view what mistakes a model has made.</p>
<p>In this post, I’ll be telling you how to easily read such matrices.</p>
<p>Jump to Section&nbsp;2 for an ultra concise rundown.</p>
<p>Ready? Here we go.</p>
<section id="case-1-introduction" class="level2">
<h2 class="anchored" data-anchor-id="case-1-introduction">Case 1: Introduction</h2>
<p><img src="https://forbo7.github.io/forblog/images/3_the_confusion_matrix/confusion_matrix_1.png" class="img-fluid" style="width:65.0%"></p>
<p>Ignore the “Actual” and “Predicted” labels for now.</p>
<p>Let’s compare grizzly bears to black bears.</p>
<p>All comparisons begin at the bottom, with the columns.</p>
<p>First, highlight the grizzly bear column.</p>
<p><img src="https://forbo7.github.io/forblog/images/3_the_confusion_matrix/confusion_matrix_1_1.png" class="img-fluid" style="width:65.0%"></p>
<p>Next, highlight the black bear row.</p>
<p><img src="https://forbo7.github.io/forblog/images/3_the_confusion_matrix/confusion_matrix_1_2.png" class="img-fluid" style="width:65.0%"></p>
<p>Now find the common entry in the highlighted column and row.</p>
<p><img src="https://forbo7.github.io/forblog/images/3_the_confusion_matrix/confusion_matrix_1_3.png" class="img-fluid" style="width:65.0%"></p>
<p>This common entry is our required information.</p>
<p>All entries in the diagonal going from the top left to the bottom right (blue) are correct classifications. All other entries are incorrect classifications.</p>
<p>Our common entry does not lie in the main diagonal. Therefore, we are looking at incorrect classifications.</p>
<p>We have compared grizzly bears to black bears. Therefore, from this deduction, <strong>three grizzly bears have been incorrectly classified as black bears</strong>.</p>
<div class="callout-note callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>There is a difference between comparing grizzly bears to black bears and black bears to grizzly bears.</strong></p>
<p>Comparing grizzly bears to black bears means, “How many grizzly bears were misclassified as black bears?”</p>
<p>Comparing black bears to grizzly bears means, “How many black bears were misclassified as grizzly bears?”</p>
</div>
</div>
</section>
<section id="sec-case2" class="level2">
<h2 class="anchored" data-anchor-id="sec-case2">Case 2: Ultra Concise</h2>
<p>Let’s compare black bears to grizzly bears.</p>
<p>Highlight the black bear column.</p>
<p><img src="https://forbo7.github.io/forblog/images/3_the_confusion_matrix/confusion_matrix_1_4.png" class="img-fluid" style="width:65.0%"></p>
<p>Highlight the grizzly bear row.</p>
<p><img src="https://forbo7.github.io/forblog/images/3_the_confusion_matrix/confusion_matrix_1_5.png" class="img-fluid" style="width:65.0%"></p>
<p>Highlight the common entry.</p>
<p><img src="https://forbo7.github.io/forblog/images/3_the_confusion_matrix/confusion_matrix_1_6.png" class="img-fluid" style="width:65.0%"></p>
<p>Zero black bears were misclassified as grizzly bears.</p>
</section>
<section id="case-3-correct-classifications" class="level2">
<h2 class="anchored" data-anchor-id="case-3-correct-classifications">Case 3: Correct Classifications</h2>
<p>Let’s see how many teddy bears were correctly classified. We are essentially comparing teddy bears to teddy bears.</p>
<p>Highlight the teddy bear column.</p>
<p><img src="https://forbo7.github.io/forblog/images/3_the_confusion_matrix/confusion_matrix_1_7.png" class="img-fluid" style="width:65.0%"></p>
<p>Highlight the teddy bear row.</p>
<p><img src="https://forbo7.github.io/forblog/images/3_the_confusion_matrix/confusion_matrix_1_8.png" class="img-fluid" style="width:65.0%"></p>
<p>Highlight the common entry.</p>
<p><img src="https://forbo7.github.io/forblog/images/3_the_confusion_matrix/confusion_matrix_1_9.png" class="img-fluid" style="width:65.0%"></p>
<p>Fifty three teddy bears were correctly classified as teddy bears.</p>
</section>
<section id="exercise-do-it-yourself" class="level2">
<h2 class="anchored" data-anchor-id="exercise-do-it-yourself">Exercise: Do It Yourself</h2>
<p>Below is a confusion matrix of a car classifier that classifies cars into their brand.</p>
<p><img src="https://forbo7.github.io/forblog/images/3_the_confusion_matrix/confusion_matrix_2.png" class="img-fluid" style="width:65.0%"></p>
<p>You learn by doing!</p>
<ul>
<li>How many Lamborghinis were correctly classified?</li>
<li>How many Jaguars were incorrectly classified?</li>
<li>How many Chevrolets were misclassified as Fords?</li>
<li>How many Fords were misclassified as Chevrolets?</li>
<li>Which two car brands did the model have the most trouble differentiating between?</li>
</ul>
<p>If you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!</p>


</section>

 ]]></description>
  <category>Analyzing Models</category>
  <guid>https://forbo7.github.io/forblog/posts/3_the_confusion_matrix.html</guid>
  <pubDate>Fri, 03 Jun 2022 00:00:00 GMT</pubDate>
  <media:content url="https://forbo7.github.io/forblog/images/3_the_confusion_matrix/thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>My first AI model</title>
  <dc:creator>Salman Naqvi</dc:creator>
  <link>https://forbo7.github.io/forblog/posts/2_bear_classifier_model.html</link>
  <description><![CDATA[ 



<p><em>This article was updated on <strong>Tuesday, 1 November 2022</strong>.</em></p>
<p><img src="https://forbo7.github.io/forblog/images/2_bear_classifier_model/thumbnail.jpg" class="img-fluid" alt="A bear waving hello."></p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>This is my first attempt at creating an AI model: an image classifier. This classifier can tell whether a grizzly bear, black bear, or teddy bear is in an image.</p>
<p>You can visit the classifier <a href="https://forbo7.github.io/web_apps/apps/bear_detector.html">here</a> to test it out for yourself!</p>
</section>
<section id="load-libraries" class="level2">
<h2 class="anchored" data-anchor-id="load-libraries">Load libraries</h2>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;"># No need to fret! fastai is specifically designed to be used with import *.</span></span>
<span id="cb1-2"><span class="im" style="color: #00769E;">from</span> fastbook <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> fastai.vision.<span class="bu" style="color: null;">all</span> <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span></code></pre></div>
</div>
</section>
<section id="download-image-files" class="level2">
<h2 class="anchored" data-anchor-id="download-image-files">Download image files</h2>
<p>Specify the bear images we wish to download.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">bear_types <span class="op" style="color: #5E5E5E;">=</span> (<span class="st" style="color: #20794D;">'grizzly'</span>, <span class="st" style="color: #20794D;">'black'</span>, <span class="st" style="color: #20794D;">'teddy'</span>,)</span>
<span id="cb2-2">path <span class="op" style="color: #5E5E5E;">=</span> Path(<span class="st" style="color: #20794D;">'bears'</span>)</span></code></pre></div>
</div>
<p>Download 200 of each bear (<code>search_images_ddg</code> defaults to 200 URLs) and assign them to a specific directory.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> path.exists():</span>
<span id="cb3-2">    path.mkdir()</span>
<span id="cb3-3">    <span class="cf" style="color: #003B4F;">for</span> bear_type <span class="kw" style="color: #003B4F;">in</span> bear_types:</span>
<span id="cb3-4">        destination <span class="op" style="color: #5E5E5E;">=</span> (path <span class="op" style="color: #5E5E5E;">/</span> bear_type)</span>
<span id="cb3-5">        destination.mkdir(exist_ok<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb3-6">        urls <span class="op" style="color: #5E5E5E;">=</span> search_images_ddg(<span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span>bear_type<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;"> bear"</span>)</span>
<span id="cb3-7">        download_iamges(destination, urls<span class="op" style="color: #5E5E5E;">=</span>urls)</span></code></pre></div>
</div>
<p>Check if our folder has the image files.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">fns <span class="op" style="color: #5E5E5E;">=</span> get_image_files(path)</span>
<span id="cb4-2">fns</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>(#802) [Path('bears/grizzly/00000238.jpg'),Path('bears/grizzly/00000047.jpg'),Path('bears/grizzly/00000199.jpg'),Path('bears/grizzly/00000237.jpg'),Path('bears/grizzly/00000055.jpg'),Path('bears/grizzly/00000000.png'),Path('bears/grizzly/00000235.jpg'),Path('bears/grizzly/00000159.jpg'),Path('bears/grizzly/00000268.jpg'),Path('bears/grizzly/00000266.jpg')...]</code></pre>
</div>
</div>
<p>Check for corrupt images.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">corrupt_images <span class="op" style="color: #5E5E5E;">=</span> verify_images(fns)</span>
<span id="cb6-2">corrupt_images</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>(#0) []</code></pre>
</div>
</div>
<p>Remove corrupt images.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">corrupt_images.<span class="bu" style="color: null;">map</span>(pathlib.Path.unlink)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>(#0) []</code></pre>
</div>
</div>
</section>
<section id="load-image-files" class="level2">
<h2 class="anchored" data-anchor-id="load-image-files">Load image files</h2>
<p>The DataBlock API for creates the necessary <code>DataLoaders</code> for us.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">bears <span class="op" style="color: #5E5E5E;">=</span> DataBlock(</span>
<span id="cb10-2">    blocks<span class="op" style="color: #5E5E5E;">=</span>(ImageBlock, CategoryBlock),</span>
<span id="cb10-3">    get_items<span class="op" style="color: #5E5E5E;">=</span>get_image_files,</span>
<span id="cb10-4">    splitter<span class="op" style="color: #5E5E5E;">=</span>RandomSplitter(valid_pct<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.2</span>, seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">42</span>),</span>
<span id="cb10-5">    get_y<span class="op" style="color: #5E5E5E;">=</span>parent_label,</span>
<span id="cb10-6">    item_tfms<span class="op" style="color: #5E5E5E;">=</span>Resize(<span class="dv" style="color: #AD0000;">128</span>),</span>
<span id="cb10-7">)</span></code></pre></div>
</div>
<p>The <code>blocks</code> parameter allows us to specify the independent and dependent variables.</p>
<p>The <code>get_items</code> parameter tells fastai how to obtain our data. We use the <code>get_image_files</code> function to obtain our images.</p>
<p>The <code>splitter</code> parameter allows us to tell fastai how to split our data into training and validation sets. Since our data is one big set, we use the <code>RandomSplitter</code> class and tell it to use 20% of our data as the validation set. We specify a seed so the same split occurs each time.</p>
<p>The <code>get_y</code> parameter obtains our labels. The <code>parent_label</code> function simply gets the name of the folder a file is in. Since we have organized our bear images into different folders, this will nicely handle our target labels.</p>
<p>The <code>item_tfms</code> parameter allows us to specify a transform to apply to our data. Since we want all our images to be of the same size, we use the <code>Resize()</code> class.</p>
<p>We now have a DataBlock object from which can load the data.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">dataloaders <span class="op" style="color: #5E5E5E;">=</span> bears.dataloaders(path)</span></code></pre></div>
</div>
<p>Let us view a few images in the validation set.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">dataloaders.valid.show_batch(max_n<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>, nrows<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/2_bear_classifier_model_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="data-augmentation" class="level2">
<h2 class="anchored" data-anchor-id="data-augmentation">Data Augmentation</h2>
<p>Data augmentation refers to creating random variations to our input data. This produces new data points based on the existing data points. This allows each data point to look different, without changing their meaning.</p>
<p>Typical examples of image augmentation include rotation, flipping, perspective warping, brightness changing, and contrast changing.</p>
<section id="cropping" class="level3">
<h3 class="anchored" data-anchor-id="cropping">Cropping</h3>
<p>The validation set images shown above are cropped. We achieved this by specifying the <code>Resize</code> argument when defining the <code>DataBlock</code>. <code>Resize</code> crops images to the size specified.</p>
<p>Cropping results in detail being lost.</p>
<p>Alternatively, we can squish or stretch images, or pad them to a desired size.</p>
</section>
<section id="squishingstretching" class="level3">
<h3 class="anchored" data-anchor-id="squishingstretching">Squishing/Stretching</h3>
<p>The problem with squishing or stretching images is that the model will learn to “see” images the way they are not supposed to be.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">bears <span class="op" style="color: #5E5E5E;">=</span> bears.new(item_tfms<span class="op" style="color: #5E5E5E;">=</span>Resize(<span class="dv" style="color: #AD0000;">128</span>, ResizeMethod.Squish))</span>
<span id="cb13-2">dataloaders <span class="op" style="color: #5E5E5E;">=</span> bears.dataloaders(path)</span>
<span id="cb13-3">dataloaders.valid.show_batch(max_n<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>, nrows<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/2_bear_classifier_model_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="padding" class="level3">
<h3 class="anchored" data-anchor-id="padding">Padding</h3>
<p>By padding, the image is surrounded typically by black, meaningless pixels. This results in extra, wasted computation.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">bears <span class="op" style="color: #5E5E5E;">=</span> bears.new(item_tfms<span class="op" style="color: #5E5E5E;">=</span>Resize(<span class="dv" style="color: #AD0000;">128</span>, ResizeMethod.Pad, pad_mode<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'zeros'</span>))</span>
<span id="cb14-2">dataloaders <span class="op" style="color: #5E5E5E;">=</span> bears.dataloaders(path)</span>
<span id="cb14-3">dataloaders.valid.show_batch(max_n<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>, nrows<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/2_bear_classifier_model_files/figure-html/cell-12-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The best approach is to take random crops of different parts of the same image. This makes sure that the model does not miss out on any details whilst letting it “know” how an object fully looks like.</p>
<p>Below, we have <code>unique=True</code> so that the same image is repeated with different variations.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">bears <span class="op" style="color: #5E5E5E;">=</span> bears.new(item_tfms<span class="op" style="color: #5E5E5E;">=</span>RandomResizedCrop(<span class="dv" style="color: #AD0000;">128</span>, min_scale<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.3</span>))</span>
<span id="cb15-2">dataloaders <span class="op" style="color: #5E5E5E;">=</span> bears.dataloaders(path)</span>
<span id="cb15-3">dataloaders.train.show_batch(max_n<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>, nrows<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, unique<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/2_bear_classifier_model_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>fastai comes with a function that applies a variety of augmentations to images. This can allow a model to “see” and recognize images in a variety of scenarios.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">bears <span class="op" style="color: #5E5E5E;">=</span> bears.new(item_tfms<span class="op" style="color: #5E5E5E;">=</span>Resize(<span class="dv" style="color: #AD0000;">128</span>), batch_tfms<span class="op" style="color: #5E5E5E;">=</span>aug_transforms(mult<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>))</span>
<span id="cb16-2">dataloaders  <span class="op" style="color: #5E5E5E;">=</span> bears.dataloaders(path)</span>
<span id="cb16-3">dataloaders.train.show_batch(max_n<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">8</span>, nrows<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, unique<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/2_bear_classifier_model_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>I have not used <code>RandomResizedCrop</code> here so that the different augmentations can be seen more clearly. <code>RandomResizedCrop</code> will be used when the model is trained.</p>
<p><code>batch_tfms</code> tells fastai that we want to use these transforms on a batch.</p>
</section>
</section>
<section id="training-the-model" class="level2">
<h2 class="anchored" data-anchor-id="training-the-model">Training the model</h2>
<p>We do not have a lot of data. Only 200 images of each bear at most. Therefore, we will augment our images not only to get more data, but so that the model can recognize data in a variety of situations.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">bears <span class="op" style="color: #5E5E5E;">=</span> bears.new(</span>
<span id="cb17-2">    item_tfms<span class="op" style="color: #5E5E5E;">=</span>RandomResizedCrop(<span class="dv" style="color: #AD0000;">224</span>, min_scale<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>),</span>
<span id="cb17-3">    batch_tfms<span class="op" style="color: #5E5E5E;">=</span>aug_transforms(),</span>
<span id="cb17-4">)</span>
<span id="cb17-5">dataloaders <span class="op" style="color: #5E5E5E;">=</span> bears.dataloaders(path)</span></code></pre></div>
</div>
<p>We will now create our learner and fine-tune it.</p>
<p>We will be using the ResNet18 architecture (which is a convolutional neural network, or CNN for short). Error rate will be the metric.</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">learn <span class="op" style="color: #5E5E5E;">=</span> cnn_learner(dataloaders, resnet18, metrics<span class="op" style="color: #5E5E5E;">=</span>error_rate)</span>
<span id="cb18-2">learn.fine_tune(<span class="dv" style="color: #AD0000;">4</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"5e2ebba3dfc945c9be0edd07e1196ebe","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.985666</td>
      <td>0.104632</td>
      <td>0.025000</td>
      <td>00:20</td>
    </tr>
  </tbody>
</table>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.132230</td>
      <td>0.073527</td>
      <td>0.012500</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.106222</td>
      <td>0.054833</td>
      <td>0.018750</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.087129</td>
      <td>0.058497</td>
      <td>0.012500</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.069890</td>
      <td>0.058845</td>
      <td>0.018750</td>
      <td>00:19</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<p>Our model only has a 1.9% error rate! Not bad! Though it seems if I had done an extra epoch, the error rate may have gone down to 1.3%, judging by the previous epochs’ error rates.</p>
</section>
<section id="visualizing-mistakes" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-mistakes">Visualizing mistakes</h2>
<p>We can visualize the mistakes the model is making by a confusion matrix.</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">interp <span class="op" style="color: #5E5E5E;">=</span> ClassificationInterpretation.from_learner(learn)</span>
<span id="cb20-2">interp.plot_confusion_matrix()</span></code></pre></div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/2_bear_classifier_model_files/figure-html/cell-17-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>3 grizzly bears were misclassified as black bears.</p>
<p>Let us see where the errors are occurring, so we can determine if they are due to a dataset problem or a model problem.</p>
<p>To do this, we will sort images by their loss.</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">interp.plot_top_losses(<span class="dv" style="color: #AD0000;">5</span>, nrows<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://forbo7.github.io/forblog/posts/2_bear_classifier_model_files/figure-html/cell-18-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="data-cleaning" class="level2">
<h2 class="anchored" data-anchor-id="data-cleaning">Data cleaning</h2>
<p>The intuitive approach to data cleaning is to do it before training the model. However, a trained model can help us clean the data. For example, we can see some mislabaled bears in the above cases.</p>
<p>fastai includes a GUI for data cleaning. This GUI allows you to choose a category/label and its associated training and validation sets. It then shows you images in order of highest-loss first, from which you can select images for removal or relabeling.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">cleaner <span class="op" style="color: #5E5E5E;">=</span> ImageClassifierCleaner(learn)</span>
<span id="cb22-2">cleaner</span></code></pre></div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"7d3bc1c4ea394fceadf6ccbf4febb422","version_major":2,"version_minor":0}
</script>
</div>
</div>
<p><code>ImageClassifierCleaner</code> does not actually delete or relabel. It just returns the indices that are to be deleted or relabeled.</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="co" style="color: #5E5E5E;"># Delete images selected for deletion.</span></span>
<span id="cb23-2"><span class="cf" style="color: #003B4F;">for</span> index <span class="kw" style="color: #003B4F;">in</span> cleaner.delete():</span>
<span id="cb23-3">    cleaner.fns[index].unlink()</span>
<span id="cb23-4"></span>
<span id="cb23-5"><span class="co" style="color: #5E5E5E;"># Relabel images selected for relabeling.</span></span>
<span id="cb23-6"><span class="cf" style="color: #003B4F;">for</span> index, category <span class="kw" style="color: #003B4F;">in</span> cleaner.change():</span>
<span id="cb23-7">    shutil.move(<span class="bu" style="color: null;">str</span>(cleaner.fns[index]), path<span class="op" style="color: #5E5E5E;">/</span>category)</span></code></pre></div>
</div>
<p>We can now retrain and better performance should be expected.</p>
</section>
<section id="saving-the-model" class="level2">
<h2 class="anchored" data-anchor-id="saving-the-model">Saving the model</h2>
<p>A model consists of two parts: the architecture and the parameters.</p>
<p>When we use the <code>export()</code> method, both of these are saved.</p>
<p>This method also saves the definition of our <code>DataLoaders</code>. This is done so that we do not have to redefine how to transform our data when the model is used in production.</p>
<p>fastai uses our validation set <code>DataLoader</code> by default, so the data augmentation will not be applied, which is generally what is wanted.</p>
<p>The <code>export()</code> method creates a file named “export.pkl”.</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">learn.export()</span></code></pre></div>
</div>
<p>Let us check that the file exists.</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">path <span class="op" style="color: #5E5E5E;">=</span> Path()</span>
<span id="cb25-2">path.ls(file_exts<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'.pkl'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>(#1) [Path('export.pkl')]</code></pre>
</div>
</div>
<p>If you wish to deploy an app, this is the file you will need.</p>
</section>
<section id="loading-the-model-for-inference" class="level2">
<h2 class="anchored" data-anchor-id="loading-the-model-for-inference">Loading the model for inference</h2>
<p>Now obviously we do not need to load the model as we already have the <code>learner</code> variable. But I shall do so anyways.</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">learn_inf <span class="op" style="color: #5E5E5E;">=</span> load_learner(path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'export.pkl'</span>)</span></code></pre></div>
</div>
<p>We generally do inference for a single image at a time.</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">learn_inf.predict(<span class="st" style="color: #20794D;">'images/grizzly.jpg'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>('grizzly', TensorBase(1), TensorBase([1.4230e-06, 1.0000e+00, 3.9502e-08]))</code></pre>
</div>
</div>
<p>Three things have been returned: the predicted category, the index of the predicted category, and the probabilities of each category.</p>
<p>The order of each category is based on the order of the <em>vocabulary</em> of the <code>DataLoaders</code>; that is, the stored tuple of all possible categories.</p>
<p>The <code>DataLoaders</code> can be accessed as an attribute of the <code>Learner</code>.</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">learn_inf.dataloaders.vocab</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>['black', 'grizzly', 'teddy']</code></pre>
</div>
</div>
</section>
<section id="why-cnns-work-so-well" class="level2">
<h2 class="anchored" data-anchor-id="why-cnns-work-so-well">Why CNNs work so well</h2>
<p>The ResNet18 architecture is a sort of CNN. Below is my understanding as to why CNNs work so well.</p>
<p>A neural network is comprised of many layers. Each layer is comprised of many neurons. In a CNN, each neuron in the same layer is given the exact same weights, while being given different input data. This allows all neurons in a layer to fire upon detecting the same pattern.</p>
<p>Because of this, CNNs can become really good at detecting objects in various patterns, orientations, shapes, positions, and so on.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Well then, that wraps up my first deep learning model! I have to say, it is much easier than I thought it would be to implement a model. You do not need to go into the nitty gritty details of artificial intelligence. A high level understanding can suffice in the beginning. It is like playing a sport: you do not need to understand the physics to be able to play it.</p>
<p>If you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!</p>


</section>

 ]]></description>
  <category>Creating Models</category>
  <guid>https://forbo7.github.io/forblog/posts/2_bear_classifier_model.html</guid>
  <pubDate>Sat, 28 May 2022 00:00:00 GMT</pubDate>
  <media:content url="https://forbo7.github.io/forblog/images/2_bear_classifier_model/thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>How to Approach Creating AI Models</title>
  <dc:creator>Salman Naqvi</dc:creator>
  <link>https://forbo7.github.io/forblog/posts/1_how_to_approach_creating_ai_models.html</link>
  <description><![CDATA[ 



<p><em>This article was rewritten on <strong>Monday, 31 October 2022</strong>.</em></p>
<p><img src="https://forbo7.github.io/forblog/images/1_how_to_approach_creating_ai_models/thumbnail.jpg" class="img-fluid" alt="A picture showing cogs and gears."></p>
<section id="introduction" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<div class="page-columns page-full"><p>How you approach making models is crucial. The way AI methods are used in today’s landscape is very different. AI methods are created to solve small, atomic problems. And we’ve got most of the methods to handle these small tasks hammered down. Therefore, applied AI is not about creating models; it’s only a small part of it. It’s 80% problem solving and 20% implementing (I would not be surprised if it actually followed the 80-20 rule<sup>1</sup>).</p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;<a href="https://en.wikipedia.org/wiki/Pareto_principle">The 80/20 Rule, also known as the Pareto Principle</a></p></li></div></div>
<p>Think of AI methods as a tool; think of it as a pencil. You can use pencils to draw, take notes, poke holes, and much more. There are also dozens of pencils out there. But what point is there in using any of those pencils if you don’t even know how to properly use a pencil in the first place? The art of creating pencils has already been perfected too.</p>
<p>One highly successful approach is the <em>Drivetrain Approach</em>, created by Jeremy Howard — who’s widely known for his fastai course and library —, Margit Zwemer, and Mike Loukides.</p>
<p>The goal of the <em>Drivetrain Approach</em> is to not just use data to generate more data — data that is in the form of predictions. But rather to use data to also generate actionable outcomes.</p>
<p>The official blogpost goes into much more depth <a href="https://www.oreilly.com/radar/drivetrain-approach-data-products/">here</a>.</p>
<p>In this post, I’ll be providing a short overview of my understanding of this approach by applying it to the <a href="https://www.elementsofai.com">Elements of AI</a> course’s final project (this online course was created by the University of Helsinki and Reaktor).</p>
</section>
<section id="overview-of-the-drivetrain-approach" class="level2">
<h2 class="anchored" data-anchor-id="overview-of-the-drivetrain-approach">Overview of the Drivetrain Approach</h2>
<p>There are four main steps to this approach:</p>
<ul>
<li>Define the objective</li>
<li>Consider your possible actions</li>
<li>Consider your data</li>
<li>Create the models</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://forbo7.github.io/forblog/images/1_how_to_approach_creating_ai_models/drivetrain_approach.png" class="img-fluid figure-img" alt="A diagram depicting the DrivetrainApproach as a flow chart."></p>
<p></p><figcaption class="figure-caption"><a href="https://www.oreilly.%20com/radar/drivetrain-approach-data-products/">Image Source</a></figcaption><p></p>
</figure>
</div>
<section id="define-the-objective" class="level3">
<h3 class="anchored" data-anchor-id="define-the-objective">Define the objective</h3>
<p>Write out what you are really trying to achieve. What is your goal? Writing it out puts it in a tangible manner.</p>
</section>
<section id="consider-your-actions" class="level3">
<h3 class="anchored" data-anchor-id="consider-your-actions">Consider your actions</h3>
<p>Think about what actions you can take to achieve your objective.</p>
<p>Also think about what would happen if you did those actions.</p>
<p>What would happen if I did <em>x</em>? Would <em>y</em> really be a good idea? What if <em>z</em> worked out <strong>too</strong> well? Will <em>x</em> lead to <em>y</em>? What would happen if <em>x</em> turned out poorly?</p>
</section>
<section id="consider-your-data" class="level3">
<h3 class="anchored" data-anchor-id="consider-your-data">Consider your data</h3>
<p>Think about the data you already have and how it could be used.</p>
<p>Think about any further data that is needed and how it could be collected.</p>
</section>
<section id="create-the-models" class="level3">
<h3 class="anchored" data-anchor-id="create-the-models">Create the models</h3>
<p>Create models. But create models that produce actions. Actions that produce the best results for your objective.</p>
</section>
</section>
<section id="endangered-language-chatbot" class="level2">
<h2 class="anchored" data-anchor-id="endangered-language-chatbot">Endangered Language Chatbot</h2>
<p>The final project of the Elements of AI course asked me to come up with my own AI method that would solve a problem, and how it would do so.</p>
<p>The problem I tackled was the endangerment of languages. The solution I came up with was to create a chatbot that could converse in these endangered languages. I created an overview of how this could be done.</p>
<p>The overview can be read <a href="https://github.com/ForBo7/Endangered-Language-Chatbot">here</a>.</p>
<p>Let’s tackle this problem through the <em>Drivetrain Approach</em>.</p>
<section id="define-the-objective-1" class="level3">
<h3 class="anchored" data-anchor-id="define-the-objective-1">Define the objective</h3>
<p>The objective is to preserve languages that are in danger of going extinct. Through preserving languages, histories and cultures can be preserved.</p>
</section>
<section id="consider-your-actions-1" class="level3">
<h3 class="anchored" data-anchor-id="consider-your-actions-1">Consider your actions</h3>
<p>One way this could be done is to create a chatbot that could converse in endangered languages. However, this would be a monumental task considering the amount of data needed to achieve this.</p>
<p>Another action that could be taken is to create an information retrieval (IR) system of sorts. A corpus of written text of the language could be provided, from which insights about the language’s history, culture, and way of conversing could be gained. In turn the language is preserved.</p>
<p>The latter action may be easier to achieve.</p>
</section>
<section id="consider-your-data-1" class="level3">
<h3 class="anchored" data-anchor-id="consider-your-data-1">Consider your data</h3>
<p>The obvious source of data would be a corpora of text.</p>
<p>However, a major problem arises for those languages which are only spoken. Audio recordings of conversations would have to be made which would take a lot of time and effort. This would be especially difficult for those languages where very few speakers remain.</p>
<p>Even if a language does have written text, gathering enough text for the language can also be a problem: the language may not have much written text. This may especially be the case for endangered languages. Again, one solution is to manually create texts — using an NLP method to create these texts is not viable.</p>
<p>In short, for some languages, there may be no choice other than to manually create the data that would be fed into the system — this manual creation also has the chance to skew the performance of the model.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://forbo7.github.io/forblog/images/1_how_to_approach_creating_ai_models/whistle_language.jpg" class="img-fluid figure-img" alt="An elderly man blowing a whistle."></p>
<p></p><figcaption class="figure-caption">Kuş dili, a whistled language spoken in Turkey. How would such a language be preserved? <a href="https://ich.unesco.org/en/USL/whistled-language-00658">Image Source</a></figcaption><p></p>
</figure>
</div>
</section>
<section id="create-the-model" class="level3">
<h3 class="anchored" data-anchor-id="create-the-model">Create the model</h3>
<p>Either a chatbot needs to be created that speaks as accurately as a native speaker, or an IR system needs to be created that gives meaningful, correct insights into a language and its associated culture.</p>
<p>This step may either be easy or hard, depending on the language. Most NLP or IR systems have been built on a few, select languages. Perhaps this step may be easy for those languages that are similar to languages on which NLP or IR systems have already been built on. It will most likely be harder for those languages which are not.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This concludes my understanding of the <em>Drivetrain Approach</em>, through an example.</p>
<p>Approaches are crucial: you can have state-of-the-art tools, but they are useless if not correctly applied. The approach you take can either make it or break it. Putting it into a concrete, organized, tangible manner goes a long way.</p>
<p>If you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!</p>


</section>


 ]]></description>
  <category>Approaching AI</category>
  <guid>https://forbo7.github.io/forblog/posts/1_how_to_approach_creating_ai_models.html</guid>
  <pubDate>Fri, 27 May 2022 00:00:00 GMT</pubDate>
  <media:content url="https://forbo7.github.io/forblog/images/1_how_to_approach_creating_ai_models/thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
